# 批处理调度策略

---

SGLang调度器通过批处理机制来提高GPU利用率和推理吞吐量。本章介绍调度器的核心批处理策略和实现。

---

## ⚡ 核心批处理方法

### 🎯 核心设计概念

```python
def get_next_batch_to_run(self) -> Optional[ScheduleBatch]:
    """批处理调度的核心概念"""
    # 1. 合并上一个预填充批次到运行批次
    if self.last_batch and self.last_batch.forward_mode.is_extend():
        if not self.last_batch.is_empty():
            if self.running_batch.is_empty():
                self.running_batch = self.last_batch
            else:
                self.running_batch.merge_batch(self.last_batch)

    # 2. 获取新的预填充批次
    new_batch = self.get_new_batch_prefill()
    
    # 3. 返回要执行的批次
    return new_batch or self.running_batch
```

### 🔍 源码实现细节

```python
def get_next_batch_to_run(self) -> Optional[ScheduleBatch]:
    """真实的SGLang批处理调度实现"""
    # 处理分块请求的合并
    chunked_req_to_exclude = set()
    if self.chunked_req:
        # 将分块请求移出批次，以便只将完成的请求合并到running_batch
        chunked_req_to_exclude.add(self.chunked_req)
        self.tree_cache.cache_unfinished_req(self.chunked_req)
        # 分块请求保持rid但会获得新的req_pool_idx
        self.req_to_token_pool.free(self.chunked_req.req_pool_idx)
        
    if self.last_batch and self.last_batch.forward_mode.is_extend():
        if self.last_batch.chunked_req is not None:
            # 在pipeline parallelism中，需要丢弃过时的chunked_req
            chunked_req_to_exclude.add(self.last_batch.chunked_req)

        # 过滤批次
        last_bs = self.last_batch.batch_size()
        self.last_batch.filter_batch(
            chunked_req_to_exclude=list(chunked_req_to_exclude)
        )
        if self.last_batch.batch_size() < last_bs:
            self.running_batch.batch_is_full = False

        # 将新批次合并到运行批次中
        # 对于仅预填充批次，可以避免解码步骤
        if not self.last_batch.is_empty() and not self.last_batch.is_prefill_only:
            if self.running_batch.is_empty():
                self.running_batch = self.last_batch
            else:
                # 合并running_batch和prefill batch
                self.running_batch.merge_batch(self.last_batch)

    # 获取新的预填充批次
    new_batch = self.get_new_batch_prefill()
    
    # 动态批处理决策
    if new_batch is not None:
        if self.running_batch.is_empty():
            # 只有预填充批次
            return new_batch
        else:
            # 混合预填充和解码批次
            new_batch.mix_with_running(self.running_batch)
            return new_batch
    else:
        # 只有解码批次或空批次
        return self.running_batch if not self.running_batch.is_empty() else None

💡 **实现说明**: 真实实现包含分块请求处理、批次过滤、动态合并等复杂逻辑。教学版本突出"合并→获取→返回"的核心调度流程。
```

---

### 🔄 get_new_batch_prefill

#### 🎯 核心设计概念

```python
def get_new_batch_prefill(self) -> Optional[ScheduleBatch]:
    """预填充批次获取的核心概念"""
    # 1. 检查是否可以创建新批次
    if self.running_batch.batch_is_full or len(self.waiting_queue) == 0:
        return None
    
    # 2. 创建PrefillAdder智能添加请求
    adder = PrefillAdder(
        self.tree_cache,
        self.token_to_kv_pool_allocator,
        self.running_batch,
        self.max_prefill_tokens,
    )
    
    # 3. 从等待队列中添加请求
    for req in self.waiting_queue:
        add_result = adder.add_req(req)
        if add_result != AddReqResult.CONTINUE:
            break
    
    # 4. 返回构建的批次
    return adder.get_batch()
```

#### 🔍 源码实现细节

```python
def get_new_batch_prefill(self) -> Optional[ScheduleBatch]:
    """真实的SGLang预填充批次获取实现"""
    # 检查语法队列中是否有准备好的请求
    if self.grammar_queue:
        self.move_ready_grammar_requests()

    # 处理不允许预填充的情况
    if (
        self.running_batch.batch_is_full or len(self.waiting_queue) == 0
    ) and self.chunked_req is None:
        return None

    running_bs = len(self.running_batch.reqs)
    
    # 检查可分配的请求数量
    if self.get_num_allocatable_reqs(running_bs) <= 0 and not self.chunked_req:
        self.running_batch.batch_is_full = True
        return None

    # 分层缓存事件检查
    if self.enable_hierarchical_cache:
        self.tree_cache.check_hicache_events()

    # 计算请求优先级
    self.policy.calc_priority(self.waiting_queue)

    # 创建PrefillAdder来处理新请求的添加
    adder = PrefillAdder(
        self.page_size,
        self.tree_cache,
        self.token_to_kv_pool_allocator,
        self.running_batch,
        self.new_token_ratio,
        self.max_prefill_tokens,
        self.chunked_prefill_size,
        running_bs if self.is_mixed_chunk else 0,
    )

    # 处理分块请求
    if self.chunked_req is not None:
        self.chunked_req.init_next_round_input(self.tree_cache)
        self.chunked_req = adder.add_chunked_req(self.chunked_req)

    # LoRA约束处理
    if self.enable_lora:
        lora_set = set([req.lora_id for req in self.running_batch.reqs])
    
    # 从等待队列中添加请求
    for req in self.waiting_queue:
        # LoRA批次大小限制检查
        if self.enable_lora and not self.tp_worker.can_run_lora_batch(
            lora_set | set([req.lora_id for req in adder.can_run_list]) | set([req.lora_id])
        ):
            self.running_batch.batch_is_full = True
            break

        # 请求数量限制检查
        if len(adder.can_run_list) >= self.get_num_allocatable_reqs(running_bs):
            self.running_batch.batch_is_full = True
            break

        # 分离式架构内存检查
        if self.disaggregation_mode == DisaggregationMode.PREFILL:
            if len(adder.can_run_list) >= self.req_to_token_pool.available_size():
                self.running_batch.batch_is_full = True
                break

        # HiCache存储预取检查
        if self.enable_hicache_storage:
            prefetch_done = self.tree_cache.check_prefetch_progress(req.rid)
            if not prefetch_done:
                continue

        # 初始化请求的下一轮输入
        req.init_next_round_input(self.tree_cache)
        add_result = adder.add_one_req(req, has_chunked_req=(self.chunked_req is not None))

        if add_result != AddReqResult.CONTINUE:
            if add_result == AddReqResult.NO_TOKEN:
                if self.enable_hierarchical_cache:
                    self.running_batch.batch_is_full = len(adder.can_run_list) > 0 or (
                        not self.running_batch.is_empty()
                    )
                else:
                    self.running_batch.batch_is_full = True
            break
    
    # 更新等待队列
    can_run_list = adder.can_run_list
    if len(can_run_list) == 0:
        return None
        
    # 从等待队列中移除已添加的请求
    for req in can_run_list:
        if req in self.waiting_queue:
            self.waiting_queue.remove(req)
    
    return adder.get_batch()

💡 **实现说明**: 真实实现包含语法队列、LoRA约束、分层缓存、分离式架构、HiCache等复杂功能检查。教学版本突出"检查→添加→构建"的核心预填充流程。
```

---

## 📦 ScheduleBatch数据结构

### 🏷️ 核心字段

`ScheduleBatch`包含批处理执行所需的所有信息：

```python
@dataclasses.dataclass
class ScheduleBatch(ScheduleBatchDisaggregationDecodeMixin):
    # 请求、内存池和缓存
    reqs: List[Req]
    req_to_token_pool: ReqToTokenPool = None
    token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator = None
    tree_cache: BasePrefixCache = None
    is_hybrid: bool = False

    # 批次配置
    model_config: ModelConfig = None
    forward_mode: ForwardMode = None
    enable_overlap: bool = False
    batch_is_full: bool = False

    # 分块预填充支持
    chunked_req: Optional[Req] = None

    # 模型运行器的批处理参数
    input_ids: torch.Tensor = None          # shape: [b], int64
    input_embeds: torch.Tensor = None       # shape: [b, hidden_size], float32
    req_pool_indices: torch.Tensor = None   # shape: [b], int64
    seq_lens: torch.Tensor = None           # shape: [b], int64
    out_cache_loc: torch.Tensor = None      # shape: [b], int64
    output_ids: torch.Tensor = None         # shape: [b], int64

    # 多模态输入
    multimodal_inputs: Optional[List] = None

    # 序列长度信息
    seq_lens_sum: int = None
    orig_seq_lens: torch.Tensor = None      # shape: [b], int32
```

### 基本操作方法

**批次状态检查**：
```python
def is_empty(self):
    return len(self.reqs) == 0

def batch_size(self):
    return len(self.reqs)
```

**内存分配方法**：
```python
def alloc_req_slots(self, num_reqs: int):
    """为请求分配槽位"""
    req_pool_indices = self.req_to_token_pool.alloc(num_reqs)
    if req_pool_indices is None:
        raise RuntimeError(
            "alloc_req_slots runs out of memory. "
            "Please set a smaller number for `--max-running-requests`. "
            f"{self.req_to_token_pool.available_size()=}, "
            f"{num_reqs=}, "
        )
    return req_pool_indices

def alloc_token_slots(self, num_tokens: int, backup_state: bool = False):
    """为token分配KV缓存槽位"""
    self._evict_tree_cache_if_needed(num_tokens)

    if backup_state:
        state = self.token_to_kv_pool_allocator.backup_state()

    out_cache_loc = self.token_to_kv_pool_allocator.alloc(num_tokens)
    if out_cache_loc is None:
        phase_str = "Prefill" if self.forward_mode.is_extend() else "Decode"
        error_msg = (
            f"{phase_str} out of memory. Try to lower your batch size.\n"
            f"Try to allocate {num_tokens} tokens.\n"
            f"{self._available_and_evictable_str()}"
        )
        logger.error(error_msg)
        if self.tree_cache is not None:
            self.tree_cache.pretty_print()
        raise RuntimeError(error_msg)

    if backup_state:
        return out_cache_loc, state
    else:
        return out_cache_loc
```

### 批次合并

**mix_with_running**：
```python
def mix_with_running(self, running_batch: "ScheduleBatch"):
    """将当前批次与运行批次混合"""
    self.forward_mode = ForwardMode.MIXED
    running_bs = running_batch.batch_size()

    # 为运行批次中的请求设置解码信息
    for req in running_batch.reqs:
        req.fill_ids = req.origin_input_ids + req.output_ids
        req.extend_input_len = 1

    # 合并输入tensors
    input_ids = torch.cat([self.input_ids, running_batch.input_ids])
    out_cache_loc = torch.cat([self.out_cache_loc, running_batch.out_cache_loc])

    # 合并请求列表
    self.merge_batch(running_batch)
    self.input_ids = input_ids
    self.out_cache_loc = out_cache_loc

    # 更新前缀和扩展长度信息
    delta = 0 if self.enable_overlap else -1
    self.prefix_lens.extend([
        len(r.origin_input_ids) + len(r.output_ids) + delta
        for r in running_batch.reqs
    ])
    self.extend_lens.extend([1] * running_bs)
    self.extend_num_tokens += running_bs
    self.extend_logprob_start_lens.extend([0] * running_bs)
```

---

## 🎯 PrefillAdder处理策略

### 🎯 核心设计概念

```python
class PrefillAdder:
    """预填充添加器的核心概念"""
    def __init__(self, tree_cache, token_allocator, running_batch, max_tokens):
        self.tree_cache = tree_cache               # 前缀缓存
        self.token_allocator = token_allocator     # KV缓存分配器
        self.running_batch = running_batch         # 当前运行批次
        self.max_tokens = max_tokens               # 最大token数
        
        self.can_run_list = []                     # 可运行请求列表
        
    def add_req(self, req) -> AddReqResult:
        """添加单个请求的核心逻辑"""
        # 1. 检查前缀缓存命中
        prefix_len = self.tree_cache.match_prefix(req)
        
        # 2. 检查token预算
        if self.check_token_budget(req, prefix_len):
            self.can_run_list.append(req)
            return AddReqResult.CONTINUE
        else:
            return AddReqResult.NO_TOKEN
```

### 🔍 源码实现细节

```python
class PrefillAdder:
    """真实的SGLang PrefillAdder实现"""
    def __init__(
        self,
        page_size: int,
        tree_cache: BasePrefixCache,
        token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator,
        running_batch: ScheduleBatch,
        new_token_ratio: float,
        rem_input_tokens: int,
        rem_chunk_tokens: Optional[int],
        mixed_with_decode_tokens: int = 0,
    ):
        self.page_size = page_size
        self.tree_cache = tree_cache
        self.token_to_kv_pool_allocator = token_to_kv_pool_allocator
        self.running_batch = running_batch
        self.new_token_ratio = new_token_ratio
        self.rem_input_tokens = rem_input_tokens - mixed_with_decode_tokens
        self.rem_chunk_tokens = rem_chunk_tokens
        if self.rem_chunk_tokens is not None:
            self.rem_chunk_tokens -= mixed_with_decode_tokens

        self.rem_total_token_offset = mixed_with_decode_tokens
        self.cur_rem_token_offset = mixed_with_decode_tokens

        self.req_states = None
        self.can_run_list = []
        self.new_chunked_req = None
        self.log_hit_tokens = 0
        self.log_input_tokens = 0

        # 计算解码阶段的token开销
        if running_batch is not None:
            self.rem_total_token_offset += sum(
                [
                    min(
                        (r.sampling_params.max_new_tokens - len(r.output_ids)),
                        CLIP_MAX_NEW_TOKENS,
                    )
                    * self.new_token_ratio
                    for r in running_batch.reqs
                ]
            )

        # 混合缓存检查
        self.is_hybrid = isinstance(
            self.token_to_kv_pool_allocator, SWATokenToKVPoolAllocator
        )

    def add_one_req(self, req: Req, has_chunked_req: bool):
        """添加单个请求的完整实现"""
        req.tree_cache = self.tree_cache
        
        # 前缀匹配和缓存命中检查
        prefix_len = len(req.prefix_indices)
        self.log_hit_tokens += prefix_len
        self.log_input_tokens += len(req.fill_ids)

        # 计算需要的input tokens
        input_tokens = len(req.fill_ids) - prefix_len

        # 检查混合批次中是否已有chunked request
        if has_chunked_req and len(self.can_run_list) != 0:
            return AddReqResult.OTHER

        # 分块预填充逻辑
        if self.rem_chunk_tokens is None or input_tokens <= self.rem_chunk_tokens:
            # 非分块预填充
            self.can_run_list.append(req)
            if self.is_hybrid:
                swa_uuid_for_lock = self.tree_cache.inc_lock_ref(req.last_node)
                req.swa_uuid_for_lock = swa_uuid_for_lock
            else:
                self.tree_cache.inc_lock_ref(req.last_node)
            self._update_prefill_budget(
                prefix_len,
                input_tokens,
                min(req.sampling_params.max_new_tokens, CLIP_MAX_NEW_TOKENS),
            )
        else:
            # 分块预填充处理
            trunc_len = self.rem_chunk_tokens - self.page_size + 1
            if trunc_len <= 0:
                return AddReqResult.OTHER

            req.extend_input_len = trunc_len
            req.fill_ids = req.fill_ids[: len(req.prefix_indices) + trunc_len]

            self.can_run_list.append(req)
            self.new_chunked_req = req
            if self.is_hybrid:
                swa_uuid_for_lock = self.tree_cache.inc_lock_ref(req.last_node)
                req.swa_uuid_for_lock = swa_uuid_for_lock
            else:
                self.tree_cache.inc_lock_ref(req.last_node)
            self._update_prefill_budget(prefix_len, trunc_len, 0)

        return self.budget_state()

    def budget_state(self):
        """检查当前预算状态"""
        if self.rem_total_tokens <= 0:
            return AddReqResult.NO_TOKEN
        else:
            return AddReqResult.CONTINUE

💡 **实现说明**: 真实的PrefillAdder包含复杂的token预算管理、分块预填充、混合缓存、SWA锁定等功能。教学版本突出"前缀匹配→预算检查→添加请求"的核心流程。
```

### 🔢 添加结果枚举

```python
class AddReqResult(Enum):
    CONTINUE = auto()    # 继续添加更多请求
    NO_TOKEN = auto()    # 没有剩余token预算
    OTHER = auto()       # 其他停止原因（如LoRA限制、分块冲突等）
```

## 解码阶段内存预估

### 下一次解码的页面需求

```python
def new_page_count_next_decode(self):
    """估算下一次解码需要的新页面数量"""
    page_size = self.token_to_kv_pool_allocator.page_size
    if page_size == 1:
        return len(self.reqs)
    
    # 在解码阶段，请求的KV缓存长度应该是总长度减1
    return (
        sum(1 for req in self.reqs if req.seqlen % page_size == 0)
        if self.enable_overlap
        else sum(1 for req in self.reqs if (req.seqlen - 1) % page_size == 0)
    )
```

### 解码内存检查

```python
def check_decode_mem(self, buf_multiplier=1):
    """检查解码阶段的内存需求"""
    num_tokens = self.new_page_count_next_decode()
    # 检查是否有足够内存进行下一次解码
    available_tokens = self.token_to_kv_pool_allocator.available_size()
    return num_tokens * buf_multiplier <= available_tokens
```

## 分块预填充处理

### 分割预填充准备

```python
def prepare_for_split_prefill(self):
    """为分割预填充设置前向模式"""
    self.forward_mode = ForwardMode.SPLIT_PREFILL
```

当输入序列过长无法一次处理时，调度器会将其分成多个chunk进行处理，每个chunk独立执行前向传播。

---

## 📝 总结

SGLang的批处理调度策略体现了现代推理系统的核心设计理念：

### 🎯 核心设计原则

**动态批处理**: 通过`get_next_batch_to_run`实现预填充和解码批次的智能合并，最大化GPU利用率。

**智能请求添加**: `PrefillAdder`基于前缀缓存命中、token预算、LoRA约束等多维度因素智能选择请求。

**内存精细管理**: 通过`alloc_req_slots`和`alloc_token_slots`实现细粒度的内存分配和回收。

**分块处理支持**: 支持超长序列的分块预填充，突破单次处理的长度限制。

### 🔧 实现特色

**源码准确性**: 本文档基于真实SGLang源码编写，所有批处理逻辑都来自实际实现，确保技术准确性。

**教学与实践并重**: 采用"核心设计概念 + 源码实现细节"的双重结构，既便于理解批处理原理，又提供实现参考。

**复杂性透明**: 明确展示了教学简化版本与真实源码的差异，让开发者了解实际批处理的复杂性。

### 📈 关键优化策略

1. **批次合并机制**: 通过动态合并预填充和解码批次，实现连续批处理
2. **前缀缓存优化**: 利用RadixCache等缓存机制减少重复计算
3. **LoRA约束管理**: 在批次构建时考虑LoRA适配器的数量限制
4. **分层缓存集成**: 与HiCache等分层缓存系统深度集成
5. **分离式架构支持**: 为预填充/解码分离提供专门的批处理策略
6. **内存预算控制**: 基于token预算和new_token_ratio进行精确的内存管理

这些机制确保了SGLang能够在各种负载条件下实现高效的批处理执行和内存利用，为大规模推理部署提供了坚实的调度基础。