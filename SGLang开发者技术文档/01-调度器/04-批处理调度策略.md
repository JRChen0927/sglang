# æ‰¹å¤„ç†è°ƒåº¦ç­–ç•¥

---

SGLangè°ƒåº¦å™¨é€šè¿‡æ™ºèƒ½çš„æ‰¹å¤„ç†æœºåˆ¶æ¥æœ€å¤§åŒ–GPUåˆ©ç”¨ç‡å’Œæ¨ç†ååé‡ã€‚æœ¬ç« æ·±å…¥ä»‹ç»è°ƒåº¦å™¨çš„æ ¸å¿ƒæ‰¹å¤„ç†ç­–ç•¥ã€è¿ç»­æ‰¹å¤„ç†å®ç°å’ŒåŠ¨æ€è°ƒåº¦ç®—æ³•ï¼Œæ­ç¤ºSGLangå¦‚ä½•å®ç°é«˜æ•ˆçš„æ··åˆé¢„å¡«å……-è§£ç æ‰¹å¤„ç†ã€‚

---

## 1. æ‰¹å¤„ç†æ¶æ„æ€»è§ˆ

SGLangçš„æ‰¹å¤„ç†è°ƒåº¦ç³»ç»Ÿæ˜¯å…¶é«˜æ€§èƒ½æ¨ç†çš„æ ¸å¿ƒå¼•æ“ï¼Œå®ƒé€šè¿‡æ™ºèƒ½çš„æ‰¹æ¬¡ç®¡ç†å’ŒåŠ¨æ€è°ƒåº¦ç®—æ³•ï¼Œå®ç°äº†é¢„å¡«å……å’Œè§£ç é˜¶æ®µçš„é«˜æ•ˆåè°ƒã€‚è¿™ä¸ªç³»ç»Ÿä¸ä»…è¦å¤„ç†å•ä¸€ç±»å‹çš„æ‰¹æ¬¡ï¼Œè¿˜è¦æ”¯æŒæ··åˆæ‰¹æ¬¡ã€åˆ†å—é¢„å¡«å……ã€è¿ç»­æ‰¹å¤„ç†ç­‰å¤æ‚åœºæ™¯ã€‚

**æ‰¹å¤„ç†ç³»ç»Ÿçš„æ ¸å¿ƒæŒ‘æˆ˜**ï¼š
- **å¼‚æ„è¯·æ±‚ç®¡ç†**ï¼šä¸åŒé•¿åº¦ã€ä¸åŒé˜¶æ®µçš„è¯·æ±‚éœ€è¦ç»Ÿä¸€çš„æ‰¹å¤„ç†æ¡†æ¶
- **å†…å­˜æ•ˆç‡ä¼˜åŒ–**ï¼šåœ¨æœ‰é™çš„GPUå†…å­˜ä¸­æœ€å¤§åŒ–æ‰¹æ¬¡å¤§å°å’Œå¤„ç†æ•ˆç‡
- **åŠ¨æ€è´Ÿè½½å¹³è¡¡**ï¼šæ ¹æ®ç³»ç»ŸçŠ¶æ€åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡ç»„æˆå’Œæ‰§è¡Œç­–ç•¥
- **å¤šæ¨¡å¼åè°ƒ**ï¼šåè°ƒé¢„å¡«å……ã€è§£ç ã€æ··åˆç­‰å¤šç§å‰å‘æ¨¡å¼çš„æ‰§è¡Œ

**è°ƒåº¦ç­–ç•¥çš„è®¾è®¡åŸåˆ™**ï¼š
- **è¿ç»­æ‰¹å¤„ç†**ï¼šé€šè¿‡last_batchå’Œrunning_batchçš„åè°ƒï¼Œå®ç°æ— ç¼çš„è¿ç»­å¤„ç†
- **æ™ºèƒ½åˆå¹¶æœºåˆ¶**ï¼šåŠ¨æ€å†³å®šæ˜¯å¦åˆå¹¶é¢„å¡«å……å’Œè§£ç æ‰¹æ¬¡ï¼Œä¼˜åŒ–GPUåˆ©ç”¨ç‡
- **åˆ†å—å¤„ç†æ”¯æŒ**ï¼šå¯¹è¶…é•¿åºåˆ—æä¾›åˆ†å—é¢„å¡«å……æ”¯æŒï¼Œçªç ´å•æ¬¡å¤„ç†çš„é•¿åº¦é™åˆ¶
- **å‰ç¼€ç¼“å­˜é›†æˆ**ï¼šä¸RadixCacheç­‰å‰ç¼€ç¼“å­˜ç³»ç»Ÿæ·±åº¦é›†æˆï¼Œå‡å°‘é‡å¤è®¡ç®—

**çŠ¶æ€ç®¡ç†çš„å¤æ‚æ€§**ï¼š
æ‰¹å¤„ç†è°ƒåº¦éœ€è¦ç®¡ç†å¤šä¸ªæ‰¹æ¬¡çŠ¶æ€ï¼ˆlast_batchã€running_batchã€chunked_reqï¼‰ï¼Œæ¯ä¸ªçŠ¶æ€éƒ½æœ‰å…¶ç‰¹å®šçš„ç”Ÿå‘½å‘¨æœŸå’Œè½¬æ¢è§„åˆ™ã€‚è¿™ç§è®¾è®¡ç¡®ä¿äº†ç³»ç»Ÿèƒ½å¤Ÿåœ¨å¤æ‚çš„è°ƒåº¦åœºæ™¯ä¸‹ä¿æŒçŠ¶æ€ä¸€è‡´æ€§ã€‚

### 1.1 æ‰¹å¤„ç†è°ƒåº¦æµç¨‹å¯è§†åŒ–

```mermaid
graph TD
    subgraph "ğŸ“‹ æ‰¹æ¬¡çŠ¶æ€ç®¡ç†"
        direction TB
        A1["last_batch<br/>ä¸Šä¸€æ‰¹æ¬¡"]
        A2["running_batch<br/>è¿è¡Œæ‰¹æ¬¡"]
        A3["chunked_req<br/>åˆ†å—è¯·æ±‚"]
    end

    subgraph "ğŸ”„ æ‰¹æ¬¡è°ƒåº¦æ ¸å¿ƒ"
        direction LR
        B1["get_next_batch_to_run()<br/>æ‰¹æ¬¡è°ƒåº¦å…¥å£"]
        B2["åˆå¹¶ä¸Šä¸€æ‰¹æ¬¡<br/>merge last_batch"]
        B3["è·å–æ–°é¢„å¡«å……æ‰¹æ¬¡<br/>get_new_batch_prefill()"]
        B4["åŠ¨æ€æ‰¹æ¬¡å†³ç­–<br/>æ··åˆæˆ–å•ä¸€æ‰¹æ¬¡"]
    end

    subgraph "ğŸ“Š é¢„å¡«å……æ‰¹æ¬¡è·å–"
        direction TB
        C1["PrefillAdder<br/>é¢„å¡«å……æ·»åŠ å™¨"]
        C2["waiting_queue<br/>ç­‰å¾…é˜Ÿåˆ—"]
        C3["grammar_queue<br/>è¯­æ³•é˜Ÿåˆ—"]
        C4["ä¼˜å…ˆçº§è®¡ç®—<br/>priority calculation"]
    end

    subgraph "ğŸ¯ æ‰¹æ¬¡ç±»å‹"
        direction LR
        D1["EXTEND<br/>çº¯é¢„å¡«å……æ‰¹æ¬¡"]
        D2["DECODE<br/>çº¯è§£ç æ‰¹æ¬¡"]
        D3["MIXED<br/>æ··åˆæ‰¹æ¬¡"]
        D4["SPLIT_PREFILL<br/>åˆ†å—é¢„å¡«å……"]
    end

    A1 --> B1
    A2 --> B1
    A3 --> B1
    B1 --> B2
    B2 --> B3
    B3 --> B4
    C2 --> C1
    C3 --> C1
    C1 --> C4
    C4 --> B3
    B4 --> D1
    B4 --> D2
    B4 --> D3
    B4 --> D4

    style A1 fill:#e3f2fd,color:#000000,stroke:#333
    style A2 fill:#e3f2fd,color:#000000,stroke:#333
    style A3 fill:#e3f2fd,color:#000000,stroke:#333
    style B1 fill:#f1f8e9,color:#000000,stroke:#333
    style B2 fill:#f1f8e9,color:#000000,stroke:#333
    style B3 fill:#f1f8e9,color:#000000,stroke:#333
    style B4 fill:#f1f8e9,color:#000000,stroke:#333
    style C1 fill:#fff3e0,color:#000000,stroke:#333
    style C2 fill:#fff3e0,color:#000000,stroke:#333
    style C3 fill:#fff3e0,color:#000000,stroke:#333
    style C4 fill:#fff3e0,color:#000000,stroke:#333
    style D1 fill:#ffebee,color:#000000,stroke:#333
    style D2 fill:#ffebee,color:#000000,stroke:#333
    style D3 fill:#ffebee,color:#000000,stroke:#333
    style D4 fill:#ffebee,color:#000000,stroke:#333
```

**å›¾ç¤ºè¯´æ˜**ï¼šè“è‰²è¡¨ç¤ºæ‰¹æ¬¡çŠ¶æ€ï¼Œç»¿è‰²è¡¨ç¤ºè°ƒåº¦æ ¸å¿ƒï¼Œæ©™è‰²è¡¨ç¤ºé¢„å¡«å……è·å–ï¼Œçº¢è‰²è¡¨ç¤ºæ‰¹æ¬¡ç±»å‹ã€‚æ•´ä¸ªæµç¨‹å±•ç¤ºäº†SGLangå¦‚ä½•é€šè¿‡æ™ºèƒ½è°ƒåº¦å®ç°é«˜æ•ˆçš„è¿ç»­æ‰¹å¤„ç†ã€‚

---

## 2. æ ¸å¿ƒæ‰¹å¤„ç†æ–¹æ³•

### 2.1 get_next_batch_to_runæ ¸å¿ƒå®ç°

get_next_batch_to_runæ–¹æ³•æ˜¯SGLangæ‰¹å¤„ç†è°ƒåº¦çš„æ ¸å¿ƒå¼•æ“ï¼Œå®ƒè´Ÿè´£åè°ƒå¤šä¸ªæ‰¹æ¬¡çŠ¶æ€çš„è½¬æ¢å’Œåˆå¹¶ã€‚è¿™ä¸ªæ–¹æ³•éœ€è¦å¤„ç†å¤æ‚çš„æ‰¹æ¬¡ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ŒåŒ…æ‹¬åˆ†å—è¯·æ±‚çš„ç‰¹æ®Šå¤„ç†ã€ä¸Šä¸€æ‰¹æ¬¡çš„åˆå¹¶ã€æ–°é¢„å¡«å……æ‰¹æ¬¡çš„è·å–ç­‰å¤šä¸ªç¯èŠ‚ã€‚

**æ‰¹æ¬¡è°ƒåº¦çš„æ ¸å¿ƒèŒè´£**ï¼š
- **åˆ†å—è¯·æ±‚ç®¡ç†**ï¼šå¤„ç†è¶…é•¿åºåˆ—çš„åˆ†å—é¢„å¡«å……ï¼Œç¡®ä¿åˆ†å—è¯·æ±‚çš„æ­£ç¡®çŠ¶æ€è½¬æ¢
- **æ‰¹æ¬¡çŠ¶æ€åè°ƒ**ï¼šç®¡ç†last_batchåˆ°running_batchçš„è½¬æ¢ï¼Œå®ç°è¿ç»­æ‰¹å¤„ç†
- **åŠ¨æ€åˆå¹¶å†³ç­–**ï¼šæ ¹æ®ç³»ç»ŸçŠ¶æ€å†³å®šæ˜¯å¦åˆå¹¶é¢„å¡«å……å’Œè§£ç æ‰¹æ¬¡
- **æŠ•æœºè§£ç å…¼å®¹**ï¼šå¤„ç†æŠ•æœºè§£ç ä¸æ•°æ®å¹¶è¡Œæ³¨æ„åŠ›çš„å…¼å®¹æ€§é—®é¢˜

**çŠ¶æ€è½¬æ¢çš„å¤æ‚æ€§**ï¼š
- **åˆ†å—è¯·æ±‚æ’é™¤**ï¼šéœ€è¦å°†å®Œæˆçš„åˆ†å—è¯·æ±‚ä»æ‰¹æ¬¡ä¸­ç§»é™¤ï¼ŒåŒæ—¶ç¼“å­˜æœªå®Œæˆçš„éƒ¨åˆ†
- **æ‰¹æ¬¡è¿‡æ»¤æœºåˆ¶**ï¼šè¿‡æ»¤æ‰éœ€è¦æ’é™¤çš„è¯·æ±‚ï¼Œæ›´æ–°æ‰¹æ¬¡çš„æ»¡è½½çŠ¶æ€
- **å†…å­˜æ± ç®¡ç†**ï¼šåŠæ—¶é‡Šæ”¾åˆ†å—è¯·æ±‚å ç”¨çš„å†…å­˜æ± æ§½ä½ï¼Œä¸ºæ–°è¯·æ±‚è…¾å‡ºç©ºé—´

**åˆå¹¶ç­–ç•¥ä¼˜åŒ–**ï¼š
ç³»ç»Ÿä¼šæ ¹æ®æ‰¹æ¬¡ç±»å‹æ™ºèƒ½å†³å®šåˆå¹¶ç­–ç•¥ã€‚å¯¹äºä»…é¢„å¡«å……æ‰¹æ¬¡ï¼ˆis_prefill_onlyï¼‰ï¼Œå¯ä»¥è·³è¿‡è§£ç é˜¶æ®µç›´æ¥å¤„ç†ï¼›å¯¹äºæ··åˆæ‰¹æ¬¡ï¼Œéœ€è¦è€ƒè™‘é¢„å¡«å……å’Œè§£ç çš„åè°ƒæ‰§è¡Œã€‚

```python
def get_next_batch_to_run(self) -> Optional[ScheduleBatch]:
    # Merge the prefill batch into the running batch
    chunked_req_to_exclude = set()
    if self.chunked_req:
        # Move the chunked request out of the batch so that we can merge
        # only finished requests to running_batch.
        chunked_req_to_exclude.add(self.chunked_req)
        self.tree_cache.cache_unfinished_req(self.chunked_req)
        # chunked request keeps its rid but will get a new req_pool_idx
        self.req_to_token_pool.free(self.chunked_req.req_pool_idx)
        
    if self.last_batch and self.last_batch.forward_mode.is_extend():
        if self.last_batch.chunked_req is not None:
            # In the context pipeline parallelism, after the last chunk, the current microbatch still track outdated chunked_req.
            # We need to discard it.
            chunked_req_to_exclude.add(self.last_batch.chunked_req)

        # Filter batch
        last_bs = self.last_batch.batch_size()
        self.last_batch.filter_batch(
            chunked_req_to_exclude=list(chunked_req_to_exclude)
        )
        if self.last_batch.batch_size() < last_bs:
            self.running_batch.batch_is_full = False

        # Merge the new batch into the running batch.
        # For prefill-only batch, we can avoid going through decoding step.
        if not self.last_batch.is_empty() and not self.last_batch.is_prefill_only:
            if self.running_batch.is_empty():
                self.running_batch = self.last_batch
            else:
                # Merge running_batch with prefill batch
                self.running_batch.merge_batch(self.last_batch)

    new_batch = self.get_new_batch_prefill()
    
    need_dp_attn_preparation = require_mlp_sync(self.server_args)
    
    if need_dp_attn_preparation and not self.spec_algorithm.is_none():
        if new_batch is not None:
            new_batch.spec_algorithm = SpeculativeAlgorithm.NONE
        if not self.running_batch.is_empty():
            self.running_batch.spec_algorithm = SpeculativeAlgorithm.NONE

    # Return the next batch to run
    if new_batch is not None:
        if self.running_batch.is_empty():
            return new_batch
        else:
            new_batch.mix_with_running(self.running_batch)
            return new_batch
    else:
        return self.running_batch if not self.running_batch.is_empty() else None
```

### 2.2 get_new_batch_prefillå®ç°

get_new_batch_prefillæ–¹æ³•æ˜¯é¢„å¡«å……æ‰¹æ¬¡æ„å»ºçš„æ ¸å¿ƒç»„ä»¶ï¼Œå®ƒä»ç­‰å¾…é˜Ÿåˆ—ä¸­æ™ºèƒ½é€‰æ‹©è¯·æ±‚å¹¶æ„å»ºæ–°çš„é¢„å¡«å……æ‰¹æ¬¡ã€‚è¿™ä¸ªæ–¹æ³•éœ€è¦è€ƒè™‘å¤šç§çº¦æŸæ¡ä»¶ï¼ŒåŒ…æ‹¬å†…å­˜é™åˆ¶ã€LoRAçº¦æŸã€è¯­æ³•é˜Ÿåˆ—çŠ¶æ€ã€åˆ†å±‚ç¼“å­˜ç­‰å¤æ‚å› ç´ ã€‚

**é¢„å¡«å……æ‰¹æ¬¡æ„å»ºçš„æ ¸å¿ƒæµç¨‹**ï¼š
- **è¯­æ³•é˜Ÿåˆ—æ£€æŸ¥**ï¼šä¼˜å…ˆå¤„ç†è¯­æ³•é˜Ÿåˆ—ä¸­å·²å‡†å¤‡å¥½çš„è¯·æ±‚
- **å®¹é‡é¢„æ£€æŸ¥**ï¼šæ£€æŸ¥è¿è¡Œæ‰¹æ¬¡æ˜¯å¦å·²æ»¡ï¼Œé¿å…ä¸å¿…è¦çš„å¤„ç†å¼€é”€
- **èµ„æºå¯ç”¨æ€§éªŒè¯**ï¼šç¡®ä¿æœ‰è¶³å¤Ÿçš„å†…å­˜å’Œè¯·æ±‚æ§½ä½æ¥æ„å»ºæ–°æ‰¹æ¬¡
- **ä¼˜å…ˆçº§è®¡ç®—**ï¼šé€šè¿‡è°ƒåº¦ç­–ç•¥è®¡ç®—è¯·æ±‚çš„å¤„ç†ä¼˜å…ˆçº§

**çº¦æŸæ¡ä»¶çš„å¤šç»´åº¦æ£€æŸ¥**ï¼š
- **LoRAæ‰¹æ¬¡é™åˆ¶**ï¼šæ£€æŸ¥LoRAé€‚é…å™¨çš„æ•°é‡æ˜¯å¦è¶…è¿‡æ‰¹æ¬¡é™åˆ¶
- **å†…å­˜èµ„æºé™åˆ¶**ï¼šéªŒè¯å¯åˆ†é…çš„è¯·æ±‚æ•°é‡å’Œtokenæ•°é‡
- **åˆ†ç¦»å¼æ¶æ„é™åˆ¶**ï¼šåœ¨åˆ†ç¦»å¼æ¨¡å¼ä¸‹æ£€æŸ¥ç‰¹å®šçš„èµ„æºçº¦æŸ
- **åˆ†å±‚ç¼“å­˜çŠ¶æ€**ï¼šæ£€æŸ¥HiCacheçš„é¢„å–è¿›åº¦å’Œå¯ç”¨æ€§

**PrefillAdderé›†æˆ**ï¼š
æ–¹æ³•é€šè¿‡PrefillAdderå®ç°æ™ºèƒ½çš„è¯·æ±‚æ·»åŠ ï¼Œè¯¥ç»„ä»¶è´Ÿè´£tokené¢„ç®—ç®¡ç†ã€å‰ç¼€ç¼“å­˜ä¼˜åŒ–ã€åˆ†å—å¤„ç†ç­‰æ ¸å¿ƒåŠŸèƒ½ã€‚PrefillAdderçš„è®¾è®¡ç¡®ä¿äº†æ‰¹æ¬¡æ„å»ºè¿‡ç¨‹çš„é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

```python
def get_new_batch_prefill(self) -> Optional[ScheduleBatch]:
    # Check if the grammar is ready in the grammar queue
    if self.grammar_queue:
        self.move_ready_grammar_requests()

    # Handle the cases where prefill is not allowed
    if (
        self.running_batch.batch_is_full or len(self.waiting_queue) == 0
    ) and self.chunked_req is None:
        return None

    running_bs = len(self.running_batch.reqs)
    # Ignore the check if self.chunked_req is not None.
    # In the non-PP case, when self.chunked_req is not None, num_allocatable_reqs should always be greater than 0,
    # as the space for the chunked request has just been released.
    # In PP case, a chunked req can start in one microbatch and end in another microbatch, so the max_running_requests per microbatch should not be strict.
    # Instead, we should always allow chunked request to be added, otherwise, there will be a memory leak.
    if self.get_num_allocatable_reqs(running_bs) <= 0 and not self.chunked_req:
        self.running_batch.batch_is_full = True
        return None

    if self.enable_hierarchical_cache:
        self.tree_cache.check_hicache_events()

    # Get priority queue
    self.policy.calc_priority(self.waiting_queue)

    # Prefill policy
    adder = PrefillAdder(
        self.page_size,
        self.tree_cache,
        self.token_to_kv_pool_allocator,
        self.running_batch,
        self.new_token_ratio,
        self.max_prefill_tokens,
        self.chunked_prefill_size,
        running_bs if self.is_mixed_chunk else 0,
    )

    if self.chunked_req is not None:
        self.chunked_req.init_next_round_input()
        self.chunked_req = adder.add_chunked_req(self.chunked_req)

    if self.enable_lora:
        lora_set = set([req.lora_id for req in self.running_batch.reqs])

    # Get requests from the waiting queue to a new prefill batch
    for req in self.waiting_queue:

        if self.enable_lora and not self.tp_worker.can_run_lora_batch(
            lora_set
            | set([req.lora_id for req in adder.can_run_list])
            | set([req.lora_id])
        ):
            self.running_batch.batch_is_full = True
            break

        if len(adder.can_run_list) >= self.get_num_allocatable_reqs(running_bs):
            self.running_batch.batch_is_full = True
            break

        if self.disaggregation_mode == DisaggregationMode.PREFILL:
            # In prefill mode, prealloc queue and transfer queue can also take memory,
            # so we need to check if the available size for the actual available size.
            if len(adder.can_run_list) >= self.req_to_token_pool.available_size():
                self.running_batch.batch_is_full = True
                break

        if self.enable_hicache_storage:
            prefetch_done = self.tree_cache.check_prefetch_progress(req.rid)
            if not prefetch_done:
                # skip staging requests that are ongoing prefetch
                continue

        req.init_next_round_input(self.tree_cache)
        res = adder.add_one_req(req, has_chunked_req=(self.chunked_req is not None))

        if res != AddReqResult.CONTINUE:
            if res == AddReqResult.NO_TOKEN:
                if self.enable_hierarchical_cache:
                    # Set batch_is_full after making sure there are requests that can be served
                    self.running_batch.batch_is_full = len(
                        adder.can_run_list
                    ) > 0 or (not self.running_batch.is_empty())
                else:
                    self.running_batch.batch_is_full = True
            break

    # Update waiting queue
    can_run_list: List[Req] = adder.can_run_list
    if len(can_run_list) == 0:
        return None

    if self.enable_metrics:
        # only record queue time when enable_metrics is True to avoid overhead
        for req in can_run_list:
            req.queue_time_end = time.perf_counter()

    self.waiting_queue = [
        x for x in self.waiting_queue if x not in set(can_run_list)
    ]

    if adder.new_chunked_req is not None:
        assert self.chunked_req is None
        self.chunked_req = adder.new_chunked_req

    if self.chunked_req:
        self.chunked_req.is_chunked += 1

    if self.current_scheduler_metrics_enabled():
        self.log_prefill_stats(adder, can_run_list, running_bs)

    new_batch = ScheduleBatch.init_new(
        can_run_list,
        self.req_to_token_pool,
        self.token_to_kv_pool_allocator,
        self.tree_cache,
        self.model_config,
        self.enable_overlap,
        self.spec_algorithm,
        chunked_req=self.chunked_req,
    )
    
    if self.enable_hierarchical_cache:
        new_batch.hicache_consumer_index = (
            self.tree_cache.ready_to_load_host_cache()
        )

    new_batch.prepare_for_extend()

    return new_batch
```

---

## 3. PrefillAdderæ™ºèƒ½æ·»åŠ ç­–ç•¥

PrefillAdderæ˜¯SGLangæ‰¹å¤„ç†ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œå®ƒè´Ÿè´£ä»ç­‰å¾…é˜Ÿåˆ—ä¸­æ™ºèƒ½é€‰æ‹©è¯·æ±‚å¹¶æ„å»ºé¢„å¡«å……æ‰¹æ¬¡ã€‚è¿™ä¸ªç»„ä»¶éœ€è¦åœ¨å¤šç§çº¦æŸæ¡ä»¶ä¸‹è¿›è¡Œä¼˜åŒ–å†³ç­–ï¼ŒåŒ…æ‹¬tokené¢„ç®—ç®¡ç†ã€å‰ç¼€ç¼“å­˜åˆ©ç”¨ã€åˆ†å—å¤„ç†æ”¯æŒç­‰å¤æ‚åŠŸèƒ½ã€‚

**æ™ºèƒ½æ·»åŠ çš„æ ¸å¿ƒç®—æ³•**ï¼š
- **tokené¢„ç®—ç®¡ç†**ï¼šåŸºäºnew_token_ratioå’Œmax_prefill_tokensè¿›è¡Œç²¾ç¡®çš„èµ„æºé¢„ç®—
- **å‰ç¼€ç¼“å­˜ä¼˜åŒ–**ï¼šåˆ©ç”¨RadixCacheç­‰ç¼“å­˜æœºåˆ¶å‡å°‘é‡å¤è®¡ç®—å¼€é”€
- **åˆ†å—å¤„ç†æ”¯æŒ**ï¼šå¯¹è¶…é•¿åºåˆ—æä¾›åˆ†å—é¢„å¡«å……ï¼Œçªç ´å•æ¬¡å¤„ç†é™åˆ¶
- **æ··åˆæ‰¹æ¬¡åè°ƒ**ï¼šåœ¨æ··åˆæ¨¡å¼ä¸‹åè°ƒé¢„å¡«å……å’Œè§£ç è¯·æ±‚çš„èµ„æºåˆ†é…

**çº¦æŸæ¡ä»¶çš„ç»¼åˆè€ƒé‡**ï¼š
- **å†…å­˜èµ„æºçº¦æŸ**ï¼šç¡®ä¿tokenåˆ†é…ä¸è¶…è¿‡GPUå†…å­˜é™åˆ¶
- **LoRAé€‚é…å™¨çº¦æŸ**ï¼šæ§åˆ¶æ‰¹æ¬¡ä¸­LoRAé€‚é…å™¨çš„æ•°é‡
- **åˆ†ç¦»å¼æ¶æ„çº¦æŸ**ï¼šåœ¨åˆ†ç¦»å¼æ¨¡å¼ä¸‹è€ƒè™‘ç‰¹æ®Šçš„å†…å­˜ç®¡ç†éœ€æ±‚
- **æ··åˆç¼“å­˜çº¦æŸ**ï¼šåœ¨SWAç­‰æ··åˆç¼“å­˜æ¨¡å¼ä¸‹çš„ç‰¹æ®Šå¤„ç†

### 3.1 PrefillAdderæ ¸å¿ƒå®ç°

PrefillAdderçš„å®ç°ä½“ç°äº†ç°ä»£æ¨ç†ç³»ç»Ÿåœ¨èµ„æºç®¡ç†å’Œæ€§èƒ½ä¼˜åŒ–æ–¹é¢çš„å¤æ‚æ€§ã€‚å®ƒä¸ä»…è¦ç®¡ç†åŸºç¡€çš„tokené¢„ç®—ï¼Œè¿˜è¦å¤„ç†å„ç§é«˜çº§åŠŸèƒ½å¸¦æ¥çš„çº¦æŸæ¡ä»¶ã€‚

```python
class PrefillAdder:
    def __init__(
        self,
        page_size: int,
        tree_cache: BasePrefixCache,
        token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator,
        running_batch: ScheduleBatch,
        new_token_ratio: float,
        rem_input_tokens: int,
        rem_chunk_tokens: Optional[int],
        mixed_with_decode_tokens: int = 0,
    ):
        self.page_size = page_size
        self.tree_cache = tree_cache
        self.token_to_kv_pool_allocator = token_to_kv_pool_allocator
        self.running_batch = running_batch
        self.new_token_ratio = new_token_ratio
        self.rem_input_tokens = rem_input_tokens - mixed_with_decode_tokens
        self.rem_chunk_tokens = rem_chunk_tokens
        if self.rem_chunk_tokens is not None:
            self.rem_chunk_tokens -= mixed_with_decode_tokens

        self.rem_total_token_offset = mixed_with_decode_tokens
        self.cur_rem_token_offset = mixed_with_decode_tokens

        self.req_states = None
        self.can_run_list = []
        self.new_chunked_req = None
        self.log_hit_tokens = 0
        self.log_input_tokens = 0

        # è®¡ç®—è§£ç é˜¶æ®µçš„tokené¢„ç®—å¼€é”€
        if running_batch is not None:
            self.rem_total_token_offset += sum(
                [
                    min(
                        (r.sampling_params.max_new_tokens - len(r.output_ids)),
                        CLIP_MAX_NEW_TOKENS,
                    )
                    * self.new_token_ratio
                    for r in running_batch.reqs
                ]
            )

        # æ£€æŸ¥æ˜¯å¦ä¸ºæ··åˆç¼“å­˜æ¨¡å¼
        self.is_hybrid = isinstance(
            self.token_to_kv_pool_allocator, SWATokenToKVPoolAllocator
        )
```

### 3.2 add_one_reqæ ¸å¿ƒæ–¹æ³•

add_one_reqæ–¹æ³•æ˜¯PrefillAdderçš„æ ¸å¿ƒï¼Œå®ƒå®ç°äº†å•ä¸ªè¯·æ±‚çš„æ™ºèƒ½æ·»åŠ é€»è¾‘ã€‚è¿™ä¸ªæ–¹æ³•éœ€è¦å¤„ç†å‰ç¼€ç¼“å­˜åŒ¹é…ã€tokené¢„ç®—è®¡ç®—ã€åˆ†å—å¤„ç†å†³ç­–ç­‰å¤æ‚é€»è¾‘ã€‚

```python
class PrefillAdder:
    def __init__(
        self,
        page_size: int,
        tree_cache: BasePrefixCache,
        token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator,
        running_batch: ScheduleBatch,
        new_token_ratio: float,
        rem_input_tokens: int,
        rem_chunk_tokens: Optional[int],
        mixed_with_decode_tokens: int = 0,
    ):
        self.page_size = page_size
        self.tree_cache = tree_cache
        self.token_to_kv_pool_allocator = token_to_kv_pool_allocator
        self.running_batch = running_batch
        self.new_token_ratio = new_token_ratio
        self.rem_input_tokens = rem_input_tokens - mixed_with_decode_tokens
        self.rem_chunk_tokens = rem_chunk_tokens
        if self.rem_chunk_tokens is not None:
            self.rem_chunk_tokens -= mixed_with_decode_tokens

        self.rem_total_token_offset = mixed_with_decode_tokens
        self.cur_rem_token_offset = mixed_with_decode_tokens

        self.req_states = None
        self.can_run_list = []
        self.new_chunked_req = None
        self.log_hit_tokens = 0
        self.log_input_tokens = 0

        # è®¡ç®—è§£ç é˜¶æ®µçš„tokené¢„ç®—å¼€é”€
        if running_batch is not None:
            self.rem_total_token_offset += sum(
                [
                    min(
                        (r.sampling_params.max_new_tokens - len(r.output_ids)),
                        CLIP_MAX_NEW_TOKENS,
                    )
                    * self.new_token_ratio
                    for r in running_batch.reqs
                ]
            )

        # æ£€æŸ¥æ˜¯å¦ä¸ºæ··åˆç¼“å­˜æ¨¡å¼
        self.is_hybrid = isinstance(
            self.token_to_kv_pool_allocator, SWATokenToKVPoolAllocator
        )

    def add_one_req(self, req: Req, has_chunked_req: bool):
        # åˆå§‹åŒ–è¯·æ±‚çš„ä¸‹ä¸€è½®è¾“å…¥
        req.init_next_round_input(self.tree_cache)
        
        # ç»Ÿè®¡å‰ç¼€ç¼“å­˜å‘½ä¸­æƒ…å†µ
        prefix_len = len(req.prefix_indices)
        self.log_hit_tokens += prefix_len
        self.log_input_tokens += len(req.fill_ids)

        # è®¡ç®—å®é™…éœ€è¦å¤„ç†çš„input tokens
        input_tokens = req.extend_input_len

        # æ£€æŸ¥æ··åˆæ‰¹æ¬¡ä¸­çš„åˆ†å—è¯·æ±‚å†²çª
        if has_chunked_req and len(self.can_run_list) != 0:
            return AddReqResult.OTHER

        # åˆ†å—é¢„å¡«å……å†³ç­–
        if self.rem_chunk_tokens is None or input_tokens <= self.rem_chunk_tokens:
            # éåˆ†å—é¢„å¡«å……è·¯å¾„
            self.can_run_list.append(req)
            if self.is_hybrid:
                swa_uuid_for_lock = self.tree_cache.inc_lock_ref(req.last_node)
                req.swa_uuid_for_lock = swa_uuid_for_lock
            else:
                self.tree_cache.inc_lock_ref(req.last_node)
            self._update_prefill_budget(
                prefix_len,
                input_tokens,
                min(
                    req.sampling_params.max_new_tokens,
                    CLIP_MAX_NEW_TOKENS,
                ),
            )
        else:
            # åˆ†å—é¢„å¡«å……è·¯å¾„
            trunc_len = self.rem_chunk_tokens - self.page_size + 1
            if trunc_len <= 0:
                return AddReqResult.OTHER

            req.extend_input_len = trunc_len
            req.fill_ids = req.fill_ids[: len(req.prefix_indices) + trunc_len]

            self.can_run_list.append(req)
            self.new_chunked_req = req
            if self.is_hybrid:
                swa_uuid_for_lock = self.tree_cache.inc_lock_ref(req.last_node)
                req.swa_uuid_for_lock = swa_uuid_for_lock
            else:
                self.tree_cache.inc_lock_ref(req.last_node)
            self._update_prefill_budget(prefix_len, trunc_len, 0)

        return self.budget_state()
```

### 3.3 æ·»åŠ ç»“æœæšä¸¾

```python
class AddReqResult(Enum):
    CONTINUE = auto()    # ç»§ç»­æ·»åŠ æ›´å¤šè¯·æ±‚
    NO_TOKEN = auto()    # æ²¡æœ‰å‰©ä½™tokené¢„ç®—
    OTHER = auto()       # å…¶ä»–åœæ­¢åŸå› ï¼ˆå¦‚LoRAé™åˆ¶ã€åˆ†å—å†²çªç­‰ï¼‰
```

---

## 4. æ‰¹æ¬¡æ“ä½œä¸å†…å­˜ç®¡ç†

### 4.1 æ‰¹æ¬¡åˆå¹¶æœºåˆ¶

æ‰¹æ¬¡åˆå¹¶æ˜¯SGLangè¿ç»­æ‰¹å¤„ç†çš„æ ¸å¿ƒæœºåˆ¶ï¼Œå®ƒé€šè¿‡mix_with_runningæ–¹æ³•å®ç°é¢„å¡«å……æ‰¹æ¬¡ä¸è§£ç æ‰¹æ¬¡çš„æ™ºèƒ½åˆå¹¶ã€‚è¿™ä¸ªè¿‡ç¨‹éœ€è¦åè°ƒä¸åŒå‰å‘æ¨¡å¼çš„è¯·æ±‚ï¼Œç¡®ä¿GPUè®¡ç®—çš„é«˜æ•ˆæ‰§è¡Œã€‚

**åˆå¹¶è¿‡ç¨‹çš„æ ¸å¿ƒæ­¥éª¤**ï¼š
- **å‰å‘æ¨¡å¼è®¾ç½®**ï¼šå°†æ‰¹æ¬¡æ¨¡å¼è®¾ç½®ä¸ºMIXEDï¼Œæ”¯æŒé¢„å¡«å……å’Œè§£ç çš„æ··åˆæ‰§è¡Œ
- **è§£ç ä¿¡æ¯å‡†å¤‡**ï¼šä¸ºè¿è¡Œæ‰¹æ¬¡ä¸­çš„è¯·æ±‚è®¾ç½®è§£ç æ‰€éœ€çš„fill_idså’Œextend_input_len
- **å¼ é‡æ•°æ®åˆå¹¶**ï¼šåˆå¹¶input_idsã€out_cache_locç­‰å…³é”®å¼ é‡æ•°æ®
- **é•¿åº¦ä¿¡æ¯æ›´æ–°**ï¼šæ›´æ–°prefix_lensã€extend_lensç­‰é•¿åº¦ç»Ÿè®¡ä¿¡æ¯

```python
def mix_with_running(self, running_batch: "ScheduleBatch"):
    self.forward_mode = ForwardMode.MIXED
    running_bs = running_batch.batch_size()

    for req in running_batch.reqs:
        req.fill_ids = req.origin_input_ids + req.output_ids
        req.extend_input_len = 1

    input_ids = torch.cat([self.input_ids, running_batch.input_ids])
    out_cache_loc = torch.cat([self.out_cache_loc, running_batch.out_cache_loc])

    self.merge_batch(running_batch)
    self.input_ids = input_ids
    self.out_cache_loc = out_cache_loc

    # For overlap scheduler, the output_ids has one step delay
    delta = 0 if self.enable_overlap else -1

    # NOTE: prefix_indices is what has been cached, but we don't cache each decode step
    self.prefix_lens.extend(
        [
            len(r.origin_input_ids) + len(r.output_ids) + delta
            for r in running_batch.reqs
        ]
    )
    self.extend_lens.extend([1] * running_bs)
    self.extend_num_tokens += running_bs
    # TODO (lianmin): Revisit this. It should be seq_len - 1
    self.extend_logprob_start_lens.extend([0] * running_bs)
```

### 4.2 è§£ç é˜¶æ®µå†…å­˜ç®¡ç†

è§£ç é˜¶æ®µçš„å†…å­˜ç®¡ç†éœ€è¦ç²¾ç¡®è®¡ç®—æ¯ä¸ªè¯·æ±‚çš„KVç¼“å­˜éœ€æ±‚ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†é¡µKVç¼“å­˜æ¨¡å¼ä¸‹ï¼Œéœ€è¦å‡†ç¡®é¢„ä¼°æ–°é¡µé¢çš„åˆ†é…éœ€æ±‚ã€‚

```python
def new_page_count_next_decode(self):
    page_size = self.token_to_kv_pool_allocator.page_size
    if page_size == 1:
        return len(self.reqs)
    # In the decoding phase, the length of a request's KV cache should be
    # the total length of the request minus 1
    return (
        sum(1 for req in self.reqs if req.seqlen % page_size == 0)
        if self.enable_overlap
        else sum(1 for req in self.reqs if (req.seqlen - 1) % page_size == 0)
    )

def check_decode_mem(self, buf_multiplier=1):
    num_tokens = self.new_page_count_next_decode()
    available_tokens = self.token_to_kv_pool_allocator.available_size()
    return num_tokens * buf_multiplier <= available_tokens
```

### 4.3 åˆ†å—é¢„å¡«å……å¤„ç†

åˆ†å—é¢„å¡«å……æ˜¯SGLangå¤„ç†è¶…é•¿åºåˆ—çš„å…³é”®æœºåˆ¶ï¼Œå½“è¾“å…¥åºåˆ—è¶…è¿‡å•æ¬¡å¤„ç†èƒ½åŠ›æ—¶ï¼Œç³»ç»Ÿä¼šå°†å…¶åˆ†å‰²æˆå¤šä¸ªchunkè¿›è¡Œå¤„ç†ã€‚

```python
def prepare_for_split_prefill(self):
    # For split prefill, we need to set the forward mode to SPLIT_PREFILL
    self.forward_mode = ForwardMode.SPLIT_PREFILL
```

## 5. è°ƒåº¦ç­–ç•¥ä¸ä¼˜åŒ–

### 5.1 è¿ç»­æ‰¹å¤„ç†ç­–ç•¥

SGLangçš„è¿ç»­æ‰¹å¤„ç†ç­–ç•¥æ˜¯å…¶é«˜æ€§èƒ½çš„å…³é”®æ‰€åœ¨ï¼Œå®ƒé€šè¿‡last_batchã€running_batchã€new_batchçš„åè°ƒç®¡ç†ï¼Œå®ç°äº†æ— ç¼çš„æ‰¹æ¬¡æµæ°´çº¿å¤„ç†ã€‚è¿™ç§ç­–ç•¥ä¸ä»…æœ€å¤§åŒ–äº†GPUåˆ©ç”¨ç‡ï¼Œè¿˜ç¡®ä¿äº†ç³»ç»Ÿçš„é«˜ååé‡ã€‚

**è¿ç»­æ‰¹å¤„ç†çš„æ ¸å¿ƒæœºåˆ¶**ï¼š
- **æ‰¹æ¬¡çŠ¶æ€æµè½¬**ï¼šlast_batch â†’ running_batch â†’ new_batchçš„å¾ªç¯æµè½¬
- **æ™ºèƒ½åˆå¹¶å†³ç­–**ï¼šæ ¹æ®æ‰¹æ¬¡ç±»å‹å’Œç³»ç»ŸçŠ¶æ€å†³å®šåˆå¹¶ç­–ç•¥
- **åˆ†å—è¯·æ±‚åè°ƒ**ï¼šç‰¹æ®Šå¤„ç†è¶…é•¿åºåˆ—çš„åˆ†å—é¢„å¡«å……è¯·æ±‚
- **å†…å­˜çŠ¶æ€åŒæ­¥**ï¼šç¡®ä¿æ‰¹æ¬¡é—´çš„å†…å­˜çŠ¶æ€ä¸€è‡´æ€§

### 5.2 ä¼˜åŒ–ç­–ç•¥é›†æˆ

SGLangçš„æ‰¹å¤„ç†è°ƒåº¦é›†æˆäº†å¤šç§å…ˆè¿›çš„ä¼˜åŒ–ç­–ç•¥ï¼Œå½¢æˆäº†å®Œæ•´çš„æ€§èƒ½ä¼˜åŒ–ä½“ç³»ï¼š

**å‰ç¼€ç¼“å­˜ä¼˜åŒ–**ï¼š
é€šè¿‡RadixCacheç­‰å‰ç¼€ç¼“å­˜æœºåˆ¶ï¼Œç³»ç»Ÿèƒ½å¤Ÿè¯†åˆ«å’Œå¤ç”¨è¯·æ±‚é—´çš„å…¬å…±å‰ç¼€ï¼Œæ˜¾è‘—å‡å°‘é‡å¤è®¡ç®—å¼€é”€ã€‚PrefillAdderåœ¨æ·»åŠ è¯·æ±‚æ—¶ä¼šè‡ªåŠ¨è¿›è¡Œå‰ç¼€åŒ¹é…ï¼Œæœ€å¤§åŒ–ç¼“å­˜åˆ©ç”¨ç‡ã€‚

**LoRAçº¦æŸç®¡ç†**ï¼š
åœ¨å¯ç”¨LoRAçš„åœºæ™¯ä¸‹ï¼Œç³»ç»Ÿä¼šæ£€æŸ¥æ‰¹æ¬¡ä¸­LoRAé€‚é…å™¨çš„æ•°é‡é™åˆ¶ï¼Œç¡®ä¿ä¸è¶…è¿‡max_loras_per_batchçš„é…ç½®ã€‚è¿™ç§çº¦æŸç®¡ç†ä¿è¯äº†LoRAåŠŸèƒ½çš„æ­£ç¡®æ‰§è¡Œã€‚

**åˆ†å±‚ç¼“å­˜é›†æˆ**ï¼š
ä¸HiCacheç­‰åˆ†å±‚ç¼“å­˜ç³»ç»Ÿçš„æ·±åº¦é›†æˆï¼Œæ”¯æŒCPU-GPUé—´çš„æ™ºèƒ½æ•°æ®é¢„å–å’Œç¼“å­˜ç®¡ç†ã€‚ç³»ç»Ÿä¼šæ£€æŸ¥é¢„å–è¿›åº¦ï¼Œç¡®ä¿æ•°æ®çš„åŠæ—¶å¯ç”¨æ€§ã€‚

**åˆ†ç¦»å¼æ¶æ„æ”¯æŒ**ï¼š
ä¸ºé¢„å¡«å……/è§£ç åˆ†ç¦»éƒ¨ç½²æä¾›ä¸“é—¨çš„æ‰¹å¤„ç†ç­–ç•¥ï¼ŒåŒ…æ‹¬ç‰¹æ®Šçš„å†…å­˜æ£€æŸ¥å’Œèµ„æºç®¡ç†æœºåˆ¶ã€‚

**å†…å­˜é¢„ç®—æ§åˆ¶**ï¼š
åŸºäºtokené¢„ç®—å’Œnew_token_ratioè¿›è¡Œç²¾ç¡®çš„å†…å­˜ç®¡ç†ï¼Œç¡®ä¿ç³»ç»Ÿåœ¨èµ„æºçº¦æŸä¸‹çš„ç¨³å®šè¿è¡Œã€‚

---

## 6. æŠ€æœ¯æ€»ç»“

SGLangçš„æ‰¹å¤„ç†è°ƒåº¦ç­–ç•¥å±•ç°äº†ç°ä»£å¤§è¯­è¨€æ¨¡å‹æ¨ç†ç³»ç»Ÿåœ¨è°ƒåº¦ç®—æ³•å’Œèµ„æºç®¡ç†æ–¹é¢çš„æŠ€æœ¯æ°´å‡†ã€‚é€šè¿‡åŠ¨æ€æ‰¹å¤„ç†ã€æ™ºèƒ½è¯·æ±‚æ·»åŠ ã€ç²¾ç»†å†…å­˜ç®¡ç†ç­‰æ ¸å¿ƒæœºåˆ¶ï¼ŒSGLangå®ç°äº†é«˜æ•ˆçš„GPUèµ„æºåˆ©ç”¨å’Œä¼˜å¼‚çš„æ¨ç†æ€§èƒ½ã€‚

**æ ¸å¿ƒæŠ€æœ¯ä»·å€¼**ï¼š
- **åŠ¨æ€æ‰¹å¤„ç†**ï¼šé€šè¿‡get_next_batch_to_runå®ç°é¢„å¡«å……å’Œè§£ç æ‰¹æ¬¡çš„æ™ºèƒ½åˆå¹¶
- **æ™ºèƒ½è¯·æ±‚é€‰æ‹©**ï¼šPrefillAdderåŸºäºå¤šç»´åº¦çº¦æŸè¿›è¡Œä¼˜åŒ–çš„è¯·æ±‚é€‰æ‹©
- **ç²¾ç»†å†…å­˜ç®¡ç†**ï¼šåˆ†é¡µKVç¼“å­˜å’Œç²¾ç¡®çš„å†…å­˜é¢„ç®—æ§åˆ¶
- **åˆ†å—å¤„ç†èƒ½åŠ›**ï¼šæ”¯æŒè¶…é•¿åºåˆ—çš„åˆ†å—é¢„å¡«å……å¤„ç†

**å·¥ç¨‹å®è·µæ„ä¹‰**ï¼š
è¿™äº›æ‰¹å¤„ç†ç­–ç•¥ä¸ºå¤§è§„æ¨¡æ¨ç†æœåŠ¡æä¾›äº†ç¨³å®šçš„è°ƒåº¦åŸºç¡€ï¼Œåœ¨ä¿è¯ç³»ç»Ÿç¨³å®šæ€§çš„åŒæ—¶ï¼Œæœ€å¤§åŒ–äº†GPUèµ„æºçš„åˆ©ç”¨æ•ˆç‡ã€‚

**æ‰¿ä¸Šå¯ä¸‹**ï¼šåœ¨å‰é¢ç« èŠ‚ä¸­æˆ‘ä»¬äº†è§£äº†è°ƒåº¦å™¨çš„æ¶æ„ã€æ•°æ®ç»“æ„å’Œè¯·æ±‚å¤„ç†æœºåˆ¶ï¼Œæœ¬ç« æ·±å…¥å‰–æäº†æ‰¹å¤„ç†è°ƒåº¦çš„æ ¸å¿ƒç®—æ³•ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬å°†æ¢è®¨å†…å­˜ç®¡ç†ç³»ç»Ÿï¼Œäº†è§£SGLangå¦‚ä½•å®ç°é«˜æ•ˆçš„KVç¼“å­˜ç®¡ç†å’Œå‰ç¼€ç¼“å­˜ä¼˜åŒ–ã€‚