# æ‰¹å¤„ç†è°ƒåº¦ç­–ç•¥

---

SGLangè°ƒåº¦å™¨é€šè¿‡æ™ºèƒ½çš„æ‰¹å¤„ç†æœºåˆ¶æ¥æœ€å¤§åŒ–GPUåˆ©ç”¨ç‡å’Œæ¨ç†ååé‡ã€‚æœ¬ç« æ·±å…¥ä»‹ç»è°ƒåº¦å™¨çš„æ ¸å¿ƒæ‰¹å¤„ç†ç­–ç•¥ã€è¿ç»­æ‰¹å¤„ç†å®ç°å’ŒåŠ¨æ€è°ƒåº¦ç®—æ³•ï¼Œæ­ç¤ºSGLangå¦‚ä½•å®ç°é«˜æ•ˆçš„æ··åˆé¢„å¡«å……-è§£ç æ‰¹å¤„ç†ã€‚

---

## 1. æ‰¹å¤„ç†æ¶æ„æ€»è§ˆ

SGLangçš„æ‰¹å¤„ç†è°ƒåº¦ç³»ç»Ÿæ˜¯å…¶é«˜æ€§èƒ½æ¨ç†çš„æ ¸å¿ƒå¼•æ“ï¼Œå®ƒé€šè¿‡æ™ºèƒ½çš„æ‰¹æ¬¡ç®¡ç†å’ŒåŠ¨æ€è°ƒåº¦ç®—æ³•ï¼Œå®ç°äº†é¢„å¡«å……å’Œè§£ç é˜¶æ®µçš„é«˜æ•ˆåè°ƒã€‚è¿™ä¸ªç³»ç»Ÿä¸ä»…è¦å¤„ç†å•ä¸€ç±»å‹çš„æ‰¹æ¬¡ï¼Œè¿˜è¦æ”¯æŒæ··åˆæ‰¹æ¬¡ã€åˆ†å—é¢„å¡«å……ã€è¿ç»­æ‰¹å¤„ç†ç­‰å¤æ‚åœºæ™¯ã€‚

**æ‰¹å¤„ç†ç³»ç»Ÿçš„æ ¸å¿ƒæŒ‘æˆ˜**ï¼š
- **å¼‚æ„è¯·æ±‚ç®¡ç†**ï¼šä¸åŒé•¿åº¦ã€ä¸åŒé˜¶æ®µçš„è¯·æ±‚éœ€è¦ç»Ÿä¸€çš„æ‰¹å¤„ç†æ¡†æ¶
- **å†…å­˜æ•ˆç‡ä¼˜åŒ–**ï¼šåœ¨æœ‰é™çš„GPUå†…å­˜ä¸­æœ€å¤§åŒ–æ‰¹æ¬¡å¤§å°å’Œå¤„ç†æ•ˆç‡
- **åŠ¨æ€è´Ÿè½½å¹³è¡¡**ï¼šæ ¹æ®ç³»ç»ŸçŠ¶æ€åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡ç»„æˆå’Œæ‰§è¡Œç­–ç•¥
- **å¤šæ¨¡å¼åè°ƒ**ï¼šåè°ƒé¢„å¡«å……ã€è§£ç ã€æ··åˆç­‰å¤šç§å‰å‘æ¨¡å¼çš„æ‰§è¡Œ

**è°ƒåº¦ç­–ç•¥çš„è®¾è®¡åŸåˆ™**ï¼š
- **è¿ç»­æ‰¹å¤„ç†**ï¼šé€šè¿‡last_batchå’Œrunning_batchçš„åè°ƒï¼Œå®ç°æ— ç¼çš„è¿ç»­å¤„ç†
- **æ™ºèƒ½åˆå¹¶æœºåˆ¶**ï¼šåŠ¨æ€å†³å®šæ˜¯å¦åˆå¹¶é¢„å¡«å……å’Œè§£ç æ‰¹æ¬¡ï¼Œä¼˜åŒ–GPUåˆ©ç”¨ç‡
- **åˆ†å—å¤„ç†æ”¯æŒ**ï¼šå¯¹è¶…é•¿åºåˆ—æä¾›åˆ†å—é¢„å¡«å……æ”¯æŒï¼Œçªç ´å•æ¬¡å¤„ç†çš„é•¿åº¦é™åˆ¶
- **å‰ç¼€ç¼“å­˜é›†æˆ**ï¼šä¸RadixCacheç­‰å‰ç¼€ç¼“å­˜ç³»ç»Ÿæ·±åº¦é›†æˆï¼Œå‡å°‘é‡å¤è®¡ç®—

**çŠ¶æ€ç®¡ç†çš„å¤æ‚æ€§**ï¼š
æ‰¹å¤„ç†è°ƒåº¦éœ€è¦ç®¡ç†å¤šä¸ªæ‰¹æ¬¡çŠ¶æ€ï¼ˆlast_batchã€running_batchã€chunked_reqï¼‰ï¼Œæ¯ä¸ªçŠ¶æ€éƒ½æœ‰å…¶ç‰¹å®šçš„ç”Ÿå‘½å‘¨æœŸå’Œè½¬æ¢è§„åˆ™ã€‚è¿™ç§è®¾è®¡ç¡®ä¿äº†ç³»ç»Ÿèƒ½å¤Ÿåœ¨å¤æ‚çš„è°ƒåº¦åœºæ™¯ä¸‹ä¿æŒçŠ¶æ€ä¸€è‡´æ€§ã€‚

### 1.1 æ‰¹å¤„ç†è°ƒåº¦æ—¶åºå›¾

```mermaid
sequenceDiagram
    participant S as Scheduler
    participant LB as last_batch
    participant RB as running_batch
    participant PA as PrefillAdder
    participant TC as TreeCache
    participant WQ as waiting_queue

    Note over S: get_next_batch_to_run() å¼€å§‹
    
    S->>S: å¤„ç†chunked_reqæ’é™¤é€»è¾‘
    alt å­˜åœ¨chunked_req
        S->>TC: cache_unfinished_req(chunked_req)
        S->>S: req_to_token_pool.free()
    end
    
    alt last_batchå­˜åœ¨ä¸”ä¸ºEXTENDæ¨¡å¼
        S->>LB: filter_batch(chunked_req_to_exclude)
        LB-->>S: è¿‡æ»¤åçš„æ‰¹æ¬¡å¤§å°
        alt æ‰¹æ¬¡å¤§å°å‡å°‘
            S->>RB: batch_is_full = False
        end
        alt éç©ºä¸”éä»…é¢„å¡«å……
            alt running_batchä¸ºç©º
                S->>S: running_batch = last_batch
            else
                S->>RB: merge_batch(last_batch)
            end
        end
    end
    
    S->>S: get_new_batch_prefill()
    Note over S: è·å–æ–°é¢„å¡«å……æ‰¹æ¬¡
    
    S->>S: æ£€æŸ¥grammar_queue
    S->>S: éªŒè¯é¢„å¡«å……æ¡ä»¶
    S->>S: è®¡ç®—ä¼˜å…ˆçº§
    S->>PA: åˆ›å»ºPrefillAdder
    
    loop éå†waiting_queue
        S->>PA: add_one_req(req)
        PA->>TC: å‰ç¼€ç¼“å­˜åŒ¹é…
        PA->>PA: tokené¢„ç®—æ£€æŸ¥
        alt é¢„ç®—å……è¶³
            PA-->>S: AddReqResult.CONTINUE
        else
            PA-->>S: AddReqResult.NO_TOKEN/OTHER
            break
        end
    end
    
    S->>S: åˆ›å»ºScheduleBatch.init_new()
    
    alt new_batchå­˜åœ¨
        alt running_batchä¸ºç©º
            S-->>S: return new_batch
        else
            S->>S: new_batch.mix_with_running(running_batch)
            S-->>S: return mixed_batch
        end
    else
        S-->>S: return running_batchæˆ–None
    end
```

### 1.2 æ‰¹æ¬¡çŠ¶æ€æœºå›¾

```mermaid
stateDiagram-v2
    [*] --> Empty: ç³»ç»Ÿå¯åŠ¨
    
    Empty --> PrefillOnly: è·å–é¢„å¡«å……æ‰¹æ¬¡
    PrefillOnly --> Mixed: åˆå¹¶è§£ç è¯·æ±‚
    PrefillOnly --> DecodeOnly: é¢„å¡«å……å®Œæˆ
    
    DecodeOnly --> Mixed: è·å–æ–°é¢„å¡«å……æ‰¹æ¬¡
    DecodeOnly --> Empty: æ‰€æœ‰è¯·æ±‚å®Œæˆ
    
    Mixed --> DecodeOnly: é¢„å¡«å……éƒ¨åˆ†å®Œæˆ
    Mixed --> Mixed: æŒç»­æ··åˆå¤„ç†
    Mixed --> Empty: æ‰€æœ‰è¯·æ±‚å®Œæˆ
    
    state PrefillOnly {
        [*] --> Extending
        Extending --> ChunkedPrefill: åºåˆ—è¿‡é•¿
        ChunkedPrefill --> Extending: åˆ†å—å®Œæˆ
        Extending --> [*]: é¢„å¡«å……å®Œæˆ
    }
    
    state DecodeOnly {
        [*] --> Decoding
        Decoding --> [*]: ç”Ÿæˆå®Œæˆ
    }
    
    state Mixed {
        [*] --> MixedForward
        MixedForward --> [*]: æ··åˆå¤„ç†å®Œæˆ
    }

    note right of PrefillOnly: EXTEND/SPLIT_PREFILLæ¨¡å¼
    note right of DecodeOnly: DECODEæ¨¡å¼  
    note right of Mixed: MIXEDæ¨¡å¼
```

### 1.3 å†…å­˜åˆ†é…æ˜ å°„å›¾

```mermaid
graph TD
    subgraph "ğŸ” é€»è¾‘è§†å›¾"
        R1["Req1<br/>seq_len=100"]
        R2["Req2<br/>seq_len=200"] 
        R3["Req3<br/>seq_len=150"]
    end

    subgraph "ğŸ“Š å†…å­˜æ± æ˜ å°„"
        RTT["ReqToTokenPool<br/>è¯·æ±‚â†’Tokenæ˜ å°„"]
        TKV["TokenToKVPool<br/>Tokenâ†’KVæ˜ å°„"]
    end

    subgraph "ğŸ’¾ ç‰©ç†å†…å­˜"
        direction TB
        P1["Page0<br/>KV Cache"]
        P2["Page1<br/>KV Cache"]
        P3["Page2<br/>KV Cache"]
        P4["Page3<br/>KV Cache"]
        P5["Page4<br/>KV Cache"]
        P6["..."]
    end

    R1 --> RTT
    R2 --> RTT  
    R3 --> RTT
    RTT --> TKV
    
    TKV --> P1
    TKV --> P2
    TKV --> P3
    TKV --> P4
    TKV --> P5

    style R1 fill:#e3f2fd,color:#000000,stroke:#333
    style R2 fill:#f1f8e9,color:#000000,stroke:#333
    style R3 fill:#fff3e0,color:#000000,stroke:#333
    style RTT fill:#ffebee,color:#000000,stroke:#333
    style TKV fill:#f3e5f5,color:#000000,stroke:#333
    style P1 fill:#e8f5e8,color:#000000,stroke:#333
    style P2 fill:#e8f5e8,color:#000000,stroke:#333
    style P3 fill:#e8f5e8,color:#000000,stroke:#333
    style P4 fill:#e8f5e8,color:#000000,stroke:#333
    style P5 fill:#e8f5e8,color:#000000,stroke:#333
```

**å›¾ç¤ºè¯´æ˜**ï¼š
- **æ—¶åºå›¾**ï¼šå±•ç¤ºäº†get_next_batch_to_runçš„å®Œæ•´æ‰§è¡Œæµç¨‹å’Œç»„ä»¶äº¤äº’
- **çŠ¶æ€æœºå›¾**ï¼šæè¿°äº†æ‰¹æ¬¡çŠ¶æ€çš„è½¬æ¢é€»è¾‘å’Œå‰å‘æ¨¡å¼çš„åˆ‡æ¢
- **å†…å­˜æ˜ å°„å›¾**ï¼šå±•ç¤ºäº†è¯·æ±‚åˆ°ç‰©ç†å†…å­˜çš„å¤šå±‚æ˜ å°„å…³ç³»

---

## 2. æ ¸å¿ƒæ‰¹å¤„ç†æ–¹æ³•

### 2.1 get_next_batch_to_runæ ¸å¿ƒå®ç°

get_next_batch_to_runæ–¹æ³•æ˜¯SGLangæ‰¹å¤„ç†è°ƒåº¦çš„æ ¸å¿ƒå¼•æ“ï¼Œå®ƒè´Ÿè´£åè°ƒå¤šä¸ªæ‰¹æ¬¡çŠ¶æ€çš„è½¬æ¢å’Œåˆå¹¶ã€‚è¿™ä¸ªæ–¹æ³•éœ€è¦å¤„ç†å¤æ‚çš„æ‰¹æ¬¡ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ŒåŒ…æ‹¬åˆ†å—è¯·æ±‚çš„ç‰¹æ®Šå¤„ç†ã€ä¸Šä¸€æ‰¹æ¬¡çš„åˆå¹¶ã€æ–°é¢„å¡«å……æ‰¹æ¬¡çš„è·å–ç­‰å¤šä¸ªç¯èŠ‚ã€‚

**æ‰¹æ¬¡è°ƒåº¦çš„æ ¸å¿ƒèŒè´£**ï¼š
- **åˆ†å—è¯·æ±‚ç®¡ç†**ï¼šå¤„ç†è¶…é•¿åºåˆ—çš„åˆ†å—é¢„å¡«å……ï¼Œç¡®ä¿åˆ†å—è¯·æ±‚çš„æ­£ç¡®çŠ¶æ€è½¬æ¢
- **æ‰¹æ¬¡çŠ¶æ€åè°ƒ**ï¼šç®¡ç†last_batchåˆ°running_batchçš„è½¬æ¢ï¼Œå®ç°è¿ç»­æ‰¹å¤„ç†
- **åŠ¨æ€åˆå¹¶å†³ç­–**ï¼šæ ¹æ®ç³»ç»ŸçŠ¶æ€å†³å®šæ˜¯å¦åˆå¹¶é¢„å¡«å……å’Œè§£ç æ‰¹æ¬¡
- **æŠ•æœºè§£ç å…¼å®¹**ï¼šå¤„ç†æŠ•æœºè§£ç ä¸æ•°æ®å¹¶è¡Œæ³¨æ„åŠ›çš„å…¼å®¹æ€§é—®é¢˜

**çŠ¶æ€è½¬æ¢çš„å¤æ‚æ€§**ï¼š
- **åˆ†å—è¯·æ±‚æ’é™¤**ï¼šéœ€è¦å°†å®Œæˆçš„åˆ†å—è¯·æ±‚ä»æ‰¹æ¬¡ä¸­ç§»é™¤ï¼ŒåŒæ—¶ç¼“å­˜æœªå®Œæˆçš„éƒ¨åˆ†
- **æ‰¹æ¬¡è¿‡æ»¤æœºåˆ¶**ï¼šè¿‡æ»¤æ‰éœ€è¦æ’é™¤çš„è¯·æ±‚ï¼Œæ›´æ–°æ‰¹æ¬¡çš„æ»¡è½½çŠ¶æ€
- **å†…å­˜æ± ç®¡ç†**ï¼šåŠæ—¶é‡Šæ”¾åˆ†å—è¯·æ±‚å ç”¨çš„å†…å­˜æ± æ§½ä½ï¼Œä¸ºæ–°è¯·æ±‚è…¾å‡ºç©ºé—´

**åˆå¹¶ç­–ç•¥ä¼˜åŒ–**ï¼š
ç³»ç»Ÿä¼šæ ¹æ®æ‰¹æ¬¡ç±»å‹æ™ºèƒ½å†³å®šåˆå¹¶ç­–ç•¥ã€‚å¯¹äºä»…é¢„å¡«å……æ‰¹æ¬¡ï¼ˆis_prefill_onlyï¼‰ï¼Œå¯ä»¥è·³è¿‡è§£ç é˜¶æ®µç›´æ¥å¤„ç†ï¼›å¯¹äºæ··åˆæ‰¹æ¬¡ï¼Œéœ€è¦è€ƒè™‘é¢„å¡«å……å’Œè§£ç çš„åè°ƒæ‰§è¡Œã€‚

```python
def get_next_batch_to_run(self) -> Optional[ScheduleBatch]:
    # Merge the prefill batch into the running batch
    chunked_req_to_exclude = set()
    if self.chunked_req:
        # Move the chunked request out of the batch so that we can merge
        # only finished requests to running_batch.
        chunked_req_to_exclude.add(self.chunked_req)
        self.tree_cache.cache_unfinished_req(self.chunked_req)
        # chunked request keeps its rid but will get a new req_pool_idx
        self.req_to_token_pool.free(self.chunked_req.req_pool_idx)
        
    if self.last_batch and self.last_batch.forward_mode.is_extend():
        if self.last_batch.chunked_req is not None:
            # In the context pipeline parallelism, after the last chunk, the current microbatch still track outdated chunked_req.
            # We need to discard it.
            chunked_req_to_exclude.add(self.last_batch.chunked_req)

        # Filter batch
        last_bs = self.last_batch.batch_size()
        self.last_batch.filter_batch(
            chunked_req_to_exclude=list(chunked_req_to_exclude)
        )
        if self.last_batch.batch_size() < last_bs:
            self.running_batch.batch_is_full = False

        # Merge the new batch into the running batch.
        # For prefill-only batch, we can avoid going through decoding step.
        if not self.last_batch.is_empty() and not self.last_batch.is_prefill_only:
            if self.running_batch.is_empty():
                self.running_batch = self.last_batch
            else:
                # Merge running_batch with prefill batch
                self.running_batch.merge_batch(self.last_batch)

    new_batch = self.get_new_batch_prefill()
    
    need_dp_attn_preparation = require_mlp_sync(self.server_args)
    
    if need_dp_attn_preparation and not self.spec_algorithm.is_none():
        if new_batch is not None:
            new_batch.spec_algorithm = SpeculativeAlgorithm.NONE
        if not self.running_batch.is_empty():
            self.running_batch.spec_algorithm = SpeculativeAlgorithm.NONE

    # Return the next batch to run
    if new_batch is not None:
        if self.running_batch.is_empty():
            return new_batch
        else:
            new_batch.mix_with_running(self.running_batch)
            return new_batch
    else:
        return self.running_batch if not self.running_batch.is_empty() else None
```

### 2.2 get_new_batch_prefillå®ç°

get_new_batch_prefillæ–¹æ³•æ˜¯é¢„å¡«å……æ‰¹æ¬¡æ„å»ºçš„æ ¸å¿ƒç»„ä»¶ï¼Œå®ƒä»ç­‰å¾…é˜Ÿåˆ—ä¸­æ™ºèƒ½é€‰æ‹©è¯·æ±‚å¹¶æ„å»ºæ–°çš„é¢„å¡«å……æ‰¹æ¬¡ã€‚è¿™ä¸ªæ–¹æ³•éœ€è¦è€ƒè™‘å¤šç§çº¦æŸæ¡ä»¶ï¼ŒåŒ…æ‹¬å†…å­˜é™åˆ¶ã€LoRAçº¦æŸã€è¯­æ³•é˜Ÿåˆ—çŠ¶æ€ã€åˆ†å±‚ç¼“å­˜ç­‰å¤æ‚å› ç´ ã€‚

**é¢„å¡«å……æ‰¹æ¬¡æ„å»ºçš„æ ¸å¿ƒæµç¨‹**ï¼š
- **è¯­æ³•é˜Ÿåˆ—æ£€æŸ¥**ï¼šä¼˜å…ˆå¤„ç†è¯­æ³•é˜Ÿåˆ—ä¸­å·²å‡†å¤‡å¥½çš„è¯·æ±‚
- **å®¹é‡é¢„æ£€æŸ¥**ï¼šæ£€æŸ¥è¿è¡Œæ‰¹æ¬¡æ˜¯å¦å·²æ»¡ï¼Œé¿å…ä¸å¿…è¦çš„å¤„ç†å¼€é”€
- **èµ„æºå¯ç”¨æ€§éªŒè¯**ï¼šç¡®ä¿æœ‰è¶³å¤Ÿçš„å†…å­˜å’Œè¯·æ±‚æ§½ä½æ¥æ„å»ºæ–°æ‰¹æ¬¡
- **ä¼˜å…ˆçº§è®¡ç®—**ï¼šé€šè¿‡è°ƒåº¦ç­–ç•¥è®¡ç®—è¯·æ±‚çš„å¤„ç†ä¼˜å…ˆçº§

**çº¦æŸæ¡ä»¶çš„å¤šç»´åº¦æ£€æŸ¥**ï¼š
- **LoRAæ‰¹æ¬¡é™åˆ¶**ï¼šæ£€æŸ¥LoRAé€‚é…å™¨çš„æ•°é‡æ˜¯å¦è¶…è¿‡æ‰¹æ¬¡é™åˆ¶
- **å†…å­˜èµ„æºé™åˆ¶**ï¼šéªŒè¯å¯åˆ†é…çš„è¯·æ±‚æ•°é‡å’Œtokenæ•°é‡
- **åˆ†ç¦»å¼æ¶æ„é™åˆ¶**ï¼šåœ¨åˆ†ç¦»å¼æ¨¡å¼ä¸‹æ£€æŸ¥ç‰¹å®šçš„èµ„æºçº¦æŸ
- **åˆ†å±‚ç¼“å­˜çŠ¶æ€**ï¼šæ£€æŸ¥HiCacheçš„é¢„å–è¿›åº¦å’Œå¯ç”¨æ€§

**PrefillAdderé›†æˆ**ï¼š
æ–¹æ³•é€šè¿‡PrefillAdderå®ç°æ™ºèƒ½çš„è¯·æ±‚æ·»åŠ ï¼Œè¯¥ç»„ä»¶è´Ÿè´£tokené¢„ç®—ç®¡ç†ã€å‰ç¼€ç¼“å­˜ä¼˜åŒ–ã€åˆ†å—å¤„ç†ç­‰æ ¸å¿ƒåŠŸèƒ½ã€‚PrefillAdderçš„è®¾è®¡ç¡®ä¿äº†æ‰¹æ¬¡æ„å»ºè¿‡ç¨‹çš„é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

### 2.2 é¢„å¡«å……æ‰¹æ¬¡æ„å»ºæµç¨‹

é¢„å¡«å……æ‰¹æ¬¡çš„æ„å»ºæ˜¯ä¸€ä¸ªå¤æ‚çš„å†³ç­–è¿‡ç¨‹ï¼Œæ¶‰åŠå¤šå±‚çº¦æŸæ£€æŸ¥å’Œä¼˜åŒ–ç­–ç•¥ã€‚ä»¥ä¸‹æ˜¯æ ¸å¿ƒæµç¨‹çš„ç²¾ç®€å®ç°ï¼š

```python
def get_new_batch_prefill(self) -> Optional[ScheduleBatch]:
    # 1. è¯­æ³•é˜Ÿåˆ—ä¼˜å…ˆå¤„ç†
    if self.grammar_queue:
        self.move_ready_grammar_requests()

    # 2. é¢„å¡«å……æ¡ä»¶æ£€æŸ¥
    if (self.running_batch.batch_is_full or len(self.waiting_queue) == 0) and self.chunked_req is None:
        return None

    # 3. åˆ›å»ºPrefillAdderè¿›è¡Œæ™ºèƒ½è¯·æ±‚é€‰æ‹©
    adder = PrefillAdder(
        self.page_size,                    # åˆ†é¡µå¤§å°
        self.tree_cache,                   # å‰ç¼€ç¼“å­˜
        self.token_to_kv_pool_allocator,   # KVç¼“å­˜åˆ†é…å™¨
        self.running_batch,                # å½“å‰è¿è¡Œæ‰¹æ¬¡
        self.new_token_ratio,              # æ–°tokenæ¯”ç‡
        self.max_prefill_tokens,           # æœ€å¤§é¢„å¡«å……tokenæ•°
        self.chunked_prefill_size,         # åˆ†å—é¢„å¡«å……å¤§å°
        running_bs if self.is_mixed_chunk else 0,  # æ··åˆæ¨¡å¼ä¸‹çš„è§£ç tokenæ•°
    )

    # 4. å¤„ç†åˆ†å—è¯·æ±‚ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if self.chunked_req is not None:
        self.chunked_req.init_next_round_input()
        self.chunked_req = adder.add_chunked_req(self.chunked_req)

    # 5. éå†ç­‰å¾…é˜Ÿåˆ—ï¼Œåº”ç”¨å¤šç»´åº¦çº¦æŸ
    for req in self.waiting_queue:
        # LoRAçº¦æŸæ£€æŸ¥
        if self.enable_lora and not self.tp_worker.can_run_lora_batch(...):
            self.running_batch.batch_is_full = True
            break

        # è¯·æ±‚æ•°é‡é™åˆ¶æ£€æŸ¥
        if len(adder.can_run_list) >= self.get_num_allocatable_reqs(running_bs):
            self.running_batch.batch_is_full = True
            break

        # æ·»åŠ è¯·æ±‚åˆ°æ‰¹æ¬¡
        req.init_next_round_input(self.tree_cache)
        res = adder.add_one_req(req, has_chunked_req=(self.chunked_req is not None))
        
        if res != AddReqResult.CONTINUE:
            break

    # 6. æ„å»ºå¹¶è¿”å›æ–°æ‰¹æ¬¡
    if len(adder.can_run_list) == 0:
        return None
        
    new_batch = ScheduleBatch.init_new(
        adder.can_run_list,              # é€‰ä¸­çš„è¯·æ±‚åˆ—è¡¨
        self.req_to_token_pool,          # å†…å­˜æ± å¼•ç”¨
        self.token_to_kv_pool_allocator, # KVç¼“å­˜åˆ†é…å™¨
        self.tree_cache,                 # å‰ç¼€ç¼“å­˜
        # ... å…¶ä»–é…ç½®å‚æ•°
    )
    
    new_batch.prepare_for_extend()  # å‡†å¤‡é¢„å¡«å……æ‰§è¡Œ
    return new_batch
```

---

## 3. PrefillAdderæ™ºèƒ½æ·»åŠ ç­–ç•¥

PrefillAdderæ˜¯SGLangæ‰¹å¤„ç†ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œå®ƒè´Ÿè´£ä»ç­‰å¾…é˜Ÿåˆ—ä¸­æ™ºèƒ½é€‰æ‹©è¯·æ±‚å¹¶æ„å»ºé¢„å¡«å……æ‰¹æ¬¡ã€‚è¿™ä¸ªç»„ä»¶éœ€è¦åœ¨å¤šç§çº¦æŸæ¡ä»¶ä¸‹è¿›è¡Œä¼˜åŒ–å†³ç­–ï¼ŒåŒ…æ‹¬tokené¢„ç®—ç®¡ç†ã€å‰ç¼€ç¼“å­˜åˆ©ç”¨ã€åˆ†å—å¤„ç†æ”¯æŒç­‰å¤æ‚åŠŸèƒ½ã€‚

**æ™ºèƒ½æ·»åŠ çš„æ ¸å¿ƒç®—æ³•**ï¼š
- **tokené¢„ç®—ç®¡ç†**ï¼šåŸºäºnew_token_ratioå’Œmax_prefill_tokensè¿›è¡Œç²¾ç¡®çš„èµ„æºé¢„ç®—
- **å‰ç¼€ç¼“å­˜ä¼˜åŒ–**ï¼šåˆ©ç”¨RadixCacheç­‰ç¼“å­˜æœºåˆ¶å‡å°‘é‡å¤è®¡ç®—å¼€é”€
- **åˆ†å—å¤„ç†æ”¯æŒ**ï¼šå¯¹è¶…é•¿åºåˆ—æä¾›åˆ†å—é¢„å¡«å……ï¼Œçªç ´å•æ¬¡å¤„ç†é™åˆ¶
- **æ··åˆæ‰¹æ¬¡åè°ƒ**ï¼šåœ¨æ··åˆæ¨¡å¼ä¸‹åè°ƒé¢„å¡«å……å’Œè§£ç è¯·æ±‚çš„èµ„æºåˆ†é…

**çº¦æŸæ¡ä»¶çš„ç»¼åˆè€ƒé‡**ï¼š
- **å†…å­˜èµ„æºçº¦æŸ**ï¼šç¡®ä¿tokenåˆ†é…ä¸è¶…è¿‡GPUå†…å­˜é™åˆ¶
- **LoRAé€‚é…å™¨çº¦æŸ**ï¼šæ§åˆ¶æ‰¹æ¬¡ä¸­LoRAé€‚é…å™¨çš„æ•°é‡
- **åˆ†ç¦»å¼æ¶æ„çº¦æŸ**ï¼šåœ¨åˆ†ç¦»å¼æ¨¡å¼ä¸‹è€ƒè™‘ç‰¹æ®Šçš„å†…å­˜ç®¡ç†éœ€æ±‚
- **æ··åˆç¼“å­˜çº¦æŸ**ï¼šåœ¨SWAç­‰æ··åˆç¼“å­˜æ¨¡å¼ä¸‹çš„ç‰¹æ®Šå¤„ç†

### 3.1 è°ƒåº¦å†³ç­–ç®—æ³•

PrefillAdderçš„æ ¸å¿ƒæ˜¯åŸºäºå¤šç»´åº¦çº¦æŸçš„æ™ºèƒ½å†³ç­–ç®—æ³•ï¼Œå®ƒéœ€è¦åœ¨tokené¢„ç®—ã€å†…å­˜é™åˆ¶ã€LoRAçº¦æŸã€ç¼“å­˜çŠ¶æ€ç­‰å¤šä¸ªå› ç´ é—´æ‰¾åˆ°æœ€ä¼˜å¹³è¡¡ã€‚

**å†³ç­–ç®—æ³•çš„æ ¸å¿ƒé€»è¾‘**ï¼š

```mermaid
graph TD
    subgraph "ğŸ¯ å†³ç­–æµç¨‹"
        A1["æ–°è¯·æ±‚è¿›å…¥"]
        A2["å‰ç¼€ç¼“å­˜åŒ¹é…<br/>tree_cache.match_prefix()"]
        A3["è®¡ç®—tokenéœ€æ±‚<br/>input_tokens = len(fill_ids) - prefix_len"]
        A4{"tokené¢„ç®—æ£€æŸ¥<br/>rem_total_tokens > 0?"}
        A5{"åˆ†å—å†³ç­–<br/>input_tokens > rem_chunk_tokens?"}
        A6["æ·»åŠ åˆ°can_run_list"]
        A7["åˆ›å»ºåˆ†å—è¯·æ±‚<br/>new_chunked_req"]
        A8["æ›´æ–°tokené¢„ç®—<br/>_update_prefill_budget()"]
        A9["è¿”å›AddReqResult"]
    end

    A1 --> A2
    A2 --> A3
    A3 --> A4
    A4 -->|æ˜¯| A5
    A4 -->|å¦| A9
    A5 -->|å¦| A6
    A5 -->|æ˜¯| A7
    A6 --> A8
    A7 --> A8
    A8 --> A9

    style A1 fill:#e3f2fd,color:#000000,stroke:#333
    style A4 fill:#fff3e0,color:#000000,stroke:#333
    style A5 fill:#fff3e0,color:#000000,stroke:#333
    style A9 fill:#ffebee,color:#000000,stroke:#333
```

**new_token_ratioçš„é¢„ç®—è®¡ç®—æœºåˆ¶**ï¼š
new_token_ratioç”¨äºé¢„ä¼°æœªæ¥è§£ç é˜¶æ®µçš„tokenæ¶ˆè€—ï¼Œç¡®ä¿é¢„å¡«å……æ‰¹æ¬¡ä¸ä¼šå¯¼è‡´åç»­è§£ç é˜¶æ®µçš„å†…å­˜ä¸è¶³ã€‚è®¡ç®—å…¬å¼ä¸ºï¼š

```
è§£ç é¢„ç®— = Î£(min(max_new_tokens - len(output_ids), CLIP_MAX_NEW_TOKENS) * new_token_ratio)
å¯ç”¨é¢„ç®— = max_prefill_tokens - è§£ç é¢„ç®— - å½“å‰è¿è¡Œæ‰¹æ¬¡å¼€é”€
```

### 3.2 å‰ç¼€ç¼“å­˜é›†æˆæœºåˆ¶

å‰ç¼€ç¼“å­˜æ˜¯SGLangæ€§èƒ½ä¼˜åŒ–çš„æ ¸å¿ƒï¼ŒPrefillAdderä¸TreeCacheçš„é›†æˆå®ç°äº†æ™ºèƒ½çš„ç¼“å­˜åˆ©ç”¨ï¼š

```python
# å‰ç¼€ç¼“å­˜åŒ¹é…ä¸ç»Ÿè®¡
def add_one_req(self, req: Req, has_chunked_req: bool):
    # åˆå§‹åŒ–ä¸‹ä¸€è½®è¾“å…¥ï¼Œè§¦å‘å‰ç¼€åŒ¹é…
    req.init_next_round_input(self.tree_cache)
    
    # ç»Ÿè®¡ç¼“å­˜å‘½ä¸­æƒ…å†µ
    prefix_len = len(req.prefix_indices)       # ç¼“å­˜å‘½ä¸­çš„tokenæ•°
    self.log_hit_tokens += prefix_len          # ç´¯è®¡å‘½ä¸­ç»Ÿè®¡
    self.log_input_tokens += len(req.fill_ids) # ç´¯è®¡è¾“å…¥ç»Ÿè®¡
    
    # è®¡ç®—å®é™…éœ€è¦å¤„ç†çš„tokenæ•°
    input_tokens = req.extend_input_len        # æ‰©å±•é•¿åº¦ï¼ˆå»é™¤å‰ç¼€åçš„é•¿åº¦ï¼‰
```

**ç¼“å­˜å‘½ä¸­ç‡ä¼˜åŒ–**ï¼š
ç³»ç»Ÿé€šè¿‡ç»Ÿè®¡log_hit_tokenså’Œlog_input_tokensæ¥ç›‘æ§å‰ç¼€ç¼“å­˜çš„æ•ˆæœï¼Œç¼“å­˜å‘½ä¸­ç‡ = log_hit_tokens / log_input_tokensã€‚é«˜ç¼“å­˜å‘½ä¸­ç‡æ„å‘³ç€æ›´å°‘çš„é‡å¤è®¡ç®—ã€‚

### 3.3 åˆ†å—é¢„å¡«å……å®ç°åŸç†

å½“è¾“å…¥åºåˆ—è¶…è¿‡rem_chunk_tokensé™åˆ¶æ—¶ï¼ŒSGLangä¼šå¯åŠ¨åˆ†å—é¢„å¡«å……æœºåˆ¶ï¼š

```python
# åˆ†å—é¢„å¡«å……çš„æ ¸å¿ƒå†³ç­–é€»è¾‘
if self.rem_chunk_tokens is None or input_tokens <= self.rem_chunk_tokens:
    # éåˆ†å—è·¯å¾„ï¼šæ­£å¸¸æ·»åŠ è¯·æ±‚
    self.can_run_list.append(req)
    self._update_prefill_budget(prefix_len, input_tokens, max_new_tokens)
else:
    # åˆ†å—è·¯å¾„ï¼šæˆªæ–­åºåˆ—å¹¶æ ‡è®°ä¸ºåˆ†å—è¯·æ±‚
    trunc_len = self.rem_chunk_tokens - self.page_size + 1
    if trunc_len <= 0:
        return AddReqResult.OTHER  # æ— æ³•åˆ†å—ï¼Œæ‹’ç»è¯·æ±‚
        
    req.extend_input_len = trunc_len  # è®¾ç½®æˆªæ–­é•¿åº¦
    req.fill_ids = req.fill_ids[: len(req.prefix_indices) + trunc_len]  # æˆªæ–­è¾“å…¥
    
    self.can_run_list.append(req)
    self.new_chunked_req = req  # æ ‡è®°ä¸ºæ–°çš„åˆ†å—è¯·æ±‚
    self._update_prefill_budget(prefix_len, trunc_len, 0)  # åˆ†å—è¯·æ±‚æ— è§£ç é¢„ç®—
```

**åˆ†å—è¯·æ±‚çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†**ï¼š

```mermaid
graph LR
    subgraph "ğŸ”„ åˆ†å—è¯·æ±‚ç”Ÿå‘½å‘¨æœŸ"
        A["é•¿åºåˆ—è¯·æ±‚<br/>input_len > chunk_size"]
        B["åˆ›å»ºåˆ†å—è¯·æ±‚<br/>new_chunked_req"]
        C["ç¬¬ä¸€å—å¤„ç†<br/>chunk 1"]
        D["ç¼“å­˜æœªå®Œæˆéƒ¨åˆ†<br/>cache_unfinished_req()"]
        E["ç»§ç»­å¤„ç†<br/>chunk 2"]
        F["å®Œæˆå¤„ç†<br/>chunked_req = None"]
    end

    A --> B
    B --> C
    C --> D
    D --> E
    E --> F

    style A fill:#e3f2fd,color:#000000,stroke:#333
    style B fill:#f1f8e9,color:#000000,stroke:#333
    style C fill:#fff3e0,color:#000000,stroke:#333
    style D fill:#ffebee,color:#000000,stroke:#333
    style E fill:#f3e5f5,color:#000000,stroke:#333
    style F fill:#e8f5e8,color:#000000,stroke:#333
```

```python
class PrefillAdder:
    def __init__(
        self,
        page_size: int,
        tree_cache: BasePrefixCache,
        token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator,
        running_batch: ScheduleBatch,
        new_token_ratio: float,
        rem_input_tokens: int,
        rem_chunk_tokens: Optional[int],
        mixed_with_decode_tokens: int = 0,
    ):
        self.page_size = page_size
        self.tree_cache = tree_cache
        self.token_to_kv_pool_allocator = token_to_kv_pool_allocator
        self.running_batch = running_batch
        self.new_token_ratio = new_token_ratio
        self.rem_input_tokens = rem_input_tokens - mixed_with_decode_tokens
        self.rem_chunk_tokens = rem_chunk_tokens
        if self.rem_chunk_tokens is not None:
            self.rem_chunk_tokens -= mixed_with_decode_tokens

        self.rem_total_token_offset = mixed_with_decode_tokens
        self.cur_rem_token_offset = mixed_with_decode_tokens

        self.req_states = None
        self.can_run_list = []
        self.new_chunked_req = None
        self.log_hit_tokens = 0
        self.log_input_tokens = 0

        # è®¡ç®—è§£ç é˜¶æ®µçš„tokené¢„ç®—å¼€é”€
        if running_batch is not None:
            self.rem_total_token_offset += sum(
                [
                    min(
                        (r.sampling_params.max_new_tokens - len(r.output_ids)),
                        CLIP_MAX_NEW_TOKENS,
                    )
                    * self.new_token_ratio
                    for r in running_batch.reqs
                ]
            )

        # æ£€æŸ¥æ˜¯å¦ä¸ºæ··åˆç¼“å­˜æ¨¡å¼
        self.is_hybrid = isinstance(
            self.token_to_kv_pool_allocator, SWATokenToKVPoolAllocator
        )
```

### 3.2 add_one_reqæ ¸å¿ƒæ–¹æ³•

add_one_reqæ–¹æ³•æ˜¯PrefillAdderçš„æ ¸å¿ƒï¼Œå®ƒå®ç°äº†å•ä¸ªè¯·æ±‚çš„æ™ºèƒ½æ·»åŠ é€»è¾‘ã€‚è¿™ä¸ªæ–¹æ³•éœ€è¦å¤„ç†å‰ç¼€ç¼“å­˜åŒ¹é…ã€tokené¢„ç®—è®¡ç®—ã€åˆ†å—å¤„ç†å†³ç­–ç­‰å¤æ‚é€»è¾‘ã€‚

### 3.4 æ ¸å¿ƒæ•°æ®ç»“æ„

PrefillAdderçš„æ ¸å¿ƒå­—æ®µè®¾è®¡ä½“ç°äº†å…¶å¤æ‚çš„é¢„ç®—ç®¡ç†å’ŒçŠ¶æ€è·Ÿè¸ªéœ€æ±‚ï¼š

```python
class PrefillAdder:
    def __init__(self, ...):
        # æ ¸å¿ƒé…ç½®
        self.page_size = page_size                    # KVç¼“å­˜åˆ†é¡µå¤§å°
        self.tree_cache = tree_cache                  # å‰ç¼€ç¼“å­˜å¼•ç”¨
        self.new_token_ratio = new_token_ratio        # è§£ç tokené¢„ä¼°æ¯”ç‡
        
        # tokené¢„ç®—ç®¡ç†
        self.rem_input_tokens = rem_input_tokens      # å‰©ä½™è¾“å…¥tokené¢„ç®—
        self.rem_chunk_tokens = rem_chunk_tokens      # åˆ†å—tokené™åˆ¶
        
        # çŠ¶æ€è·Ÿè¸ª
        self.can_run_list = []                        # å¯è¿è¡Œè¯·æ±‚åˆ—è¡¨
        self.new_chunked_req = None                   # æ–°åˆ›å»ºçš„åˆ†å—è¯·æ±‚
        self.log_hit_tokens = 0                       # å‰ç¼€ç¼“å­˜å‘½ä¸­ç»Ÿè®¡
        self.log_input_tokens = 0                     # æ€»è¾“å…¥tokenç»Ÿè®¡
        
        # æ··åˆç¼“å­˜æ”¯æŒ
        self.is_hybrid = isinstance(
            self.token_to_kv_pool_allocator, SWATokenToKVPoolAllocator
        )
```

### 3.5 æ·»åŠ ç»“æœæšä¸¾

```python
class AddReqResult(Enum):
    CONTINUE = auto()    # ç»§ç»­æ·»åŠ æ›´å¤šè¯·æ±‚
    NO_TOKEN = auto()    # æ²¡æœ‰å‰©ä½™tokené¢„ç®—
    OTHER = auto()       # å…¶ä»–åœæ­¢åŸå› ï¼ˆå¦‚LoRAé™åˆ¶ã€åˆ†å—å†²çªç­‰ï¼‰
```

---

## 4. æ‰¹æ¬¡æ“ä½œä¸å†…å­˜ç®¡ç†

### 4.1 æ‰¹æ¬¡åˆå¹¶æœºåˆ¶

æ‰¹æ¬¡åˆå¹¶æ˜¯SGLangè¿ç»­æ‰¹å¤„ç†çš„æ ¸å¿ƒæœºåˆ¶ï¼Œå®ƒé€šè¿‡mix_with_runningæ–¹æ³•å®ç°é¢„å¡«å……æ‰¹æ¬¡ä¸è§£ç æ‰¹æ¬¡çš„æ™ºèƒ½åˆå¹¶ã€‚è¿™ä¸ªè¿‡ç¨‹éœ€è¦åè°ƒä¸åŒå‰å‘æ¨¡å¼çš„è¯·æ±‚ï¼Œç¡®ä¿GPUè®¡ç®—çš„é«˜æ•ˆæ‰§è¡Œã€‚

**åˆå¹¶è¿‡ç¨‹çš„æ ¸å¿ƒæ­¥éª¤**ï¼š
- **å‰å‘æ¨¡å¼è®¾ç½®**ï¼šå°†æ‰¹æ¬¡æ¨¡å¼è®¾ç½®ä¸ºMIXEDï¼Œæ”¯æŒé¢„å¡«å……å’Œè§£ç çš„æ··åˆæ‰§è¡Œ
- **è§£ç ä¿¡æ¯å‡†å¤‡**ï¼šä¸ºè¿è¡Œæ‰¹æ¬¡ä¸­çš„è¯·æ±‚è®¾ç½®è§£ç æ‰€éœ€çš„fill_idså’Œextend_input_len
- **å¼ é‡æ•°æ®åˆå¹¶**ï¼šåˆå¹¶input_idsã€out_cache_locç­‰å…³é”®å¼ é‡æ•°æ®
- **é•¿åº¦ä¿¡æ¯æ›´æ–°**ï¼šæ›´æ–°prefix_lensã€extend_lensç­‰é•¿åº¦ç»Ÿè®¡ä¿¡æ¯

```python
def mix_with_running(self, running_batch: "ScheduleBatch"):
    self.forward_mode = ForwardMode.MIXED
    running_bs = running_batch.batch_size()

    for req in running_batch.reqs:
        req.fill_ids = req.origin_input_ids + req.output_ids
        req.extend_input_len = 1

    input_ids = torch.cat([self.input_ids, running_batch.input_ids])
    out_cache_loc = torch.cat([self.out_cache_loc, running_batch.out_cache_loc])

    self.merge_batch(running_batch)
    self.input_ids = input_ids
    self.out_cache_loc = out_cache_loc

    # For overlap scheduler, the output_ids has one step delay
    delta = 0 if self.enable_overlap else -1

    # NOTE: prefix_indices is what has been cached, but we don't cache each decode step
    self.prefix_lens.extend(
        [
            len(r.origin_input_ids) + len(r.output_ids) + delta
            for r in running_batch.reqs
        ]
    )
    self.extend_lens.extend([1] * running_bs)
    self.extend_num_tokens += running_bs
    # TODO (lianmin): Revisit this. It should be seq_len - 1
    self.extend_logprob_start_lens.extend([0] * running_bs)
```

### 4.2 è§£ç é˜¶æ®µå†…å­˜ç®¡ç†

è§£ç é˜¶æ®µçš„å†…å­˜ç®¡ç†éœ€è¦ç²¾ç¡®è®¡ç®—æ¯ä¸ªè¯·æ±‚çš„KVç¼“å­˜éœ€æ±‚ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†é¡µKVç¼“å­˜æ¨¡å¼ä¸‹ï¼Œéœ€è¦å‡†ç¡®é¢„ä¼°æ–°é¡µé¢çš„åˆ†é…éœ€æ±‚ã€‚

**åˆ†é¡µKVç¼“å­˜çš„å†…å­˜é¢„ä¼°**ï¼š
åœ¨åˆ†é¡µKVç¼“å­˜ç³»ç»Ÿä¸­ï¼Œæ¯ä¸ªè¯·æ±‚çš„KVç¼“å­˜è¢«åˆ†å‰²æˆå›ºå®šå¤§å°çš„é¡µé¢ã€‚è§£ç é˜¶æ®µéœ€è¦ä¸ºæ¯ä¸ªè¯·æ±‚åˆ†é…æ–°çš„é¡µé¢æ¥å­˜å‚¨æ–°ç”Ÿæˆtokençš„KVçŠ¶æ€ã€‚

**é¡µé¢åˆ†é…çš„è®¡ç®—é€»è¾‘**ï¼š
- å½“è¯·æ±‚çš„åºåˆ—é•¿åº¦è¾¾åˆ°é¡µé¢è¾¹ç•Œæ—¶ï¼ˆseqlen % page_size == 0ï¼‰ï¼Œéœ€è¦åˆ†é…æ–°é¡µé¢
- é‡å è°ƒåº¦æ¨¡å¼ä¸‹ï¼Œä½¿ç”¨å½“å‰åºåˆ—é•¿åº¦ï¼›éé‡å æ¨¡å¼ä¸‹ï¼Œä½¿ç”¨åºåˆ—é•¿åº¦å‡1
- é¡µé¢å¤§å°ä¸º1æ—¶ï¼Œæ¯ä¸ªè¯·æ±‚æ¯æ¬¡è§£ç éƒ½éœ€è¦æ–°é¡µé¢

```python
def new_page_count_next_decode(self):
    page_size = self.token_to_kv_pool_allocator.page_size
    if page_size == 1:
        return len(self.reqs)  # æ¯ä¸ªè¯·æ±‚éƒ½éœ€è¦æ–°é¡µé¢
    
    # è®¡ç®—éœ€è¦æ–°é¡µé¢çš„è¯·æ±‚æ•°é‡
    return (
        sum(1 for req in self.reqs if req.seqlen % page_size == 0)
        if self.enable_overlap
        else sum(1 for req in self.reqs if (req.seqlen - 1) % page_size == 0)
    )

def check_decode_mem(self, buf_multiplier=1):
    num_tokens = self.new_page_count_next_decode()  # é¢„ä¼°éœ€è¦çš„é¡µé¢æ•°
    available_tokens = self.token_to_kv_pool_allocator.available_size()  # å¯ç”¨é¡µé¢æ•°
    return num_tokens * buf_multiplier <= available_tokens  # å†…å­˜å……è¶³æ€§æ£€æŸ¥
```

**å†…å­˜é¢„ä¼°çš„å‡†ç¡®æ€§**ï¼š
å‡†ç¡®çš„å†…å­˜é¢„ä¼°å¯¹äºé¿å…OOMé”™è¯¯è‡³å…³é‡è¦ã€‚ç³»ç»Ÿé€šè¿‡buf_multiplierå‚æ•°æä¾›å®‰å…¨ç¼“å†²ï¼Œç¡®ä¿åœ¨å†…å­˜ç´§å¼ æ—¶ä»èƒ½ç¨³å®šè¿è¡Œã€‚

### 4.3 åˆ†å—é¢„å¡«å……å¤„ç†

åˆ†å—é¢„å¡«å……æ˜¯SGLangå¤„ç†è¶…é•¿åºåˆ—çš„å…³é”®æœºåˆ¶ï¼Œå½“è¾“å…¥åºåˆ—è¶…è¿‡å•æ¬¡å¤„ç†èƒ½åŠ›æ—¶ï¼Œç³»ç»Ÿä¼šå°†å…¶åˆ†å‰²æˆå¤šä¸ªchunkè¿›è¡Œå¤„ç†ã€‚

```python
def prepare_for_split_prefill(self):
    # For split prefill, we need to set the forward mode to SPLIT_PREFILL
    self.forward_mode = ForwardMode.SPLIT_PREFILL
```

## 5. é«˜çº§è°ƒåº¦ç­–ç•¥

### 5.1 è¿ç»­æ‰¹å¤„ç†çš„æµæ°´çº¿è®¾è®¡

SGLangçš„è¿ç»­æ‰¹å¤„ç†é‡‡ç”¨äº†ä¸‰çº§æµæ°´çº¿è®¾è®¡ï¼Œé€šè¿‡last_batchã€running_batchã€new_batchçš„åè°ƒï¼Œå®ç°äº†GPUè®¡ç®—èµ„æºçš„æœ€å¤§åŒ–åˆ©ç”¨ã€‚

**æµæ°´çº¿è°ƒåº¦çš„æ ¸å¿ƒåŸç†**ï¼š

```mermaid
graph LR
    subgraph "ğŸ”„ æ‰¹æ¬¡æµæ°´çº¿"
        A["last_batch<br/>ä¸Šè½®é¢„å¡«å……ç»“æœ"]
        B["running_batch<br/>å½“å‰è§£ç æ‰¹æ¬¡"]  
        C["new_batch<br/>æ–°é¢„å¡«å……æ‰¹æ¬¡"]
        D["next_batch<br/>ä¸‹è½®æ‰¹æ¬¡"]
    end

    A -->|merge_batch()| B
    B -->|mix_with_running()| C
    C -->|forwardæ‰§è¡Œ| D
    D -->|çŠ¶æ€æµè½¬| A

    style A fill:#e3f2fd,color:#000000,stroke:#333
    style B fill:#f1f8e9,color:#000000,stroke:#333
    style C fill:#fff3e0,color:#000000,stroke:#333
    style D fill:#ffebee,color:#000000,stroke:#333
```

**åŠ¨æ€åˆå¹¶å†³ç­–ç®—æ³•**ï¼š
ç³»ç»Ÿæ ¹æ®æ‰¹æ¬¡çŠ¶æ€åŠ¨æ€å†³å®šæ‰§è¡Œæ¨¡å¼ï¼Œå®ç°æœ€ä¼˜çš„GPUåˆ©ç”¨ç‡ï¼š

```python
# åŠ¨æ€æ‰¹æ¬¡åˆå¹¶çš„å†³ç­–é€»è¾‘
if new_batch is not None:
    if self.running_batch.is_empty():
        return new_batch                    # çº¯é¢„å¡«å……æ¨¡å¼ï¼ˆEXTENDï¼‰
    else:
        new_batch.mix_with_running(self.running_batch)  # æ··åˆæ¨¡å¼ï¼ˆMIXEDï¼‰
        return new_batch
else:
    return self.running_batch if not self.running_batch.is_empty() else None  # çº¯è§£ç æ¨¡å¼ï¼ˆDECODEï¼‰
```

### 5.2 å¤šç»´åº¦çº¦æŸä¼˜åŒ–

SGLangçš„æ‰¹å¤„ç†è°ƒåº¦éœ€è¦åŒæ—¶æ»¡è¶³å¤šä¸ªç»´åº¦çš„çº¦æŸæ¡ä»¶ï¼Œå½¢æˆäº†å¤æ‚çš„å¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼š

**çº¦æŸä¼˜å…ˆçº§å±‚æ¬¡**ï¼š
1. **ç¡¬çº¦æŸ**ï¼šå†…å­˜é™åˆ¶ã€è¯·æ±‚æ•°é‡é™åˆ¶ï¼ˆå¿…é¡»æ»¡è¶³ï¼‰
2. **è½¯çº¦æŸ**ï¼šLoRAæ•°é‡ã€åˆ†å—å†²çªï¼ˆå½±å“æ€§èƒ½ä½†ä¸å½±å“æ­£ç¡®æ€§ï¼‰
3. **ä¼˜åŒ–ç›®æ ‡**ï¼šå‰ç¼€ç¼“å­˜å‘½ä¸­ç‡ã€æ‰¹æ¬¡å¤§å°æœ€å¤§åŒ–

**LoRAé€‚é…å™¨çº¦æŸç®¡ç†**ï¼š
```python
# LoRAæ‰¹æ¬¡å…¼å®¹æ€§æ£€æŸ¥
if self.enable_lora and not self.tp_worker.can_run_lora_batch(
    lora_set | set([req.lora_id for req in adder.can_run_list]) | set([req.lora_id])
):
    self.running_batch.batch_is_full = True  # è¾¾åˆ°LoRAé™åˆ¶ï¼Œåœæ­¢æ·»åŠ 
    break
```

**åˆ†å±‚ç¼“å­˜çš„é¢„å–åè°ƒ**ï¼š
```python
# HiCacheé¢„å–çŠ¶æ€æ£€æŸ¥
if self.enable_hicache_storage:
    prefetch_done = self.tree_cache.check_prefetch_progress(req.rid)
    if not prefetch_done:
        continue  # è·³è¿‡é¢„å–æœªå®Œæˆçš„è¯·æ±‚ï¼Œé¿å…é˜»å¡
```

### 5.3 æ€§èƒ½ç›‘æ§ä¸è°ƒä¼˜

SGLangæä¾›äº†å®Œæ•´çš„æ€§èƒ½ç›‘æ§æœºåˆ¶ï¼Œå¸®åŠ©å¼€å‘è€…ç†è§£å’Œä¼˜åŒ–æ‰¹å¤„ç†æ€§èƒ½ï¼š

**å…³é”®æ€§èƒ½æŒ‡æ ‡**ï¼š
- **å‰ç¼€ç¼“å­˜å‘½ä¸­ç‡**ï¼šlog_hit_tokens / log_input_tokens
- **æ‰¹æ¬¡åˆ©ç”¨ç‡**ï¼šå®é™…æ‰¹æ¬¡å¤§å° / æœ€å¤§æ‰¹æ¬¡å¤§å°  
- **å†…å­˜åˆ©ç”¨ç‡**ï¼šå·²ç”¨tokenæ•° / æ€»å¯ç”¨tokenæ•°
- **ååé‡æŒ‡æ ‡**ï¼šå¤„ç†çš„tokenæ•° / æ—¶é—´

```python
# æ€§èƒ½ç»Ÿè®¡çš„æ ¸å¿ƒå®ç°
def log_prefill_stats(self, adder: PrefillAdder, can_run_list: List[Req], running_bs: int):
    # è®¡ç®—è¾“å…¥ååé‡
    gap_latency = time.perf_counter() - self.last_prefill_stats_tic
    self.last_input_throughput = self.last_prefill_tokens / gap_latency
    
    # ç»Ÿè®¡æ‰¹æ¬¡ä¿¡æ¯
    num_new_seq = len(can_run_list)           # æ–°åºåˆ—æ•°é‡
    new_tokens = adder.log_input_tokens       # æ–°tokenæ•°é‡
    cached_tokens = adder.log_hit_tokens      # ç¼“å­˜å‘½ä¸­tokenæ•°é‡
    
    logger.info(f"Prefill batch. #new-seq: {num_new_seq}, "
                f"#new-token: {new_tokens}, #cached-token: {cached_tokens}")
```

---

## 6. æ¶æ„æ€»ç»“ä¸æŠ€æœ¯ä»·å€¼

### 6.1 æ‰¹å¤„ç†è°ƒåº¦çš„æ ¸å¿ƒè´¡çŒ®

SGLangçš„æ‰¹å¤„ç†è°ƒåº¦ç³»ç»Ÿåœ¨ç°ä»£å¤§è¯­è¨€æ¨¡å‹æ¨ç†é¢†åŸŸåšå‡ºäº†é‡è¦çš„æŠ€æœ¯è´¡çŒ®ï¼Œå…¶è®¾è®¡ç†å¿µå’Œå®ç°æ–¹æ³•ä¸ºæ¨ç†ç³»ç»Ÿçš„å‘å±•æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

**æŠ€æœ¯åˆ›æ–°è¦ç‚¹**ï¼š
- **è¿ç»­æ‰¹å¤„ç†æ¶æ„**ï¼šä¸‰çº§æµæ°´çº¿è®¾è®¡å®ç°äº†GPUèµ„æºçš„æœ€å¤§åŒ–åˆ©ç”¨
- **æ™ºèƒ½è¯·æ±‚è°ƒåº¦**ï¼šå¤šç»´åº¦çº¦æŸä¸‹çš„ä¼˜åŒ–è¯·æ±‚é€‰æ‹©ç®—æ³•
- **åˆ†å—é¢„å¡«å……æœºåˆ¶**ï¼šçªç ´å•æ¬¡å¤„ç†é•¿åº¦é™åˆ¶çš„åˆ›æ–°è§£å†³æ–¹æ¡ˆ
- **å‰ç¼€ç¼“å­˜é›†æˆ**ï¼šæ·±åº¦é›†æˆçš„ç¼“å­˜ä¼˜åŒ–ï¼Œæ˜¾è‘—å‡å°‘é‡å¤è®¡ç®—

### 6.2 å·¥ç¨‹å®ç°çš„å¤æ‚æ€§

æ‰¹å¤„ç†è°ƒåº¦ç³»ç»Ÿçš„å®ç°ä½“ç°äº†ç”Ÿäº§çº§æ¨ç†ç³»ç»Ÿçš„å·¥ç¨‹å¤æ‚æ€§ï¼š

**çŠ¶æ€ç®¡ç†çš„ç²¾ç»†åŒ–**ï¼š
```python
# å¤æ‚çš„çŠ¶æ€è½¬æ¢ç®¡ç†
chunked_req_to_exclude = set()              # éœ€è¦æ’é™¤çš„åˆ†å—è¯·æ±‚
if self.chunked_req:
    chunked_req_to_exclude.add(self.chunked_req)
    self.tree_cache.cache_unfinished_req(self.chunked_req)  # ç¼“å­˜æœªå®Œæˆéƒ¨åˆ†
    self.req_to_token_pool.free(self.chunked_req.req_pool_idx)  # é‡Šæ”¾å†…å­˜æ§½ä½
```

**å¤šæ¨¡å¼åè°ƒçš„å¤æ‚æ€§**ï¼š
ç³»ç»Ÿéœ€è¦åœ¨EXTENDã€DECODEã€MIXEDã€SPLIT_PREFILLå››ç§å‰å‘æ¨¡å¼é—´è¿›è¡Œæ™ºèƒ½åˆ‡æ¢ï¼Œæ¯ç§æ¨¡å¼éƒ½æœ‰å…¶ç‰¹å®šçš„æ‰§è¡Œè·¯å¾„å’Œä¼˜åŒ–ç­–ç•¥ã€‚

**èµ„æºçº¦æŸçš„å…¨é¢è€ƒé‡**ï¼š
ä»åŸºç¡€çš„å†…å­˜é™åˆ¶åˆ°é«˜çº§çš„LoRAçº¦æŸï¼Œä»åˆ†ç¦»å¼æ¶æ„çš„ç‰¹æ®Šéœ€æ±‚åˆ°åˆ†å±‚ç¼“å­˜çš„é¢„å–åè°ƒï¼Œç³»ç»Ÿéœ€è¦ç»¼åˆè€ƒè™‘æ‰€æœ‰çº¦æŸæ¡ä»¶ã€‚

### 6.3 æ€§èƒ½ä¼˜åŒ–çš„ç³»ç»Ÿæ€§æ–¹æ³•

SGLangçš„æ‰¹å¤„ç†è°ƒåº¦é‡‡ç”¨äº†ç³»ç»Ÿæ€§çš„æ€§èƒ½ä¼˜åŒ–æ–¹æ³•ï¼š

**è®¡ç®—æ•ˆç‡ä¼˜åŒ–**ï¼š
- å‰ç¼€ç¼“å­˜å‡å°‘é‡å¤è®¡ç®—
- æ‰¹æ¬¡åˆå¹¶æœ€å¤§åŒ–GPUåˆ©ç”¨ç‡
- åˆ†å—å¤„ç†çªç ´é•¿åº¦é™åˆ¶

**å†…å­˜æ•ˆç‡ä¼˜åŒ–**ï¼š
- ç²¾ç¡®çš„tokené¢„ç®—ç®¡ç†
- åˆ†é¡µKVç¼“å­˜çš„é«˜æ•ˆåˆ†é…
- æ™ºèƒ½çš„å†…å­˜å›æ”¶æœºåˆ¶

**è°ƒåº¦æ•ˆç‡ä¼˜åŒ–**ï¼š
- ä¼˜å…ˆçº§é©±åŠ¨çš„è¯·æ±‚é€‰æ‹©
- å¤šç»´åº¦çº¦æŸçš„å¿«é€Ÿæ£€æŸ¥
- åŠ¨æ€çš„æ‰¹æ¬¡ç»„åˆå†³ç­–

**æ‰¿ä¸Šå¯ä¸‹**ï¼šåœ¨å‰é¢ç« èŠ‚ä¸­æˆ‘ä»¬äº†è§£äº†è°ƒåº¦å™¨çš„æ¶æ„ã€æ•°æ®ç»“æ„å’Œè¯·æ±‚å¤„ç†æœºåˆ¶ï¼Œæœ¬ç« æ·±å…¥å‰–æäº†æ‰¹å¤„ç†è°ƒåº¦çš„æ ¸å¿ƒç®—æ³•ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬å°†æ¢è®¨å†…å­˜ç®¡ç†ç³»ç»Ÿï¼Œäº†è§£SGLangå¦‚ä½•å®ç°é«˜æ•ˆçš„KVç¼“å­˜ç®¡ç†å’Œå‰ç¼€ç¼“å­˜ä¼˜åŒ–ã€‚