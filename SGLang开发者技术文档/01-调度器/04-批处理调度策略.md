# 批处理调度策略

SGLang调度器通过批处理机制来提高GPU利用率和推理吞吐量。本章介绍调度器的核心批处理策略和实现。

## 核心批处理方法

### get_next_batch_to_run

调度器的主要批处理方法，负责准备下一个要执行的批次：

```python
def get_next_batch_to_run(self) -> Optional[ScheduleBatch]:
    # 处理分块请求的合并
    chunked_req_to_exclude = set()
    if self.chunked_req:
        # 将分块请求移出批次，以便只将完成的请求合并到running_batch
        chunked_req_to_exclude.add(self.chunked_req)
        self.tree_cache.cache_unfinished_req(self.chunked_req)
        # 分块请求保持rid但会获得新的req_pool_idx
        self.req_to_token_pool.free(self.chunked_req.req_pool_idx)
        
    if self.last_batch and self.last_batch.forward_mode.is_extend():
        if self.last_batch.chunked_req is not None:
            # 在pipeline parallelism中，需要丢弃过时的chunked_req
            chunked_req_to_exclude.add(self.last_batch.chunked_req)

        # 过滤批次
        last_bs = self.last_batch.batch_size()
        self.last_batch.filter_batch(
            chunked_req_to_exclude=list(chunked_req_to_exclude)
        )
        if self.last_batch.batch_size() < last_bs:
            self.running_batch.batch_is_full = False

        # 将新批次合并到运行批次中
        # 对于仅预填充批次，可以避免解码步骤
        if not self.last_batch.is_empty() and not self.last_batch.is_prefill_only:
            if self.running_batch.is_empty():
                self.running_batch = self.last_batch
            else:
                # 合并running_batch和prefill batch
                self.running_batch.merge_batch(self.last_batch)

    # 获取新的预填充批次
    new_batch = self.get_new_batch_prefill()
    
    # 返回处理后的批次...
```

### get_new_batch_prefill

获取新的预填充批次的核心逻辑（实际实现由PrefillAdder处理）：

```python
def get_new_batch_prefill(self) -> Optional[ScheduleBatch]:
    # 首先检查语法队列中是否有准备好的请求
    if self.grammar_queue:
        self.move_ready_grammar_requests()

    # 处理不允许预填充的情况
    if (
        self.running_batch.batch_is_full or len(self.waiting_queue) == 0
    ) and self.chunked_req is None:
        return None

    running_bs = len(self.running_batch.reqs)
    
    # 检查可分配的请求数量
    if self.get_num_allocatable_reqs(running_bs) <= 0 and not self.chunked_req:
        self.running_batch.batch_is_full = True
        return None

    if self.enable_hierarchical_cache:
        self.tree_cache.check_hicache_events()

    # 获取优先级队列
    self.policy.calc_priority(self.waiting_queue)

    # 创建PrefillAdder来处理新请求的添加
    adder = PrefillAdder(
        self.page_size,
        self.tree_cache,
        self.token_to_kv_pool_allocator,
        self.running_batch,
        self.new_token_ratio,
        self.max_prefill_tokens,
        self.chunked_prefill_size,
        running_bs if self.is_mixed_chunk else 0,
    )

    # 处理分块请求
    if self.chunked_req is not None:
        self.chunked_req.init_next_round_input()
        self.chunked_req = adder.add_chunked_req(self.chunked_req)

    # 处理LoRA约束
    if self.enable_lora:
        lora_set = set([req.lora_id for req in self.running_batch.reqs])
    
    # 从等待队列中添加请求
    for req in self.waiting_queue:
        # LoRA批次大小限制
        if self.enable_lora:
            if req.lora_id not in lora_set:
                if len(lora_set) >= self.max_loras_per_batch:
                    break
                lora_set.add(req.lora_id)
        
        add_result = adder.add_req(req)
        if add_result != AddReqResult.CONTINUE:
            break
    
    return adder.get_batch()
```

## ScheduleBatch数据结构

### 核心字段

`ScheduleBatch`包含批处理执行所需的所有信息：

```python
@dataclasses.dataclass
class ScheduleBatch(ScheduleBatchDisaggregationDecodeMixin):
    # 请求、内存池和缓存
    reqs: List[Req]
    req_to_token_pool: ReqToTokenPool = None
    token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator = None
    tree_cache: BasePrefixCache = None
    is_hybrid: bool = False

    # 批次配置
    model_config: ModelConfig = None
    forward_mode: ForwardMode = None
    enable_overlap: bool = False
    batch_is_full: bool = False

    # 分块预填充支持
    chunked_req: Optional[Req] = None

    # 模型运行器的批处理参数
    input_ids: torch.Tensor = None          # shape: [b], int64
    input_embeds: torch.Tensor = None       # shape: [b, hidden_size], float32
    req_pool_indices: torch.Tensor = None   # shape: [b], int64
    seq_lens: torch.Tensor = None           # shape: [b], int64
    out_cache_loc: torch.Tensor = None      # shape: [b], int64
    output_ids: torch.Tensor = None         # shape: [b], int64

    # 多模态输入
    multimodal_inputs: Optional[List] = None

    # 序列长度信息
    seq_lens_sum: int = None
    orig_seq_lens: torch.Tensor = None      # shape: [b], int32
```

### 基本操作方法

**批次状态检查**：
```python
def is_empty(self):
    return len(self.reqs) == 0

def batch_size(self):
    return len(self.reqs)
```

**内存分配方法**：
```python
def alloc_req_slots(self, num_reqs: int):
    """为请求分配槽位"""
    req_pool_indices = self.req_to_token_pool.alloc(num_reqs)
    if req_pool_indices is None:
        raise RuntimeError(
            "alloc_req_slots runs out of memory. "
            "Please set a smaller number for `--max-running-requests`. "
            f"{self.req_to_token_pool.available_size()=}, "
            f"{num_reqs=}, "
        )
    return req_pool_indices

def alloc_token_slots(self, num_tokens: int, backup_state: bool = False):
    """为token分配KV缓存槽位"""
    self._evict_tree_cache_if_needed(num_tokens)

    if backup_state:
        state = self.token_to_kv_pool_allocator.backup_state()

    out_cache_loc = self.token_to_kv_pool_allocator.alloc(num_tokens)
    if out_cache_loc is None:
        phase_str = "Prefill" if self.forward_mode.is_extend() else "Decode"
        error_msg = (
            f"{phase_str} out of memory. Try to lower your batch size.\n"
            f"Try to allocate {num_tokens} tokens.\n"
            f"{self._available_and_evictable_str()}"
        )
        logger.error(error_msg)
        if self.tree_cache is not None:
            self.tree_cache.pretty_print()
        raise RuntimeError(error_msg)

    if backup_state:
        return out_cache_loc, state
    else:
        return out_cache_loc
```

### 批次合并

**mix_with_running**：
```python
def mix_with_running(self, running_batch: "ScheduleBatch"):
    """将当前批次与运行批次混合"""
    self.forward_mode = ForwardMode.MIXED
    running_bs = running_batch.batch_size()

    # 为运行批次中的请求设置解码信息
    for req in running_batch.reqs:
        req.fill_ids = req.origin_input_ids + req.output_ids
        req.extend_input_len = 1

    # 合并输入tensors
    input_ids = torch.cat([self.input_ids, running_batch.input_ids])
    out_cache_loc = torch.cat([self.out_cache_loc, running_batch.out_cache_loc])

    # 合并请求列表
    self.merge_batch(running_batch)
    self.input_ids = input_ids
    self.out_cache_loc = out_cache_loc

    # 更新前缀和扩展长度信息
    delta = 0 if self.enable_overlap else -1
    self.prefix_lens.extend([
        len(r.origin_input_ids) + len(r.output_ids) + delta
        for r in running_batch.reqs
    ])
    self.extend_lens.extend([1] * running_bs)
    self.extend_num_tokens += running_bs
    self.extend_logprob_start_lens.extend([0] * running_bs)
```

## PrefillAdder处理策略

### 请求添加逻辑

PrefillAdder负责将等待队列中的请求添加到新批次中：

```python
class PrefillAdder:
    def __init__(self, page_size: int, tree_cache: BasePrefixCache,
                 token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator,
                 running_batch: ScheduleBatch, new_token_ratio: float,
                 rem_input_tokens: int, rem_chunk_tokens: Optional[int],
                 mixed_with_decode_tokens: int = 0):
        self.page_size = page_size
        self.tree_cache = tree_cache
        self.token_to_kv_pool_allocator = token_to_kv_pool_allocator
        self.running_batch = running_batch
        self.new_token_ratio = new_token_ratio
        self.rem_input_tokens = rem_input_tokens - mixed_with_decode_tokens
        self.rem_chunk_tokens = rem_chunk_tokens
        
        self.can_run_list = []
        self.new_chunked_req = None
        self.log_hit_tokens = 0
        self.log_input_tokens = 0
```

### 添加结果枚举

```python
class AddReqResult(Enum):
    CONTINUE = auto()    # 继续添加更多请求
    NO_TOKEN = auto()    # 没有剩余token
    OTHER = auto()       # 其他停止原因
```

## 解码阶段内存预估

### 下一次解码的页面需求

```python
def new_page_count_next_decode(self):
    """估算下一次解码需要的新页面数量"""
    page_size = self.token_to_kv_pool_allocator.page_size
    if page_size == 1:
        return len(self.reqs)
    
    # 在解码阶段，请求的KV缓存长度应该是总长度减1
    return (
        sum(1 for req in self.reqs if req.seqlen % page_size == 0)
        if self.enable_overlap
        else sum(1 for req in self.reqs if (req.seqlen - 1) % page_size == 0)
    )
```

### 解码内存检查

```python
def check_decode_mem(self, buf_multiplier=1):
    """检查解码阶段的内存需求"""
    num_tokens = self.new_page_count_next_decode()
    # 检查是否有足够内存进行下一次解码
    available_tokens = self.token_to_kv_pool_allocator.available_size()
    return num_tokens * buf_multiplier <= available_tokens
```

## 分块预填充处理

### 分割预填充准备

```python
def prepare_for_split_prefill(self):
    """为分割预填充设置前向模式"""
    self.forward_mode = ForwardMode.SPLIT_PREFILL
```

当输入序列过长无法一次处理时，调度器会将其分成多个chunk进行处理，每个chunk独立执行前向传播。

## 总结

SGLang的批处理调度策略包括：

1. **批次合并机制**: 通过`get_next_batch_to_run`合并预填充和解码批次
2. **内存管理**: 通过`alloc_req_slots`和`alloc_token_slots`管理内存分配
3. **请求添加策略**: 使用`PrefillAdder`智能添加新请求到批次
4. **分块处理**: 支持超长序列的分块预填充
5. **内存预估**: 提前检查下一步解码的内存需求

这些机制确保了高效的批处理执行和内存利用。