# è°ƒåº¦å™¨ç›‘æ§ä¸è°ƒè¯•

---

SGLangè°ƒåº¦å™¨æä¾›äº†åŸºç¡€çš„ç›‘æ§å’Œè°ƒè¯•åŠŸèƒ½ï¼Œé€šè¿‡æŒ‡æ ‡æ”¶é›†ã€æ€§èƒ½åˆ†æå’Œå†…å­˜æ£€æŸ¥ï¼Œå¸®åŠ©å¼€å‘è€…äº†è§£ç³»ç»ŸçŠ¶æ€å’Œæ’æŸ¥é—®é¢˜ã€‚æœ¬ç« åŸºäºçœŸå®æºç è§£æè¿™äº›åŠŸèƒ½ã€‚

---

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡ä½“ç³»

### ğŸ¯ SchedulerMetricsMixinæ ¸å¿ƒæŒ‡æ ‡

æ ¹æ®`scheduler_metrics_mixin.py`çš„å®é™…å®ç°ï¼Œè°ƒåº¦å™¨æ”¶é›†ä»¥ä¸‹æ ¸å¿ƒæŒ‡æ ‡ï¼š

```python
class SchedulerMetricsMixin:
    def init_metrics(self, tp_rank: int, pp_rank: int, dp_rank: Optional[int]):
        """åˆå§‹åŒ–æŒ‡æ ‡æ”¶é›†ç³»ç»Ÿ"""
        self.last_gen_throughput: float = 0.0          # æœ€è¿‘ç”Ÿæˆååé‡
        self.last_input_throughput: float = 0.0        # æœ€è¿‘è¾“å…¥ååé‡
        self.step_time_dict = defaultdict(list)         # æ‰¹æ¬¡å¤§å° -> æ­¥éª¤æ—¶é—´æ˜ å°„
        self.spec_num_total_accepted_tokens = 0         # æŠ•æœºè§£ç æ¥å—tokenæ€»æ•°
        self.spec_num_total_forward_ct = 0              # æŠ•æœºè§£ç å‰å‘è®¡ç®—æ¬¡æ•°
        self.cum_spec_accept_length = 0                 # ç´¯è®¡æŠ•æœºæ¥å—é•¿åº¦
        self.cum_spec_accept_count = 0                  # ç´¯è®¡æŠ•æœºæ¥å—æ¬¡æ•°
        self.total_retracted_reqs = 0                   # æ€»å›é€€è¯·æ±‚æ•°
        self.stats = SchedulerStats()                   # è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
        
        # å¦‚æœå¯ç”¨æŒ‡æ ‡æ”¶é›†ï¼Œåˆ›å»ºæ”¶é›†å™¨
        if self.enable_metrics:
            engine_type = "unified"
            labels = {
                "model_name": self.server_args.served_model_name,
                "engine_type": engine_type,
                "tp_rank": tp_rank,
                "pp_rank": pp_rank,
            }
            if dp_rank is not None:
                labels["dp_rank"] = dp_rank
            self.metrics_collector = SchedulerMetricsCollector(labels=labels)
```

### ğŸ”„ KVç¼“å­˜äº‹ä»¶ç›‘æ§

```python
def init_kv_events(self, kv_events_config: Optional[str]):
    """åˆå§‹åŒ–KVç¼“å­˜äº‹ä»¶ç›‘æ§"""
    if self.enable_kv_cache_events:
        self.kv_event_publisher = EventPublisherFactory.create(
            kv_events_config, self.attn_dp_rank
        )
```

## ç»Ÿè®¡ä¿¡æ¯è®°å½•

### é¢„å¡«å……é˜¶æ®µç»Ÿè®¡

```python
def log_prefill_stats(self, adder: PrefillAdder, can_run_list: List[Req], running_bs: int):
    """è®°å½•é¢„å¡«å……é˜¶æ®µçš„ç»Ÿè®¡ä¿¡æ¯"""
    
    # è®¡ç®—å¯ä»¥è¿è¡Œçš„è¯·æ±‚æ•°é‡å’Œtokenæ•°
    num_can_run_reqs = len(can_run_list)
    num_new_tokens = sum(req.extend_input_len for req in can_run_list)
    
    # è®¡ç®—æ–°tokenæ¯”ä¾‹
    if adder.cur_rem_tokens != 0:
        new_token_ratio = num_new_tokens / adder.cur_rem_tokens
    else:
        new_token_ratio = 0.0
    
    # æ„å»ºæ—¥å¿—æ¶ˆæ¯
    running_reqs_info = ""
    if running_bs > 0:
        running_reqs_info = f", #running_reqs: {running_bs}"
    
    msg = (
        f"#queue_req: {len(self.waiting_queue)}, "
        f"#can_run_req: {num_can_run_reqs}, "
        f"#new_token: {num_new_tokens}, "
        f"new_token_ratio: {new_token_ratio:.3f}"
        f"{running_reqs_info}"
    )
    
    logger.info(msg)
```

### è§£ç é˜¶æ®µç»Ÿè®¡

```python
def log_decode_stats(self, can_run_cuda_graph: bool, running_batch: ScheduleBatch = None):
    """è®°å½•è§£ç é˜¶æ®µçš„ç»Ÿè®¡ä¿¡æ¯"""
    
    # è·å–è¿è¡Œä¸­çš„è¯·æ±‚ä¿¡æ¯
    running_batch = running_batch or self.running_batch
    num_running_reqs = len(running_batch.reqs)
    
    if num_running_reqs == 0:
        return
    
    # è®¡ç®—tokenä½¿ç”¨æƒ…å†µ
    if hasattr(self, '_get_token_info'):
        if self.is_hybrid:
            # å¤„ç†æ··åˆç¼“å­˜æƒ…å†µ
            (full_num_used, swa_num_used, _, _, 
             full_available_size, full_evictable_size,
             swa_available_size, swa_evictable_size) = self._get_swa_token_info()
            num_used = full_num_used + swa_num_used
            available_size = full_available_size + swa_available_size
        else:
            num_used, _, available_size, evictable_size = self._get_token_info()
    else:
        num_used = available_size = 0
    
    # è®¡ç®—tokenä½¿ç”¨ç‡
    if self.max_total_num_tokens > 0:
        token_usage = num_used / self.max_total_num_tokens
    else:
        token_usage = 0.0
    
    # è®¡ç®—æŠ•æœºè§£ç æ¥å—é•¿åº¦
    if self.cum_spec_accept_count > 0:
        spec_accept_length = self.cum_spec_accept_length / self.cum_spec_accept_count
    else:
        spec_accept_length = 0.0
    
    # æ„å»ºè¯¦ç»†çš„çŠ¶æ€ä¿¡æ¯
    msg_parts = [
        f"#running_req: {num_running_reqs}",
        f"#queue_req: {len(self.waiting_queue)}",
        f"#token: {num_used}",
        f"token_usage: {token_usage:.3f}",
        f"gen_throughput: {self.last_gen_throughput:.3f} token/s",
    ]
    
    if hasattr(self, 'grammar_queue'):
        msg_parts.append(f"#grammar_queue_req: {len(self.grammar_queue)}")
    
    if spec_accept_length > 0:
        msg_parts.append(f"spec_accept_length: {spec_accept_length:.3f}")
    
    if can_run_cuda_graph:
        msg_parts.append("cuda_graph: True")
    
    msg = ", ".join(msg_parts)
    logger.info(msg)
    
    # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
    if self.enable_metrics:
        self.stats.num_running_reqs = num_running_reqs
        self.stats.num_used_tokens = num_used
        self.stats.token_usage = round(token_usage, 2)
        self.stats.cache_hit_rate = 0.0  # TODO: å®é™…è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
        self.stats.gen_throughput = self.last_gen_throughput
        self.stats.num_queue_reqs = len(self.waiting_queue)
        if hasattr(self, 'grammar_queue'):
            self.stats.num_grammar_queue_reqs = len(self.grammar_queue)
        self.stats.spec_accept_length = spec_accept_length
        self.stats.total_retracted_reqs = self.total_retracted_reqs
        
        # å‘é€æŒ‡æ ‡åˆ°æ”¶é›†å™¨
        self.metrics_collector.log_stats(self.stats)
        
        # å¤„ç†åˆ†ç¦»å¼æ¶æ„çš„é¢å¤–æŒ‡æ ‡
        if self.disaggregation_mode == DisaggregationMode.DECODE:
            self.stats.num_decode_prealloc_queue_reqs = len(
                self.disagg_decode_prealloc_queue.queue
            )
            self.stats.num_decode_transfer_queue_reqs = len(
                self.disagg_decode_transfer_queue.queue
            )
        
        # å‘é€KVæŒ‡æ ‡
        self._emit_kv_metrics()
    
    # å‘å¸ƒKVäº‹ä»¶
    self._publish_kv_events()
```

### KVæŒ‡æ ‡å‘é€

```python
def _emit_kv_metrics(self):
    """å‘é€KVç¼“å­˜ç›¸å…³æŒ‡æ ‡"""
    kv_metrics = KvMetrics()
    kv_metrics.request_active_slots = self.stats.num_running_reqs
    kv_metrics.request_total_slots = self.max_running_requests
    kv_metrics.kv_active_blocks = int(
        self.stats.token_usage * self.max_total_num_tokens
    )
    kv_metrics.kv_total_blocks = self.max_total_num_tokens
    kv_metrics.num_requests_waiting = self.stats.num_queue_reqs
    kv_metrics.gpu_cache_usage_perc = self.stats.token_usage
    kv_metrics.gpu_prefix_cache_hit_rate = self.stats.cache_hit_rate
    kv_metrics.data_parallel_rank = self.dp_rank if self.dp_rank is not None else 0
    
    # é€šè¿‡IPCå‘é€æŒ‡æ ‡
    if not self.send_metrics_from_scheduler.closed:
        self.send_metrics_from_scheduler.send_pyobj(kv_metrics)
```

## æ€§èƒ½åˆ†æå·¥å…·

### SchedulerProfilerMixin

è°ƒåº¦å™¨é›†æˆäº†PyTorch Profilerç”¨äºæ€§èƒ½åˆ†æï¼š

```python
class SchedulerProfilerMixin:
    def init_profier(self):
        """åˆå§‹åŒ–profilerç›¸å…³å˜é‡"""
        self.torch_profiler = None
        self.torch_profiler_output_dir: Optional[str] = None
        self.profiler_activities: Optional[List[str]] = None
        self.profile_id: Optional[str] = None
        self.profiler_start_forward_ct: Optional[int] = None
        self.profiler_target_forward_ct: Optional[int] = None
        self.profiler_target_prefill_ct: Optional[int] = None
        self.profiler_target_decode_ct: Optional[int] = None
        self.profiler_prefill_ct: Optional[int] = None
        self.profiler_decode_ct: Optional[int] = None
        self.profile_by_stage: bool = False
        self.profile_steps: Optional[int] = None
        self.profile_in_progress: bool = False
        self.rpd_profiler = None
```

### æ€§èƒ½åˆ†æé…ç½®

```python
def init_profile(self, output_dir: Optional[str], start_step: Optional[int],
                num_steps: Optional[int], activities: Optional[List[str]],
                with_stack: Optional[bool], record_shapes: Optional[bool],
                profile_by_stage: bool, profile_id: str) -> ProfileReqOutput:
    """åˆå§‹åŒ–æ€§èƒ½åˆ†æé…ç½®"""
    
    if self.profile_in_progress:
        return ProfileReqOutput(
            success=False,
            message="Profiling is already in progress. Call /stop_profile first.",
        )
    
    self.profile_by_stage = profile_by_stage
    
    # è®¾ç½®é»˜è®¤å€¼
    if output_dir is None:
        output_dir = os.getenv("SGLANG_TORCH_PROFILER_DIR", "/tmp")
    if activities is None:
        activities = ["CPU", "GPU"]
    
    # ä¿å­˜é…ç½®
    self.torch_profiler_output_dir = output_dir
    self.torch_profiler_with_stack = with_stack
    self.torch_profiler_record_shapes = record_shapes
    self.profiler_activities = activities
    self.profile_id = profile_id
    
    # è®¾ç½®å¼€å§‹æ­¥éª¤
    if start_step:
        self.profiler_start_forward_ct = max(start_step, self.forward_ct + 1)
    
    # è®¾ç½®åˆ†ææ­¥æ•°
    if num_steps:
        self.profile_steps = num_steps
        if self.profile_by_stage:
            self.profiler_target_prefill_ct = num_steps
            self.profiler_target_decode_ct = num_steps
            self.profiler_prefill_ct = 0
            self.profiler_decode_ct = 0
        elif start_step:
            self.profiler_target_forward_ct = (
                self.profiler_start_forward_ct + num_steps
            )
        else:
            self.profiler_target_forward_ct = self.forward_ct + num_steps
    else:
        self.profiler_target_forward_ct = None
    
    return ProfileReqOutput(success=True, message="Succeeded")
```

## ç³»ç»ŸçŠ¶æ€æ£€æŸ¥

### å†…å­˜æ³„æ¼æ£€æµ‹

è°ƒåº¦å™¨æä¾›äº†å®é™…çš„å†…å­˜æ£€æŸ¥åŠŸèƒ½ï¼š

```python
def check_memory(self):
    """æ£€æŸ¥å†…å­˜æ³„æ¼å’ŒçŠ¶æ€ä¸€è‡´æ€§"""
    
    if self.is_hybrid:
        # æ··åˆç¼“å­˜çš„å†…å­˜æ£€æŸ¥
        (full_num_used, swa_num_used, _, _,
         full_available_size, full_evictable_size,
         swa_available_size, swa_evictable_size) = self._get_swa_token_info()
        
        memory_leak = full_num_used != 0 or swa_num_used != 0
        token_msg = (
            f"{self.full_tokens_per_layer=}, {full_available_size=}, {full_evictable_size=}, "
            f"{self.tree_cache.full_protected_size()=}\n"
            f"{self.swa_tokens_per_layer=}, {swa_available_size=}, {swa_evictable_size=}, "
            f"{self.tree_cache.swa_protected_size()=}\n"
        )
    else:
        # æ ‡å‡†ç¼“å­˜çš„å†…å­˜æ£€æŸ¥
        _, _, available_size, evictable_size = self._get_token_info()
        protected_size = self.tree_cache.protected_size()
        memory_leak = (available_size + evictable_size) != (
            self.max_total_num_tokens
            if not self.enable_hierarchical_cache
            else self.max_total_num_tokens - protected_size
        )
        token_msg = f"{self.max_total_num_tokens=}, {available_size=}, {evictable_size=}, {protected_size=}\n"
    
    # æ£€æµ‹åˆ°å†…å­˜æ³„æ¼æ—¶æŠ›å‡ºå¼‚å¸¸
    if memory_leak:
        msg = "token_to_kv_pool_allocator memory leak detected! " f"{token_msg}"
        raise ValueError(msg)
    
    # æ£€æŸ¥è¯·æ±‚æ± å†…å­˜æ³„æ¼
    if self.disaggregation_mode == DisaggregationMode.DECODE:
        req_total_size = (
            self.req_to_token_pool.size + self.req_to_token_pool.pre_alloc_size
        )
    else:
        req_total_size = self.req_to_token_pool.size
    
    if len(self.req_to_token_pool.free_slots) != req_total_size:
        msg = (
            "req_to_token_pool memory leak detected!"
            f"available_size={len(self.req_to_token_pool.free_slots)}, "
            f"total_size={self.req_to_token_pool.size}\n"
        )
        raise ValueError(msg)
```

### è°ƒåº¦å™¨çŠ¶æ€æŸ¥è¯¢

```python
def current_scheduler_metrics_enabled(self):
    """æ£€æŸ¥å½“å‰è°ƒåº¦å™¨æ˜¯å¦å¯ç”¨æŒ‡æ ‡æ”¶é›†"""
    return self.attn_tp_rank == 0 or self.enable_metrics_for_all_schedulers

def get_print_prefix(self):
    """è·å–æ—¥å¿—æ‰“å°å‰ç¼€"""
    prefix = ""
    if self.attn_dp_rank is not None:
        prefix += f" DP{self.attn_dp_rank}"
    if self.server_args.tp_size > 1:
        prefix += f" TP{self.tp_rank}"
    if self.pp_size > 1:
        prefix += f" PP{self.pp_rank}"
    return prefix
```

## ç©ºé—²æ—¶çš„ç³»ç»Ÿç»´æŠ¤

### ç©ºé—²æ£€æŸ¥æœºåˆ¶

```python
def self_check_during_idle(self):
    """ç³»ç»Ÿç©ºé—²æ—¶çš„è‡ªæ£€å’Œç»´æŠ¤"""
    
    # æ‰§è¡Œå†…å­˜æ£€æŸ¥
    self.check_memory()
    
    # å¦‚æœæœ‰ç©ºé—²ç¡çœ å™¨ï¼Œå¯èƒ½è¿›å…¥ç¡çœ èŠ‚èƒ½æ¨¡å¼
    self.maybe_sleep_on_idle()

def maybe_sleep_on_idle(self):
    """ç©ºé—²æ—¶å¯èƒ½è¿›å…¥ç¡çœ çŠ¶æ€ä»¥èŠ‚çœCPU"""
    if self.idle_sleeper is not None:
        self.idle_sleeper.maybe_sleep()
```

### IdleSleeperèŠ‚èƒ½æœºåˆ¶

```python
class IdleSleeper:
    """
    åœ¨é•¿æ—¶é—´ä¸æ´»è·ƒæœŸé—´ï¼Œé€šè¿‡å‡å°‘ç³»ç»ŸåŠŸè€—æ¥èŠ‚èƒ½ã€‚
    è¿™ä¸ä»…èƒ½èŠ‚çœç”µåŠ›ï¼Œè¿˜èƒ½åœ¨è¯·æ±‚åˆ°æ¥æ—¶æä¾›æ›´å¤šçš„CPUçƒ­å®¹é‡ã€‚
    è¿™åœ¨å¤šGPUè¿æ¥çš„æƒ…å†µä¸‹ç‰¹åˆ«é‡è¦ï¼Œå¦åˆ™æ¯ä¸ªGPUéƒ½ä¼šè®©ä¸€ä¸ªçº¿ç¨‹100%å ç”¨CPUã€‚
    
    æœ€ç®€å•çš„è§£å†³æ–¹æ¡ˆæ˜¯åœ¨æ‰€æœ‰å¯èƒ½æ¥æ”¶éœ€è¦ç«‹å³å¤„ç†æ•°æ®çš„socketä¸Šä½¿ç”¨zmq.Pollerã€‚
    """
    pass  # å…·ä½“å®ç°åœ¨ç±»ä¸­
```

## è°ƒè¯•å’Œé—®é¢˜æ’æŸ¥

### ç¯å¢ƒå˜é‡æ§åˆ¶

SGLangæä¾›äº†å¤šä¸ªç¯å¢ƒå˜é‡æ¥æ§åˆ¶è°ƒè¯•è¡Œä¸ºï¼š

```python
# è®°å½•æ­¥éª¤æ—¶é—´
RECORD_STEP_TIME = get_bool_env_var("SGLANG_RECORD_STEP_TIME")

# Profilerè¾“å‡ºç›®å½•
output_dir = os.getenv("SGLANG_TORCH_PROFILER_DIR", "/tmp")
```

### åƒåœ¾å›æ”¶æ§åˆ¶

è°ƒåº¦å™¨æ”¯æŒåŠ¨æ€æ§åˆ¶åƒåœ¾å›æ”¶è¡Œä¸ºï¼Œè¿™å¯¹äºæ€§èƒ½è°ƒä¼˜å¾ˆé‡è¦ï¼š

```python
def handle_freeze_gc(self, recv_req: FreezeGCReq):
    """å¤„ç†å†»ç»“GCè¯·æ±‚ï¼šå†»ç»“è°ƒåº¦å™¨çš„GCå¹¶è½¬å‘åˆ°detokenizer"""
    freeze_gc("Scheduler")
    self.send_to_detokenizer.send_pyobj(recv_req)
    return None
```

### å†…éƒ¨çŠ¶æ€ç®¡ç†

è°ƒåº¦å™¨æä¾›äº†å†…éƒ¨çŠ¶æ€çš„è·å–å’Œè®¾ç½®æ¥å£ï¼š

```python
def get_internal_state(self, recv_req: GetInternalStateReq):
    """è·å–è°ƒåº¦å™¨å†…éƒ¨çŠ¶æ€"""
    # è¿”å›å½“å‰è°ƒåº¦å™¨çš„å†…éƒ¨çŠ¶æ€ä¿¡æ¯
    state = {
        "running_batch_size": len(self.running_batch.reqs),
        "waiting_queue_size": len(self.waiting_queue),
        "forward_ct": self.forward_ct,
        "enable_overlap": self.enable_overlap,
    }
    if hasattr(self, 'grammar_queue'):
        state["grammar_queue_size"] = len(self.grammar_queue)
    return GetInternalStateReqOutput(state)

def set_internal_state(self, recv_req: SetInternalStateReq):
    """è®¾ç½®è°ƒåº¦å™¨å†…éƒ¨çŠ¶æ€"""
    # æ ¹æ®è¯·æ±‚è®¾ç½®å†…éƒ¨çŠ¶æ€
    for key, value in recv_req.state.items():
        if hasattr(self, key):
            setattr(self, key, value)
    return SetInternalStateReqOutput(success=True)
```

## æ€»ç»“

SGLangè°ƒåº¦å™¨çš„ç›‘æ§ä¸è°ƒè¯•ç³»ç»Ÿå…·æœ‰ä»¥ä¸‹å®é™…ç‰¹æ€§ï¼š

**åŸºç¡€æŒ‡æ ‡æ”¶é›†**: é€šè¿‡SchedulerMetricsMixinæ”¶é›†ååé‡ã€tokenä½¿ç”¨ç‡ã€é˜Ÿåˆ—é•¿åº¦ç­‰å…³é”®æŒ‡æ ‡ã€‚

**æ€§èƒ½åˆ†æé›†æˆ**: é›†æˆPyTorch Profilerï¼Œæ”¯æŒCPU/GPUæ´»åŠ¨åˆ†æã€å†…å­˜åˆ†æç­‰ã€‚

**å†…å­˜æ³„æ¼æ£€æµ‹**: æä¾›å®é™…çš„å†…å­˜ä¸€è‡´æ€§æ£€æŸ¥ï¼Œèƒ½å¤Ÿæ£€æµ‹tokenæ± å’Œè¯·æ±‚æ± çš„å†…å­˜æ³„æ¼ã€‚

**çŠ¶æ€æ—¥å¿—**: è¯¦ç»†çš„é¢„å¡«å……å’Œè§£ç é˜¶æ®µç»Ÿè®¡æ—¥å¿—ï¼Œä¾¿äºæ€§èƒ½åˆ†æå’Œé—®é¢˜æ’æŸ¥ã€‚

**èŠ‚èƒ½æœºåˆ¶**: åœ¨ç©ºé—²æœŸé—´é€šè¿‡IdleSleeperæœºåˆ¶å‡å°‘CPUå ç”¨ï¼Œé€‚åˆå¤šGPUéƒ¨ç½²ã€‚

**IPCæŒ‡æ ‡ä¼ è¾“**: é€šè¿‡è¿›ç¨‹é—´é€šä¿¡å°†æŒ‡æ ‡å‘é€åˆ°ä¸Šå±‚ç»„ä»¶ï¼Œæ”¯æŒåˆ†å¸ƒå¼ç›‘æ§ã€‚

è¿™äº›åŠŸèƒ½è™½ç„¶ç›¸å¯¹ç®€æ´ï¼Œä½†æä¾›äº†ç”Ÿäº§ç¯å¢ƒæ‰€éœ€çš„åŸºæœ¬ç›‘æ§å’Œè°ƒè¯•èƒ½åŠ›ã€‚