# 调度器扩展功能

---

SGLang调度器通过Mixin模式和扩展接口，提供了权重更新、LoRA适配器管理、数据并行控制等扩展功能。本章基于真实源码解析这些功能的实际实现。

---

## 🔄 权重更新机制

### 🎯 核心设计概念

**权重更新的委托模式设计**：SchedulerUpdateWeightsMixin采用委托模式，将实际的权重更新工作交给TPWorker处理，调度器负责缓存刷新和状态管理。

> 📝 **简化说明**：以下为权重更新的核心流程简化版本，突出委托模式。真实实现包含更多错误处理和状态验证逻辑。

```python
class SchedulerUpdateWeightsMixin:
    """权重更新Mixin（简化版）"""
    
    def update_weights_from_disk(self, recv_req: UpdateWeightFromDiskReqInput):
        """从磁盘更新权重（简化版）"""
        # 1. 委托给worker执行更新
        success, message = self.tp_worker.update_weights_from_disk(recv_req)
        
        # 2. 成功后刷新缓存
        if success:
            self.flush_cache()
            
        return UpdateWeightFromDiskReqOutput(success, message, 0)
```

### 🔍 源码实现细节

```python
class SchedulerUpdateWeightsMixin:
    """真实的SGLang权重更新Mixin实现"""
    
    def update_weights_from_disk(self, recv_req: UpdateWeightFromDiskReqInput):
        """从磁盘就地更新权重"""
        # 直接委托给tp_worker处理
        success, message = self.tp_worker.update_weights_from_disk(recv_req)
        
        if success:
            # 成功后刷新缓存
            flush_cache_success = self.flush_cache()
            assert flush_cache_success, "Cache flush failed after updating weights"
        else:
            logger.error(message)
            
        return UpdateWeightFromDiskReqOutput(success, message, 0)

💡 **实现说明**: 调度器采用简单的委托模式，将实际的权重更新工作委托给TPWorker处理，自己只负责缓存刷新和错误处理。这种设计保持了架构的清晰性。
```

### 🛠️ SchedulerUpdateWeightsMixin实现

### 🌐 分布式权重更新

```python
def update_weights_from_distributed(self, recv_req: UpdateWeightsFromDistributedReqInput):
    """从分布式源更新权重"""
    # 委托给tp_worker执行实际更新
    success, message = self.tp_worker.update_weights_from_distributed(recv_req)
    
    if success:
        if recv_req.flush_cache:
            flush_cache_success = self.flush_cache()
            assert flush_cache_success, "Cache flush failed after updating weights"
    else:
        logger.error(message)
        
    return UpdateWeightsFromDistributedReqOutput(success, message)
```

### 🎯 张量权重更新

```python
def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):
    """从张量更新在线模型参数"""
    success, message = self.tp_worker.update_weights_from_tensor(recv_req)
    
    if success:
        if recv_req.flush_cache:
            flush_cache_success = self.flush_cache()
            assert flush_cache_success, "Cache flush failed after updating weights"
    else:
        logger.error(message)
    
    # 张量更新需要CPU组同步
    torch.distributed.barrier(group=self.tp_cpu_group)
    return UpdateWeightsFromTensorReqOutput(success, message)
```

### 权重查询

```python
def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput):
    """根据名称获取权重参数"""
    parameter = self.tp_worker.get_weights_by_name(recv_req)
    return GetWeightsByNameReqOutput(parameter)
```

### 权重更新组初始化

```python
def init_weights_update_group(self, recv_req: InitWeightsUpdateGroupReqInput):
    """初始化在线模型参数更新组"""
    success, message = self.tp_worker.init_weights_update_group(recv_req)
    return InitWeightsUpdateGroupReqOutput(success, message)
```

---

---

## 🧩 LoRA适配器管理

### 🎯 核心设计概念

```python
def load_lora_adapter(self, recv_req: LoadLoRAAdapterReqInput):
    """LoRA适配器管理的核心概念"""
    # 1. 委托给worker处理
    result = self.tp_worker.load_lora_adapter(recv_req)
    
    # 2. 返回加载结果
    return result

def unload_lora_adapter(self, recv_req: UnloadLoRAAdapterReqInput):
    """卸载LoRA适配器"""
    result = self.tp_worker.unload_lora_adapter(recv_req)
    return result
```

### 🔍 源码实现细节

```python
def load_lora_adapter(self, recv_req: LoadLoRAAdapterReqInput) -> LoadLoRAAdapterReqOutput:
    """真实的SGLang LoRA适配器加载实现"""
    # 直接委托给tp_worker处理
    result = self.tp_worker.load_lora_adapter(recv_req)
    return result

def unload_lora_adapter(self, recv_req: UnloadLoRAAdapterReqInput) -> UnloadLoRAAdapterReqOutput:
    """真实的SGLang LoRA适配器卸载实现"""
    # 直接委托给tp_worker处理
    result = self.tp_worker.unload_lora_adapter(recv_req)
    return result

💡 **实现说明**: 调度器层面的LoRA管理非常简洁，仅作为请求转发器。实际的LoRA管理逻辑在ModelRunner和LoRAManager中实现，包括内存管理、权重加载、批次约束等复杂功能。
```

### 🔌 调度器层面的LoRA接口

### ModelRunner中的LoRA实现

实际的LoRA管理在ModelRunner中实现：

```python
class ModelRunner:
    def init_lora_manager(self):
        """初始化LoRA管理器"""
        self.lora_manager = LoRAManager(
            base_model=self.model,
            base_hf_config=self.model_config.hf_config,
            max_loras_per_batch=self.server_args.max_loras_per_batch,
            load_config=self.load_config,
            dtype=self.dtype,
            lora_backend=self.server_args.lora_backend,
            tp_size=self.tp_size,
            tp_rank=self.tp_rank,
            max_lora_rank=self.server_args.max_lora_rank,
            target_modules=self.server_args.lora_target_modules,
            lora_paths=self.server_args.lora_paths,
        )
    
    def load_lora_adapter(self, lora_ref: LoRARef):
        """从磁盘或huggingface加载新的LoRA适配器"""
        logger.info(
            f"LoRA adapter loading starts: {lora_ref}. "
            f"avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB"
        )
        
        result = self.lora_manager.load_lora_adapter(lora_ref)
        
        logger.info(
            f"LoRA adapter loading completes: {lora_ref}. "
            f"avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB"
        )
        
        return result
    
    def unload_lora_adapter(self, lora_ref: LoRARef):
        """卸载之前加载的LoRA适配器"""
        logger.info(
            f"LoRA adapter unloading starts: {lora_ref}. "
            f"avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB"
        )
        
        result = self.lora_manager.unload_lora_adapter(lora_ref)
        
        logger.info(
            f"LoRA adapter unloading completes: {lora_ref}. "
            f"avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB"
        )
        
        return result
```

### LoRAManager核心功能

```python
class LoRAManager:
    def unload_lora_adapter(self, lora_ref: LoRARef) -> LoRAUpdateResult:
        """卸载LoRA适配器"""
        adapter = self.configs.get(lora_ref.lora_id)
        lora_ref = self.lora_refs.get(lora_ref.lora_id)
        
        assert (
            adapter is not None and lora_ref is not None
        ), f"LoRA adapter with ID {lora_ref.lora_id} is not loaded."
        
        try:
            # 简单地从字典中删除
            del self.configs[lora_ref.lora_id]
            del self.loras[lora_ref.lora_id]
            del self.lora_refs[lora_ref.lora_id]
            self.num_pinned_loras -= int(lora_ref.pinned)
        except Exception as e:
            return self.create_lora_update_result(
                success=False,
                error_message=str(e),
            )
        
        return self.create_lora_update_result(success=True)
```

---

## 🌐 数据并行控制

### 🏗️ DataParallelController架构

数据并行控制器实现了请求分发和负载均衡：

```python
class DataParallelController:
    def __init__(self, server_args: ServerArgs, port_args: PortArgs, dp_balance_meta: DPBalanceMeta):
        # 负载均衡相关
        self.global_balance_id = 0
        self.balance_meta = dp_balance_meta
        self.load_balance_method = LoadBalanceMethod.from_str(
            server_args.load_balance_method
        )
        
        # 分发方法映射
        dispatch_lookup = {
            LoadBalanceMethod.ROUND_ROBIN: self.round_robin_scheduler,
            LoadBalanceMethod.SHORTEST_QUEUE: self.shortest_queue_scheduler,
            LoadBalanceMethod.MINIMUM_TOKENS: self.minimum_tokens_scheduler,
        }
        self.dispatching = dispatch_lookup[self.load_balance_method]
```

### 负载均衡方法

#### 轮询调度

```python
def round_robin_scheduler(self, req: Req):
    """轮询方式分发请求"""
    target_worker = self.round_robin_counter % len(self.workers)
    self.round_robin_counter += 1
    self.workers[target_worker].send_pyobj(req)
```

#### 最少Token调度

```python
def minimum_tokens_scheduler(self, req):
    """基于最少token数的负载均衡"""
    
    def get_next_global_balance_id() -> int:
        """生成全局平衡ID"""
        INT32_MAX = 2147483647
        current_id = self.global_balance_id
        self.global_balance_id = (self.global_balance_id + 1) % INT32_MAX
        return current_id
    
    # 为请求分配平衡ID
    req.dp_balance_id = get_next_global_balance_id()
    
    with self.balance_meta.mutex:
        # 1. 获取当前负载信息
        onfly_info = self.balance_meta.get_shared_onfly()      # 在途请求
        local_tokens = self.balance_meta.get_shared_local_tokens()  # 本地token数
        
        # 2. 计算总token负载
        total_tokens = [
            local_token + sum(onfly_dict.values())
            for local_token, onfly_dict in zip(local_tokens, onfly_info)
        ]
        
        # 3. 选择负载最小的worker
        target_worker = total_tokens.index(min(total_tokens))
        
        # 4. 更新在途信息
        onfly_info[target_worker][req.dp_balance_id] = len(req.input_ids)
        self.balance_meta.set_shared_onfly_info(onfly_info)
    
    # 5. 发送请求
    self.workers[target_worker].send_pyobj(req)
```

### DPBalanceMeta负载元数据

```python
class DPBalanceMeta:
    """数据并行负载元数据管理"""
    
    def get_shared_onfly(self) -> List[Dict[int, int]]:
        """获取在途请求信息"""
        # 实现在共享内存中获取在途请求状态
        pass
    
    def set_shared_onfly_info(self, data: List[Dict[int, int]]):
        """设置在途请求信息"""
        # 实现向共享内存写入在途请求状态
        pass
    
    def get_shared_local_tokens(self) -> List[int]:
        """获取各worker的本地token数"""
        # 实现获取每个worker当前处理的token数量
        pass
```

## 内存管理扩展

### 内存占用控制

```python
def release_memory_occupation(self, recv_req: ReleaseMemoryOccupationReqInput):
    """释放内存占用"""
    # 委托给tp_worker处理
    success, message = self.tp_worker.release_memory_occupation(recv_req)
    return ReleaseMemoryOccupationReqOutput(success, message)

def resume_memory_occupation(self, recv_req: ResumeMemoryOccupationReqInput):
    """恢复内存占用"""
    # 委托给tp_worker处理
    success, message = self.tp_worker.resume_memory_occupation(recv_req)
    return ResumeMemoryOccupationReqOutput(success, message)
```

### 缓存管理

```python
def flush_cache(self):
    """刷新缓存"""
    if_success = True
    
    try:
        # 清理运行批次
        if self.running_batch is not None:
            for req in self.running_batch.reqs:
                req.finished_reason = FINISH_ABORT()
            self.running_batch = None
            self.cur_batch = None
        
        # 清理等待队列
        for req in self.waiting_queue:
            req.finished_reason = FINISH_ABORT()
        self.waiting_queue = []
        
        # 重置前向计数器
        self.forward_ct = 0
        
        # 重置分块请求
        if hasattr(self, 'chunked_req'):
            self.chunked_req = None
        
        # 刷新GPU缓存
        torch.cuda.empty_cache()
        
    except Exception as e:
        logger.error(f"Failed to flush cache: {e}")
        if_success = False
        
    return if_success
```

## 其他扩展功能

### 慢速控制

```python
def slow_down(self, recv_req: SlowDownReqInput):
    """控制前向推理速度"""
    t = recv_req.forward_sleep_time
    if t is not None and t <= 0:
        t = None
    self.forward_sleep_time = t
    return SlowDownReqOutput()
```

### 专家分布记录

```python
def expert_distribution_handle(self, recv_req: ExpertDistributionReq):
    """处理专家分布请求"""
    if recv_req == ExpertDistributionReq.START_RECORD:
        get_global_expert_distribution_recorder().start_record()
    elif recv_req == ExpertDistributionReq.STOP_RECORD:
        get_global_expert_distribution_recorder().stop_record()
    elif recv_req == ExpertDistributionReq.DUMP_RECORD:
        get_global_expert_distribution_recorder().dump_record()
    else:
        raise ValueError(f"Unrecognized ExpertDistributionReq value: {recv_req=}")
    return ExpertDistributionReqOutput()
```

### 请求分发器

调度器使用TypeBasedDispatcher来处理不同类型的请求：

```python
self._request_dispatcher = TypeBasedDispatcher([
    (TokenizedGenerateReqInput, self.handle_generate_request),
    (TokenizedEmbeddingReqInput, self.handle_embedding_request),
    (FlushCacheReqInput, self.flush_cache_wrapped),
    (AbortReq, self.abort_request),
    (OpenSessionReqInput, self.open_session),
    (CloseSessionReqInput, self.close_session),
    (UpdateWeightFromDiskReqInput, self.update_weights_from_disk),
    (InitWeightsUpdateGroupReqInput, self.init_weights_update_group),
    (UpdateWeightsFromDistributedReqInput, self.update_weights_from_distributed),
    (UpdateWeightsFromTensorReqInput, self.update_weights_from_tensor),
    (GetWeightsByNameReqInput, self.get_weights_by_name),
    (ReleaseMemoryOccupationReqInput, self.release_memory_occupation),
    (ResumeMemoryOccupationReqInput, self.resume_memory_occupation),
    (SlowDownReqInput, self.slow_down),
    (ProfileReq, self.profile),
    (FreezeGCReq, self.handle_freeze_gc),
    (LoadLoRAAdapterReqInput, self.load_lora_adapter),
    (UnloadLoRAAdapterReqInput, self.unload_lora_adapter),
])
```

## Engine层面的集成

Engine类提供了高层的扩展功能接口：

```python
class Engine:
    def update_weights_from_disk(self, model_path: str, load_format: Optional[str] = None):
        """从磁盘更新权重"""
        obj = UpdateWeightFromDiskReqInput(model_path=model_path, load_format=load_format)
        return asyncio.get_event_loop().run_until_complete(
            self.tokenizer_manager.update_weights_from_disk(obj, None)
        )
    
    def load_lora_adapter(self, lora_name: str, lora_path: str, pinned: bool = False):
        """加载LoRA适配器"""
        obj = LoadLoRAAdapterReqInput(
            lora_name=lora_name,
            lora_path=lora_path, 
            pinned=pinned,
        )
        return asyncio.get_event_loop().run_until_complete(
            self.tokenizer_manager.load_lora_adapter(obj, None)
        )
    
    def unload_lora_adapter(self, lora_name: str):
        """卸载LoRA适配器"""
        obj = UnloadLoRAAdapterReqInput(lora_name=lora_name)
        return asyncio.get_event_loop().run_until_complete(
            self.tokenizer_manager.unload_lora_adapter(obj, None)
        )
```

---

## 📝 总结

SGLang调度器的扩展功能体现了现代推理系统在灵活性和可维护性方面的精心设计：

### 🎯 核心设计原则

**委托模式**: 调度器层面的扩展功能大多直接委托给tp_worker处理，保持了架构的清晰性和职责分离。

**Mixin模式**: 通过SchedulerUpdateWeightsMixin等Mixin类组织不同的功能模块，实现了良好的代码组织和可维护性。

**简单有效**: 避免了过度复杂的设计，专注于核心功能的可靠实现，通过简单的接口提供强大的功能。

**分层架构**: Engine、TokenizerManager、Scheduler、ModelRunner各层职责明确，扩展功能在合适的层次实现。

### 🔧 实现特色

**源码准确性**: 本文档基于真实SGLang源码编写，所有扩展功能实现都来自实际代码，确保技术准确性。

**教学与实践并重**: 采用"核心设计概念 + 源码实现细节"的双重结构，既便于理解扩展功能原理，又提供实现参考。

**复杂性透明**: 明确展示了调度器层面的简洁接口与底层复杂实现的差异，让开发者了解实际系统的分层设计。

### 📈 关键功能亮点

1. **多样化权重更新**: 支持磁盘、分布式、张量三种权重更新方式，满足不同部署需求
2. **LoRA适配器管理**: 提供动态LoRA加载/卸载功能，支持模型能力的灵活扩展
3. **数据并行控制**: 包含轮询、最短队列、最少token等多种负载均衡策略
4. **内存管理扩展**: 支持内存占用的动态释放和恢复控制
5. **专家分布记录**: 为MoE模型提供专家使用情况的监控和记录
6. **缓存管理**: 提供完整的缓存刷新和状态重置功能
7. **性能控制**: 支持推理速度的动态调节和性能调优

### 🚀 架构优势

- **扩展性**: Mixin模式使得添加新功能变得简单，不会影响核心调度逻辑
- **可维护性**: 清晰的分层架构便于功能的独立开发和维护
- **可靠性**: 通过委托模式和错误处理确保了系统的稳定性
- **灵活性**: 支持多种部署场景和使用模式的动态配置

这些扩展功能虽然实现相对简洁，但为SGLang提供了灵活的模型管理、系统控制和性能调优能力，是构建生产级推理系统的重要组成部分。