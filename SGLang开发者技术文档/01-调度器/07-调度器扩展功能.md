# 调度器扩展功能

SGLang调度器通过Mixin模式和扩展接口，提供了权重更新、LoRA适配器管理、数据并行控制等扩展功能。本章基于真实源码解析这些功能的实际实现。

## 权重更新机制

### SchedulerUpdateWeightsMixin实现

调度器支持多种权重更新方式，都通过简单的委托模式实现：

```python
class SchedulerUpdateWeightsMixin:
    
    def update_weights_from_disk(self, recv_req: UpdateWeightFromDiskReqInput):
        """从磁盘就地更新权重"""
        # 直接委托给tp_worker处理
        success, message = self.tp_worker.update_weights_from_disk(recv_req)
        
        if success:
            # 成功后刷新缓存
            flush_cache_success = self.flush_cache()
            assert flush_cache_success, "Cache flush failed after updating weights"
        else:
            logger.error(message)
            
        return UpdateWeightFromDiskReqOutput(success, message, 0)
```

### 分布式权重更新

```python
def update_weights_from_distributed(self, recv_req: UpdateWeightsFromDistributedReqInput):
    """从分布式源更新权重"""
    # 委托给tp_worker执行实际更新
    success, message = self.tp_worker.update_weights_from_distributed(recv_req)
    
    if success:
        if recv_req.flush_cache:
            flush_cache_success = self.flush_cache()
            assert flush_cache_success, "Cache flush failed after updating weights"
    else:
        logger.error(message)
        
    return UpdateWeightsFromDistributedReqOutput(success, message)
```

### 张量权重更新

```python
def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):
    """从张量更新在线模型参数"""
    success, message = self.tp_worker.update_weights_from_tensor(recv_req)
    
    if success:
        if recv_req.flush_cache:
            flush_cache_success = self.flush_cache()
            assert flush_cache_success, "Cache flush failed after updating weights"
    else:
        logger.error(message)
    
    # 张量更新需要CPU组同步
    torch.distributed.barrier(group=self.tp_cpu_group)
    return UpdateWeightsFromTensorReqOutput(success, message)
```

### 权重查询

```python
def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput):
    """根据名称获取权重参数"""
    parameter = self.tp_worker.get_weights_by_name(recv_req)
    return GetWeightsByNameReqOutput(parameter)
```

### 权重更新组初始化

```python
def init_weights_update_group(self, recv_req: InitWeightsUpdateGroupReqInput):
    """初始化在线模型参数更新组"""
    success, message = self.tp_worker.init_weights_update_group(recv_req)
    return InitWeightsUpdateGroupReqOutput(success, message)
```

## LoRA适配器管理

### 调度器层面的LoRA接口

调度器提供了简单的LoRA管理接口：

```python
def load_lora_adapter(self, recv_req: LoadLoRAAdapterReqInput) -> LoadLoRAAdapterReqOutput:
    """就地加载新的LoRA适配器"""
    # 直接委托给tp_worker处理
    result = self.tp_worker.load_lora_adapter(recv_req)
    return result

def unload_lora_adapter(self, recv_req: UnloadLoRAAdapterReqInput) -> UnloadLoRAAdapterReqOutput:
    """卸载LoRA适配器"""
    # 直接委托给tp_worker处理
    result = self.tp_worker.unload_lora_adapter(recv_req)
    return result
```

### ModelRunner中的LoRA实现

实际的LoRA管理在ModelRunner中实现：

```python
class ModelRunner:
    def init_lora_manager(self):
        """初始化LoRA管理器"""
        self.lora_manager = LoRAManager(
            base_model=self.model,
            base_hf_config=self.model_config.hf_config,
            max_loras_per_batch=self.server_args.max_loras_per_batch,
            load_config=self.load_config,
            dtype=self.dtype,
            lora_backend=self.server_args.lora_backend,
            tp_size=self.tp_size,
            tp_rank=self.tp_rank,
            max_lora_rank=self.server_args.max_lora_rank,
            target_modules=self.server_args.lora_target_modules,
            lora_paths=self.server_args.lora_paths,
        )
    
    def load_lora_adapter(self, lora_ref: LoRARef):
        """从磁盘或huggingface加载新的LoRA适配器"""
        logger.info(
            f"LoRA adapter loading starts: {lora_ref}. "
            f"avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB"
        )
        
        result = self.lora_manager.load_lora_adapter(lora_ref)
        
        logger.info(
            f"LoRA adapter loading completes: {lora_ref}. "
            f"avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB"
        )
        
        return result
    
    def unload_lora_adapter(self, lora_ref: LoRARef):
        """卸载之前加载的LoRA适配器"""
        logger.info(
            f"LoRA adapter unloading starts: {lora_ref}. "
            f"avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB"
        )
        
        result = self.lora_manager.unload_lora_adapter(lora_ref)
        
        logger.info(
            f"LoRA adapter unloading completes: {lora_ref}. "
            f"avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB"
        )
        
        return result
```

### LoRAManager核心功能

```python
class LoRAManager:
    def unload_lora_adapter(self, lora_ref: LoRARef) -> LoRAUpdateResult:
        """卸载LoRA适配器"""
        adapter = self.configs.get(lora_ref.lora_id)
        lora_ref = self.lora_refs.get(lora_ref.lora_id)
        
        assert (
            adapter is not None and lora_ref is not None
        ), f"LoRA adapter with ID {lora_ref.lora_id} is not loaded."
        
        try:
            # 简单地从字典中删除
            del self.configs[lora_ref.lora_id]
            del self.loras[lora_ref.lora_id]
            del self.lora_refs[lora_ref.lora_id]
            self.num_pinned_loras -= int(lora_ref.pinned)
        except Exception as e:
            return self.create_lora_update_result(
                success=False,
                error_message=str(e),
            )
        
        return self.create_lora_update_result(success=True)
```

## 数据并行控制

### DataParallelController架构

数据并行控制器实现了请求分发和负载均衡：

```python
class DataParallelController:
    def __init__(self, server_args: ServerArgs, port_args: PortArgs, dp_balance_meta: DPBalanceMeta):
        # 负载均衡相关
        self.global_balance_id = 0
        self.balance_meta = dp_balance_meta
        self.load_balance_method = LoadBalanceMethod.from_str(
            server_args.load_balance_method
        )
        
        # 分发方法映射
        dispatch_lookup = {
            LoadBalanceMethod.ROUND_ROBIN: self.round_robin_scheduler,
            LoadBalanceMethod.SHORTEST_QUEUE: self.shortest_queue_scheduler,
            LoadBalanceMethod.MINIMUM_TOKENS: self.minimum_tokens_scheduler,
        }
        self.dispatching = dispatch_lookup[self.load_balance_method]
```

### 负载均衡方法

#### 轮询调度

```python
def round_robin_scheduler(self, req: Req):
    """轮询方式分发请求"""
    target_worker = self.round_robin_counter % len(self.workers)
    self.round_robin_counter += 1
    self.workers[target_worker].send_pyobj(req)
```

#### 最少Token调度

```python
def minimum_tokens_scheduler(self, req):
    """基于最少token数的负载均衡"""
    
    def get_next_global_balance_id() -> int:
        """生成全局平衡ID"""
        INT32_MAX = 2147483647
        current_id = self.global_balance_id
        self.global_balance_id = (self.global_balance_id + 1) % INT32_MAX
        return current_id
    
    # 为请求分配平衡ID
    req.dp_balance_id = get_next_global_balance_id()
    
    with self.balance_meta.mutex:
        # 1. 获取当前负载信息
        onfly_info = self.balance_meta.get_shared_onfly()      # 在途请求
        local_tokens = self.balance_meta.get_shared_local_tokens()  # 本地token数
        
        # 2. 计算总token负载
        total_tokens = [
            local_token + sum(onfly_dict.values())
            for local_token, onfly_dict in zip(local_tokens, onfly_info)
        ]
        
        # 3. 选择负载最小的worker
        target_worker = total_tokens.index(min(total_tokens))
        
        # 4. 更新在途信息
        onfly_info[target_worker][req.dp_balance_id] = len(req.input_ids)
        self.balance_meta.set_shared_onfly_info(onfly_info)
    
    # 5. 发送请求
    self.workers[target_worker].send_pyobj(req)
```

### DPBalanceMeta负载元数据

```python
class DPBalanceMeta:
    """数据并行负载元数据管理"""
    
    def get_shared_onfly(self) -> List[Dict[int, int]]:
        """获取在途请求信息"""
        # 实现在共享内存中获取在途请求状态
        pass
    
    def set_shared_onfly_info(self, data: List[Dict[int, int]]):
        """设置在途请求信息"""
        # 实现向共享内存写入在途请求状态
        pass
    
    def get_shared_local_tokens(self) -> List[int]:
        """获取各worker的本地token数"""
        # 实现获取每个worker当前处理的token数量
        pass
```

## 内存管理扩展

### 内存占用控制

```python
def release_memory_occupation(self, recv_req: ReleaseMemoryOccupationReqInput):
    """释放内存占用"""
    # 委托给tp_worker处理
    success, message = self.tp_worker.release_memory_occupation(recv_req)
    return ReleaseMemoryOccupationReqOutput(success, message)

def resume_memory_occupation(self, recv_req: ResumeMemoryOccupationReqInput):
    """恢复内存占用"""
    # 委托给tp_worker处理
    success, message = self.tp_worker.resume_memory_occupation(recv_req)
    return ResumeMemoryOccupationReqOutput(success, message)
```

### 缓存管理

```python
def flush_cache(self):
    """刷新缓存"""
    if_success = True
    
    try:
        # 清理运行批次
        if self.running_batch is not None:
            for req in self.running_batch.reqs:
                req.finished_reason = FINISH_ABORT()
            self.running_batch = None
            self.cur_batch = None
        
        # 清理等待队列
        for req in self.waiting_queue:
            req.finished_reason = FINISH_ABORT()
        self.waiting_queue = []
        
        # 重置前向计数器
        self.forward_ct = 0
        
        # 重置分块请求
        if hasattr(self, 'chunked_req'):
            self.chunked_req = None
        
        # 刷新GPU缓存
        torch.cuda.empty_cache()
        
    except Exception as e:
        logger.error(f"Failed to flush cache: {e}")
        if_success = False
        
    return if_success
```

## 其他扩展功能

### 慢速控制

```python
def slow_down(self, recv_req: SlowDownReqInput):
    """控制前向推理速度"""
    t = recv_req.forward_sleep_time
    if t is not None and t <= 0:
        t = None
    self.forward_sleep_time = t
    return SlowDownReqOutput()
```

### 专家分布记录

```python
def expert_distribution_handle(self, recv_req: ExpertDistributionReq):
    """处理专家分布请求"""
    if recv_req == ExpertDistributionReq.START_RECORD:
        get_global_expert_distribution_recorder().start_record()
    elif recv_req == ExpertDistributionReq.STOP_RECORD:
        get_global_expert_distribution_recorder().stop_record()
    elif recv_req == ExpertDistributionReq.DUMP_RECORD:
        get_global_expert_distribution_recorder().dump_record()
    else:
        raise ValueError(f"Unrecognized ExpertDistributionReq value: {recv_req=}")
    return ExpertDistributionReqOutput()
```

### 请求分发器

调度器使用TypeBasedDispatcher来处理不同类型的请求：

```python
self._request_dispatcher = TypeBasedDispatcher([
    (TokenizedGenerateReqInput, self.handle_generate_request),
    (TokenizedEmbeddingReqInput, self.handle_embedding_request),
    (FlushCacheReqInput, self.flush_cache_wrapped),
    (AbortReq, self.abort_request),
    (OpenSessionReqInput, self.open_session),
    (CloseSessionReqInput, self.close_session),
    (UpdateWeightFromDiskReqInput, self.update_weights_from_disk),
    (InitWeightsUpdateGroupReqInput, self.init_weights_update_group),
    (UpdateWeightsFromDistributedReqInput, self.update_weights_from_distributed),
    (UpdateWeightsFromTensorReqInput, self.update_weights_from_tensor),
    (GetWeightsByNameReqInput, self.get_weights_by_name),
    (ReleaseMemoryOccupationReqInput, self.release_memory_occupation),
    (ResumeMemoryOccupationReqInput, self.resume_memory_occupation),
    (SlowDownReqInput, self.slow_down),
    (ProfileReq, self.profile),
    (FreezeGCReq, self.handle_freeze_gc),
    (LoadLoRAAdapterReqInput, self.load_lora_adapter),
    (UnloadLoRAAdapterReqInput, self.unload_lora_adapter),
])
```

## Engine层面的集成

Engine类提供了高层的扩展功能接口：

```python
class Engine:
    def update_weights_from_disk(self, model_path: str, load_format: Optional[str] = None):
        """从磁盘更新权重"""
        obj = UpdateWeightFromDiskReqInput(model_path=model_path, load_format=load_format)
        return asyncio.get_event_loop().run_until_complete(
            self.tokenizer_manager.update_weights_from_disk(obj, None)
        )
    
    def load_lora_adapter(self, lora_name: str, lora_path: str, pinned: bool = False):
        """加载LoRA适配器"""
        obj = LoadLoRAAdapterReqInput(
            lora_name=lora_name,
            lora_path=lora_path, 
            pinned=pinned,
        )
        return asyncio.get_event_loop().run_until_complete(
            self.tokenizer_manager.load_lora_adapter(obj, None)
        )
    
    def unload_lora_adapter(self, lora_name: str):
        """卸载LoRA适配器"""
        obj = UnloadLoRAAdapterReqInput(lora_name=lora_name)
        return asyncio.get_event_loop().run_until_complete(
            self.tokenizer_manager.unload_lora_adapter(obj, None)
        )
```

## 总结

SGLang调度器的扩展功能采用了简洁的设计模式：

**委托模式**: 调度器层面的扩展功能大多直接委托给tp_worker处理，保持了架构的清晰。

**Mixin模式**: 通过Mixin类组织不同的功能模块，实现了良好的代码组织和可维护性。

**简单有效**: 避免了过度复杂的设计，专注于核心功能的可靠实现。

**分层架构**: Engine、TokenizerManager、Scheduler、ModelRunner各层职责明确，扩展功能在合适的层次实现。

这些扩展功能虽然实现相对简单，但为SGLang提供了灵活的模型管理和系统控制能力。