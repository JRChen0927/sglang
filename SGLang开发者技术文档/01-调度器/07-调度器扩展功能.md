# è°ƒåº¦å™¨æ‰©å±•åŠŸèƒ½

---

SGLangè°ƒåº¦å™¨é€šè¿‡Mixinæ¨¡å¼å’Œæ‰©å±•æ¥å£ï¼Œæä¾›äº†æƒé‡æ›´æ–°ã€LoRAé€‚é…å™¨ç®¡ç†ã€æ•°æ®å¹¶è¡Œæ§åˆ¶ç­‰æ‰©å±•åŠŸèƒ½ã€‚æœ¬ç« åŸºäºçœŸå®æºç è§£æè¿™äº›åŠŸèƒ½çš„å®é™…å®ç°ã€‚

---

## ğŸ”„ æƒé‡æ›´æ–°æœºåˆ¶

### ğŸ› ï¸ SchedulerUpdateWeightsMixinå®ç°

è°ƒåº¦å™¨æ”¯æŒå¤šç§æƒé‡æ›´æ–°æ–¹å¼ï¼Œéƒ½é€šè¿‡ç®€å•çš„å§”æ‰˜æ¨¡å¼å®ç°ï¼š

```python
class SchedulerUpdateWeightsMixin:
    
    def update_weights_from_disk(self, recv_req: UpdateWeightFromDiskReqInput):
        """ä»ç£ç›˜å°±åœ°æ›´æ–°æƒé‡"""
        # ç›´æ¥å§”æ‰˜ç»™tp_workerå¤„ç†
        success, message = self.tp_worker.update_weights_from_disk(recv_req)
        
        if success:
            # æˆåŠŸååˆ·æ–°ç¼“å­˜
            flush_cache_success = self.flush_cache()
            assert flush_cache_success, "Cache flush failed after updating weights"
        else:
            logger.error(message)
            
        return UpdateWeightFromDiskReqOutput(success, message, 0)
```

### ğŸŒ åˆ†å¸ƒå¼æƒé‡æ›´æ–°

```python
def update_weights_from_distributed(self, recv_req: UpdateWeightsFromDistributedReqInput):
    """ä»åˆ†å¸ƒå¼æºæ›´æ–°æƒé‡"""
    # å§”æ‰˜ç»™tp_workeræ‰§è¡Œå®é™…æ›´æ–°
    success, message = self.tp_worker.update_weights_from_distributed(recv_req)
    
    if success:
        if recv_req.flush_cache:
            flush_cache_success = self.flush_cache()
            assert flush_cache_success, "Cache flush failed after updating weights"
    else:
        logger.error(message)
        
    return UpdateWeightsFromDistributedReqOutput(success, message)
```

### ğŸ¯ å¼ é‡æƒé‡æ›´æ–°

```python
def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):
    """ä»å¼ é‡æ›´æ–°åœ¨çº¿æ¨¡å‹å‚æ•°"""
    success, message = self.tp_worker.update_weights_from_tensor(recv_req)
    
    if success:
        if recv_req.flush_cache:
            flush_cache_success = self.flush_cache()
            assert flush_cache_success, "Cache flush failed after updating weights"
    else:
        logger.error(message)
    
    # å¼ é‡æ›´æ–°éœ€è¦CPUç»„åŒæ­¥
    torch.distributed.barrier(group=self.tp_cpu_group)
    return UpdateWeightsFromTensorReqOutput(success, message)
```

### æƒé‡æŸ¥è¯¢

```python
def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput):
    """æ ¹æ®åç§°è·å–æƒé‡å‚æ•°"""
    parameter = self.tp_worker.get_weights_by_name(recv_req)
    return GetWeightsByNameReqOutput(parameter)
```

### æƒé‡æ›´æ–°ç»„åˆå§‹åŒ–

```python
def init_weights_update_group(self, recv_req: InitWeightsUpdateGroupReqInput):
    """åˆå§‹åŒ–åœ¨çº¿æ¨¡å‹å‚æ•°æ›´æ–°ç»„"""
    success, message = self.tp_worker.init_weights_update_group(recv_req)
    return InitWeightsUpdateGroupReqOutput(success, message)
```

---

## ğŸ§© LoRAé€‚é…å™¨ç®¡ç†

### ğŸ”Œ è°ƒåº¦å™¨å±‚é¢çš„LoRAæ¥å£

è°ƒåº¦å™¨æä¾›äº†ç®€å•çš„LoRAç®¡ç†æ¥å£ï¼š

```python
def load_lora_adapter(self, recv_req: LoadLoRAAdapterReqInput) -> LoadLoRAAdapterReqOutput:
    """å°±åœ°åŠ è½½æ–°çš„LoRAé€‚é…å™¨"""
    # ç›´æ¥å§”æ‰˜ç»™tp_workerå¤„ç†
    result = self.tp_worker.load_lora_adapter(recv_req)
    return result

def unload_lora_adapter(self, recv_req: UnloadLoRAAdapterReqInput) -> UnloadLoRAAdapterReqOutput:
    """å¸è½½LoRAé€‚é…å™¨"""
    # ç›´æ¥å§”æ‰˜ç»™tp_workerå¤„ç†
    result = self.tp_worker.unload_lora_adapter(recv_req)
    return result
```

### ModelRunnerä¸­çš„LoRAå®ç°

å®é™…çš„LoRAç®¡ç†åœ¨ModelRunnerä¸­å®ç°ï¼š

```python
class ModelRunner:
    def init_lora_manager(self):
        """åˆå§‹åŒ–LoRAç®¡ç†å™¨"""
        self.lora_manager = LoRAManager(
            base_model=self.model,
            base_hf_config=self.model_config.hf_config,
            max_loras_per_batch=self.server_args.max_loras_per_batch,
            load_config=self.load_config,
            dtype=self.dtype,
            lora_backend=self.server_args.lora_backend,
            tp_size=self.tp_size,
            tp_rank=self.tp_rank,
            max_lora_rank=self.server_args.max_lora_rank,
            target_modules=self.server_args.lora_target_modules,
            lora_paths=self.server_args.lora_paths,
        )
    
    def load_lora_adapter(self, lora_ref: LoRARef):
        """ä»ç£ç›˜æˆ–huggingfaceåŠ è½½æ–°çš„LoRAé€‚é…å™¨"""
        logger.info(
            f"LoRA adapter loading starts: {lora_ref}. "
            f"avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB"
        )
        
        result = self.lora_manager.load_lora_adapter(lora_ref)
        
        logger.info(
            f"LoRA adapter loading completes: {lora_ref}. "
            f"avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB"
        )
        
        return result
    
    def unload_lora_adapter(self, lora_ref: LoRARef):
        """å¸è½½ä¹‹å‰åŠ è½½çš„LoRAé€‚é…å™¨"""
        logger.info(
            f"LoRA adapter unloading starts: {lora_ref}. "
            f"avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB"
        )
        
        result = self.lora_manager.unload_lora_adapter(lora_ref)
        
        logger.info(
            f"LoRA adapter unloading completes: {lora_ref}. "
            f"avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB"
        )
        
        return result
```

### LoRAManageræ ¸å¿ƒåŠŸèƒ½

```python
class LoRAManager:
    def unload_lora_adapter(self, lora_ref: LoRARef) -> LoRAUpdateResult:
        """å¸è½½LoRAé€‚é…å™¨"""
        adapter = self.configs.get(lora_ref.lora_id)
        lora_ref = self.lora_refs.get(lora_ref.lora_id)
        
        assert (
            adapter is not None and lora_ref is not None
        ), f"LoRA adapter with ID {lora_ref.lora_id} is not loaded."
        
        try:
            # ç®€å•åœ°ä»å­—å…¸ä¸­åˆ é™¤
            del self.configs[lora_ref.lora_id]
            del self.loras[lora_ref.lora_id]
            del self.lora_refs[lora_ref.lora_id]
            self.num_pinned_loras -= int(lora_ref.pinned)
        except Exception as e:
            return self.create_lora_update_result(
                success=False,
                error_message=str(e),
            )
        
        return self.create_lora_update_result(success=True)
```

---

## ğŸŒ æ•°æ®å¹¶è¡Œæ§åˆ¶

### ğŸ—ï¸ DataParallelControlleræ¶æ„

æ•°æ®å¹¶è¡Œæ§åˆ¶å™¨å®ç°äº†è¯·æ±‚åˆ†å‘å’Œè´Ÿè½½å‡è¡¡ï¼š

```python
class DataParallelController:
    def __init__(self, server_args: ServerArgs, port_args: PortArgs, dp_balance_meta: DPBalanceMeta):
        # è´Ÿè½½å‡è¡¡ç›¸å…³
        self.global_balance_id = 0
        self.balance_meta = dp_balance_meta
        self.load_balance_method = LoadBalanceMethod.from_str(
            server_args.load_balance_method
        )
        
        # åˆ†å‘æ–¹æ³•æ˜ å°„
        dispatch_lookup = {
            LoadBalanceMethod.ROUND_ROBIN: self.round_robin_scheduler,
            LoadBalanceMethod.SHORTEST_QUEUE: self.shortest_queue_scheduler,
            LoadBalanceMethod.MINIMUM_TOKENS: self.minimum_tokens_scheduler,
        }
        self.dispatching = dispatch_lookup[self.load_balance_method]
```

### è´Ÿè½½å‡è¡¡æ–¹æ³•

#### è½®è¯¢è°ƒåº¦

```python
def round_robin_scheduler(self, req: Req):
    """è½®è¯¢æ–¹å¼åˆ†å‘è¯·æ±‚"""
    target_worker = self.round_robin_counter % len(self.workers)
    self.round_robin_counter += 1
    self.workers[target_worker].send_pyobj(req)
```

#### æœ€å°‘Tokenè°ƒåº¦

```python
def minimum_tokens_scheduler(self, req):
    """åŸºäºæœ€å°‘tokenæ•°çš„è´Ÿè½½å‡è¡¡"""
    
    def get_next_global_balance_id() -> int:
        """ç”Ÿæˆå…¨å±€å¹³è¡¡ID"""
        INT32_MAX = 2147483647
        current_id = self.global_balance_id
        self.global_balance_id = (self.global_balance_id + 1) % INT32_MAX
        return current_id
    
    # ä¸ºè¯·æ±‚åˆ†é…å¹³è¡¡ID
    req.dp_balance_id = get_next_global_balance_id()
    
    with self.balance_meta.mutex:
        # 1. è·å–å½“å‰è´Ÿè½½ä¿¡æ¯
        onfly_info = self.balance_meta.get_shared_onfly()      # åœ¨é€”è¯·æ±‚
        local_tokens = self.balance_meta.get_shared_local_tokens()  # æœ¬åœ°tokenæ•°
        
        # 2. è®¡ç®—æ€»tokenè´Ÿè½½
        total_tokens = [
            local_token + sum(onfly_dict.values())
            for local_token, onfly_dict in zip(local_tokens, onfly_info)
        ]
        
        # 3. é€‰æ‹©è´Ÿè½½æœ€å°çš„worker
        target_worker = total_tokens.index(min(total_tokens))
        
        # 4. æ›´æ–°åœ¨é€”ä¿¡æ¯
        onfly_info[target_worker][req.dp_balance_id] = len(req.input_ids)
        self.balance_meta.set_shared_onfly_info(onfly_info)
    
    # 5. å‘é€è¯·æ±‚
    self.workers[target_worker].send_pyobj(req)
```

### DPBalanceMetaè´Ÿè½½å…ƒæ•°æ®

```python
class DPBalanceMeta:
    """æ•°æ®å¹¶è¡Œè´Ÿè½½å…ƒæ•°æ®ç®¡ç†"""
    
    def get_shared_onfly(self) -> List[Dict[int, int]]:
        """è·å–åœ¨é€”è¯·æ±‚ä¿¡æ¯"""
        # å®ç°åœ¨å…±äº«å†…å­˜ä¸­è·å–åœ¨é€”è¯·æ±‚çŠ¶æ€
        pass
    
    def set_shared_onfly_info(self, data: List[Dict[int, int]]):
        """è®¾ç½®åœ¨é€”è¯·æ±‚ä¿¡æ¯"""
        # å®ç°å‘å…±äº«å†…å­˜å†™å…¥åœ¨é€”è¯·æ±‚çŠ¶æ€
        pass
    
    def get_shared_local_tokens(self) -> List[int]:
        """è·å–å„workerçš„æœ¬åœ°tokenæ•°"""
        # å®ç°è·å–æ¯ä¸ªworkerå½“å‰å¤„ç†çš„tokenæ•°é‡
        pass
```

## å†…å­˜ç®¡ç†æ‰©å±•

### å†…å­˜å ç”¨æ§åˆ¶

```python
def release_memory_occupation(self, recv_req: ReleaseMemoryOccupationReqInput):
    """é‡Šæ”¾å†…å­˜å ç”¨"""
    # å§”æ‰˜ç»™tp_workerå¤„ç†
    success, message = self.tp_worker.release_memory_occupation(recv_req)
    return ReleaseMemoryOccupationReqOutput(success, message)

def resume_memory_occupation(self, recv_req: ResumeMemoryOccupationReqInput):
    """æ¢å¤å†…å­˜å ç”¨"""
    # å§”æ‰˜ç»™tp_workerå¤„ç†
    success, message = self.tp_worker.resume_memory_occupation(recv_req)
    return ResumeMemoryOccupationReqOutput(success, message)
```

### ç¼“å­˜ç®¡ç†

```python
def flush_cache(self):
    """åˆ·æ–°ç¼“å­˜"""
    if_success = True
    
    try:
        # æ¸…ç†è¿è¡Œæ‰¹æ¬¡
        if self.running_batch is not None:
            for req in self.running_batch.reqs:
                req.finished_reason = FINISH_ABORT()
            self.running_batch = None
            self.cur_batch = None
        
        # æ¸…ç†ç­‰å¾…é˜Ÿåˆ—
        for req in self.waiting_queue:
            req.finished_reason = FINISH_ABORT()
        self.waiting_queue = []
        
        # é‡ç½®å‰å‘è®¡æ•°å™¨
        self.forward_ct = 0
        
        # é‡ç½®åˆ†å—è¯·æ±‚
        if hasattr(self, 'chunked_req'):
            self.chunked_req = None
        
        # åˆ·æ–°GPUç¼“å­˜
        torch.cuda.empty_cache()
        
    except Exception as e:
        logger.error(f"Failed to flush cache: {e}")
        if_success = False
        
    return if_success
```

## å…¶ä»–æ‰©å±•åŠŸèƒ½

### æ…¢é€Ÿæ§åˆ¶

```python
def slow_down(self, recv_req: SlowDownReqInput):
    """æ§åˆ¶å‰å‘æ¨ç†é€Ÿåº¦"""
    t = recv_req.forward_sleep_time
    if t is not None and t <= 0:
        t = None
    self.forward_sleep_time = t
    return SlowDownReqOutput()
```

### ä¸“å®¶åˆ†å¸ƒè®°å½•

```python
def expert_distribution_handle(self, recv_req: ExpertDistributionReq):
    """å¤„ç†ä¸“å®¶åˆ†å¸ƒè¯·æ±‚"""
    if recv_req == ExpertDistributionReq.START_RECORD:
        get_global_expert_distribution_recorder().start_record()
    elif recv_req == ExpertDistributionReq.STOP_RECORD:
        get_global_expert_distribution_recorder().stop_record()
    elif recv_req == ExpertDistributionReq.DUMP_RECORD:
        get_global_expert_distribution_recorder().dump_record()
    else:
        raise ValueError(f"Unrecognized ExpertDistributionReq value: {recv_req=}")
    return ExpertDistributionReqOutput()
```

### è¯·æ±‚åˆ†å‘å™¨

è°ƒåº¦å™¨ä½¿ç”¨TypeBasedDispatcheræ¥å¤„ç†ä¸åŒç±»å‹çš„è¯·æ±‚ï¼š

```python
self._request_dispatcher = TypeBasedDispatcher([
    (TokenizedGenerateReqInput, self.handle_generate_request),
    (TokenizedEmbeddingReqInput, self.handle_embedding_request),
    (FlushCacheReqInput, self.flush_cache_wrapped),
    (AbortReq, self.abort_request),
    (OpenSessionReqInput, self.open_session),
    (CloseSessionReqInput, self.close_session),
    (UpdateWeightFromDiskReqInput, self.update_weights_from_disk),
    (InitWeightsUpdateGroupReqInput, self.init_weights_update_group),
    (UpdateWeightsFromDistributedReqInput, self.update_weights_from_distributed),
    (UpdateWeightsFromTensorReqInput, self.update_weights_from_tensor),
    (GetWeightsByNameReqInput, self.get_weights_by_name),
    (ReleaseMemoryOccupationReqInput, self.release_memory_occupation),
    (ResumeMemoryOccupationReqInput, self.resume_memory_occupation),
    (SlowDownReqInput, self.slow_down),
    (ProfileReq, self.profile),
    (FreezeGCReq, self.handle_freeze_gc),
    (LoadLoRAAdapterReqInput, self.load_lora_adapter),
    (UnloadLoRAAdapterReqInput, self.unload_lora_adapter),
])
```

## Engineå±‚é¢çš„é›†æˆ

Engineç±»æä¾›äº†é«˜å±‚çš„æ‰©å±•åŠŸèƒ½æ¥å£ï¼š

```python
class Engine:
    def update_weights_from_disk(self, model_path: str, load_format: Optional[str] = None):
        """ä»ç£ç›˜æ›´æ–°æƒé‡"""
        obj = UpdateWeightFromDiskReqInput(model_path=model_path, load_format=load_format)
        return asyncio.get_event_loop().run_until_complete(
            self.tokenizer_manager.update_weights_from_disk(obj, None)
        )
    
    def load_lora_adapter(self, lora_name: str, lora_path: str, pinned: bool = False):
        """åŠ è½½LoRAé€‚é…å™¨"""
        obj = LoadLoRAAdapterReqInput(
            lora_name=lora_name,
            lora_path=lora_path, 
            pinned=pinned,
        )
        return asyncio.get_event_loop().run_until_complete(
            self.tokenizer_manager.load_lora_adapter(obj, None)
        )
    
    def unload_lora_adapter(self, lora_name: str):
        """å¸è½½LoRAé€‚é…å™¨"""
        obj = UnloadLoRAAdapterReqInput(lora_name=lora_name)
        return asyncio.get_event_loop().run_until_complete(
            self.tokenizer_manager.unload_lora_adapter(obj, None)
        )
```

## æ€»ç»“

SGLangè°ƒåº¦å™¨çš„æ‰©å±•åŠŸèƒ½é‡‡ç”¨äº†ç®€æ´çš„è®¾è®¡æ¨¡å¼ï¼š

**å§”æ‰˜æ¨¡å¼**: è°ƒåº¦å™¨å±‚é¢çš„æ‰©å±•åŠŸèƒ½å¤§å¤šç›´æ¥å§”æ‰˜ç»™tp_workerå¤„ç†ï¼Œä¿æŒäº†æ¶æ„çš„æ¸…æ™°ã€‚

**Mixinæ¨¡å¼**: é€šè¿‡Mixinç±»ç»„ç»‡ä¸åŒçš„åŠŸèƒ½æ¨¡å—ï¼Œå®ç°äº†è‰¯å¥½çš„ä»£ç ç»„ç»‡å’Œå¯ç»´æŠ¤æ€§ã€‚

**ç®€å•æœ‰æ•ˆ**: é¿å…äº†è¿‡åº¦å¤æ‚çš„è®¾è®¡ï¼Œä¸“æ³¨äºæ ¸å¿ƒåŠŸèƒ½çš„å¯é å®ç°ã€‚

**åˆ†å±‚æ¶æ„**: Engineã€TokenizerManagerã€Schedulerã€ModelRunnerå„å±‚èŒè´£æ˜ç¡®ï¼Œæ‰©å±•åŠŸèƒ½åœ¨åˆé€‚çš„å±‚æ¬¡å®ç°ã€‚

è¿™äº›æ‰©å±•åŠŸèƒ½è™½ç„¶å®ç°ç›¸å¯¹ç®€å•ï¼Œä½†ä¸ºSGLangæä¾›äº†çµæ´»çš„æ¨¡å‹ç®¡ç†å’Œç³»ç»Ÿæ§åˆ¶èƒ½åŠ›ã€‚