# è°ƒåº¦å™¨æ‰©å±•åŠŸèƒ½

---

SGLangè°ƒåº¦å™¨é€šè¿‡Mixinæ¨¡å¼å’Œæ‰©å±•æ¥å£ï¼Œæä¾›äº†æƒé‡æ›´æ–°ã€LoRAé€‚é…å™¨ç®¡ç†ã€æ•°æ®å¹¶è¡Œæ§åˆ¶ç­‰æ‰©å±•åŠŸèƒ½ã€‚æœ¬ç« åŸºäºçœŸå®æºç è§£æè¿™äº›åŠŸèƒ½çš„å®é™…å®ç°ã€‚

---

## ğŸ”„ æƒé‡æ›´æ–°æœºåˆ¶

### ğŸ¯ æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ

**æƒé‡æ›´æ–°çš„å§”æ‰˜æ¨¡å¼è®¾è®¡**ï¼šSchedulerUpdateWeightsMixiné‡‡ç”¨å§”æ‰˜æ¨¡å¼ï¼Œå°†å®é™…çš„æƒé‡æ›´æ–°å·¥ä½œäº¤ç»™TPWorkerå¤„ç†ï¼Œè°ƒåº¦å™¨è´Ÿè´£ç¼“å­˜åˆ·æ–°å’ŒçŠ¶æ€ç®¡ç†ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹ä¸ºæƒé‡æ›´æ–°çš„æ ¸å¿ƒæµç¨‹ç®€åŒ–ç‰ˆæœ¬ï¼Œçªå‡ºå§”æ‰˜æ¨¡å¼ã€‚çœŸå®å®ç°åŒ…å«æ›´å¤šé”™è¯¯å¤„ç†å’ŒçŠ¶æ€éªŒè¯é€»è¾‘ã€‚

```python
class SchedulerUpdateWeightsMixin:
    """æƒé‡æ›´æ–°Mixinï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    
    def update_weights_from_disk(self, recv_req: UpdateWeightFromDiskReqInput):
        """ä»ç£ç›˜æ›´æ–°æƒé‡ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # 1. å§”æ‰˜ç»™workeræ‰§è¡Œæ›´æ–°
        success, message = self.tp_worker.update_weights_from_disk(recv_req)
        
        # 2. æˆåŠŸååˆ·æ–°ç¼“å­˜
        if success:
            self.flush_cache()
            
        return UpdateWeightFromDiskReqOutput(success, message, 0)
```

### ğŸ” æºç å®ç°ç»†èŠ‚

```python
class SchedulerUpdateWeightsMixin:
    """çœŸå®çš„SGLangæƒé‡æ›´æ–°Mixinå®ç°"""
    
    def update_weights_from_disk(self, recv_req: UpdateWeightFromDiskReqInput):
        """ä»ç£ç›˜å°±åœ°æ›´æ–°æƒé‡"""
        # ç›´æ¥å§”æ‰˜ç»™tp_workerå¤„ç†
        success, message = self.tp_worker.update_weights_from_disk(recv_req)
        
        if success:
            # æˆåŠŸååˆ·æ–°ç¼“å­˜
            flush_cache_success = self.flush_cache()
            assert flush_cache_success, "Cache flush failed after updating weights"
        else:
            logger.error(message)
            
        return UpdateWeightFromDiskReqOutput(success, message, 0)

ğŸ’¡ **å®ç°è¯´æ˜**: è°ƒåº¦å™¨é‡‡ç”¨ç®€å•çš„å§”æ‰˜æ¨¡å¼ï¼Œå°†å®é™…çš„æƒé‡æ›´æ–°å·¥ä½œå§”æ‰˜ç»™TPWorkerå¤„ç†ï¼Œè‡ªå·±åªè´Ÿè´£ç¼“å­˜åˆ·æ–°å’Œé”™è¯¯å¤„ç†ã€‚è¿™ç§è®¾è®¡ä¿æŒäº†æ¶æ„çš„æ¸…æ™°æ€§ã€‚
```

### ğŸ› ï¸ SchedulerUpdateWeightsMixinå®ç°

### ğŸŒ åˆ†å¸ƒå¼æƒé‡æ›´æ–°

```python
def update_weights_from_distributed(self, recv_req: UpdateWeightsFromDistributedReqInput):
    """ä»åˆ†å¸ƒå¼æºæ›´æ–°æƒé‡"""
    # å§”æ‰˜ç»™tp_workeræ‰§è¡Œå®é™…æ›´æ–°
    success, message = self.tp_worker.update_weights_from_distributed(recv_req)
    
    if success:
        if recv_req.flush_cache:
            flush_cache_success = self.flush_cache()
            assert flush_cache_success, "Cache flush failed after updating weights"
    else:
        logger.error(message)
        
    return UpdateWeightsFromDistributedReqOutput(success, message)
```

### ğŸ¯ å¼ é‡æƒé‡æ›´æ–°

```python
def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):
    """ä»å¼ é‡æ›´æ–°åœ¨çº¿æ¨¡å‹å‚æ•°"""
    success, message = self.tp_worker.update_weights_from_tensor(recv_req)
    
    if success:
        if recv_req.flush_cache:
            flush_cache_success = self.flush_cache()
            assert flush_cache_success, "Cache flush failed after updating weights"
    else:
        logger.error(message)
    
    # å¼ é‡æ›´æ–°éœ€è¦CPUç»„åŒæ­¥
    torch.distributed.barrier(group=self.tp_cpu_group)
    return UpdateWeightsFromTensorReqOutput(success, message)
```

### æƒé‡æŸ¥è¯¢

```python
def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput):
    """æ ¹æ®åç§°è·å–æƒé‡å‚æ•°"""
    parameter = self.tp_worker.get_weights_by_name(recv_req)
    return GetWeightsByNameReqOutput(parameter)
```

### æƒé‡æ›´æ–°ç»„åˆå§‹åŒ–

```python
def init_weights_update_group(self, recv_req: InitWeightsUpdateGroupReqInput):
    """åˆå§‹åŒ–åœ¨çº¿æ¨¡å‹å‚æ•°æ›´æ–°ç»„"""
    success, message = self.tp_worker.init_weights_update_group(recv_req)
    return InitWeightsUpdateGroupReqOutput(success, message)
```

---

---

## ğŸ§© LoRAé€‚é…å™¨ç®¡ç†

### ğŸ¯ æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ

```python
def load_lora_adapter(self, recv_req: LoadLoRAAdapterReqInput):
    """LoRAé€‚é…å™¨ç®¡ç†çš„æ ¸å¿ƒæ¦‚å¿µ"""
    # 1. å§”æ‰˜ç»™workerå¤„ç†
    result = self.tp_worker.load_lora_adapter(recv_req)
    
    # 2. è¿”å›åŠ è½½ç»“æœ
    return result

def unload_lora_adapter(self, recv_req: UnloadLoRAAdapterReqInput):
    """å¸è½½LoRAé€‚é…å™¨"""
    result = self.tp_worker.unload_lora_adapter(recv_req)
    return result
```

### ğŸ” æºç å®ç°ç»†èŠ‚

```python
def load_lora_adapter(self, recv_req: LoadLoRAAdapterReqInput) -> LoadLoRAAdapterReqOutput:
    """çœŸå®çš„SGLang LoRAé€‚é…å™¨åŠ è½½å®ç°"""
    # ç›´æ¥å§”æ‰˜ç»™tp_workerå¤„ç†
    result = self.tp_worker.load_lora_adapter(recv_req)
    return result

def unload_lora_adapter(self, recv_req: UnloadLoRAAdapterReqInput) -> UnloadLoRAAdapterReqOutput:
    """çœŸå®çš„SGLang LoRAé€‚é…å™¨å¸è½½å®ç°"""
    # ç›´æ¥å§”æ‰˜ç»™tp_workerå¤„ç†
    result = self.tp_worker.unload_lora_adapter(recv_req)
    return result

ğŸ’¡ **å®ç°è¯´æ˜**: è°ƒåº¦å™¨å±‚é¢çš„LoRAç®¡ç†éå¸¸ç®€æ´ï¼Œä»…ä½œä¸ºè¯·æ±‚è½¬å‘å™¨ã€‚å®é™…çš„LoRAç®¡ç†é€»è¾‘åœ¨ModelRunnerå’ŒLoRAManagerä¸­å®ç°ï¼ŒåŒ…æ‹¬å†…å­˜ç®¡ç†ã€æƒé‡åŠ è½½ã€æ‰¹æ¬¡çº¦æŸç­‰å¤æ‚åŠŸèƒ½ã€‚
```

### ğŸ”Œ è°ƒåº¦å™¨å±‚é¢çš„LoRAæ¥å£

### ModelRunnerä¸­çš„LoRAå®ç°

å®é™…çš„LoRAç®¡ç†åœ¨ModelRunnerä¸­å®ç°ï¼š

```python
class ModelRunner:
    def init_lora_manager(self):
        """åˆå§‹åŒ–LoRAç®¡ç†å™¨"""
        self.lora_manager = LoRAManager(
            base_model=self.model,
            base_hf_config=self.model_config.hf_config,
            max_loras_per_batch=self.server_args.max_loras_per_batch,
            load_config=self.load_config,
            dtype=self.dtype,
            lora_backend=self.server_args.lora_backend,
            tp_size=self.tp_size,
            tp_rank=self.tp_rank,
            max_lora_rank=self.server_args.max_lora_rank,
            target_modules=self.server_args.lora_target_modules,
            lora_paths=self.server_args.lora_paths,
        )
    
    def load_lora_adapter(self, lora_ref: LoRARef):
        """ä»ç£ç›˜æˆ–huggingfaceåŠ è½½æ–°çš„LoRAé€‚é…å™¨"""
        logger.info(
            f"LoRA adapter loading starts: {lora_ref}. "
            f"avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB"
        )
        
        result = self.lora_manager.load_lora_adapter(lora_ref)
        
        logger.info(
            f"LoRA adapter loading completes: {lora_ref}. "
            f"avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB"
        )
        
        return result
    
    def unload_lora_adapter(self, lora_ref: LoRARef):
        """å¸è½½ä¹‹å‰åŠ è½½çš„LoRAé€‚é…å™¨"""
        logger.info(
            f"LoRA adapter unloading starts: {lora_ref}. "
            f"avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB"
        )
        
        result = self.lora_manager.unload_lora_adapter(lora_ref)
        
        logger.info(
            f"LoRA adapter unloading completes: {lora_ref}. "
            f"avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB"
        )
        
        return result
```

### LoRAManageræ ¸å¿ƒåŠŸèƒ½

```python
class LoRAManager:
    def unload_lora_adapter(self, lora_ref: LoRARef) -> LoRAUpdateResult:
        """å¸è½½LoRAé€‚é…å™¨"""
        adapter = self.configs.get(lora_ref.lora_id)
        lora_ref = self.lora_refs.get(lora_ref.lora_id)
        
        assert (
            adapter is not None and lora_ref is not None
        ), f"LoRA adapter with ID {lora_ref.lora_id} is not loaded."
        
        try:
            # ç®€å•åœ°ä»å­—å…¸ä¸­åˆ é™¤
            del self.configs[lora_ref.lora_id]
            del self.loras[lora_ref.lora_id]
            del self.lora_refs[lora_ref.lora_id]
            self.num_pinned_loras -= int(lora_ref.pinned)
        except Exception as e:
            return self.create_lora_update_result(
                success=False,
                error_message=str(e),
            )
        
        return self.create_lora_update_result(success=True)
```

---

## ğŸŒ æ•°æ®å¹¶è¡Œæ§åˆ¶

### ğŸ—ï¸ DataParallelControlleræ¶æ„

æ•°æ®å¹¶è¡Œæ§åˆ¶å™¨å®ç°äº†è¯·æ±‚åˆ†å‘å’Œè´Ÿè½½å‡è¡¡ï¼š

```python
class DataParallelController:
    def __init__(self, server_args: ServerArgs, port_args: PortArgs, dp_balance_meta: DPBalanceMeta):
        # è´Ÿè½½å‡è¡¡ç›¸å…³
        self.global_balance_id = 0
        self.balance_meta = dp_balance_meta
        self.load_balance_method = LoadBalanceMethod.from_str(
            server_args.load_balance_method
        )
        
        # åˆ†å‘æ–¹æ³•æ˜ å°„
        dispatch_lookup = {
            LoadBalanceMethod.ROUND_ROBIN: self.round_robin_scheduler,
            LoadBalanceMethod.SHORTEST_QUEUE: self.shortest_queue_scheduler,
            LoadBalanceMethod.MINIMUM_TOKENS: self.minimum_tokens_scheduler,
        }
        self.dispatching = dispatch_lookup[self.load_balance_method]
```

### è´Ÿè½½å‡è¡¡æ–¹æ³•

#### è½®è¯¢è°ƒåº¦

```python
def round_robin_scheduler(self, req: Req):
    """è½®è¯¢æ–¹å¼åˆ†å‘è¯·æ±‚"""
    target_worker = self.round_robin_counter % len(self.workers)
    self.round_robin_counter += 1
    self.workers[target_worker].send_pyobj(req)
```

#### æœ€å°‘Tokenè°ƒåº¦

```python
def minimum_tokens_scheduler(self, req):
    """åŸºäºæœ€å°‘tokenæ•°çš„è´Ÿè½½å‡è¡¡"""
    
    def get_next_global_balance_id() -> int:
        """ç”Ÿæˆå…¨å±€å¹³è¡¡ID"""
        INT32_MAX = 2147483647
        current_id = self.global_balance_id
        self.global_balance_id = (self.global_balance_id + 1) % INT32_MAX
        return current_id
    
    # ä¸ºè¯·æ±‚åˆ†é…å¹³è¡¡ID
    req.dp_balance_id = get_next_global_balance_id()
    
    with self.balance_meta.mutex:
        # 1. è·å–å½“å‰è´Ÿè½½ä¿¡æ¯
        onfly_info = self.balance_meta.get_shared_onfly()      # åœ¨é€”è¯·æ±‚
        local_tokens = self.balance_meta.get_shared_local_tokens()  # æœ¬åœ°tokenæ•°
        
        # 2. è®¡ç®—æ€»tokenè´Ÿè½½
        total_tokens = [
            local_token + sum(onfly_dict.values())
            for local_token, onfly_dict in zip(local_tokens, onfly_info)
        ]
        
        # 3. é€‰æ‹©è´Ÿè½½æœ€å°çš„worker
        target_worker = total_tokens.index(min(total_tokens))
        
        # 4. æ›´æ–°åœ¨é€”ä¿¡æ¯
        onfly_info[target_worker][req.dp_balance_id] = len(req.input_ids)
        self.balance_meta.set_shared_onfly_info(onfly_info)
    
    # 5. å‘é€è¯·æ±‚
    self.workers[target_worker].send_pyobj(req)
```

### DPBalanceMetaè´Ÿè½½å…ƒæ•°æ®

```python
class DPBalanceMeta:
    """æ•°æ®å¹¶è¡Œè´Ÿè½½å…ƒæ•°æ®ç®¡ç†"""
    
    def get_shared_onfly(self) -> List[Dict[int, int]]:
        """è·å–åœ¨é€”è¯·æ±‚ä¿¡æ¯"""
        # å®ç°åœ¨å…±äº«å†…å­˜ä¸­è·å–åœ¨é€”è¯·æ±‚çŠ¶æ€
        pass
    
    def set_shared_onfly_info(self, data: List[Dict[int, int]]):
        """è®¾ç½®åœ¨é€”è¯·æ±‚ä¿¡æ¯"""
        # å®ç°å‘å…±äº«å†…å­˜å†™å…¥åœ¨é€”è¯·æ±‚çŠ¶æ€
        pass
    
    def get_shared_local_tokens(self) -> List[int]:
        """è·å–å„workerçš„æœ¬åœ°tokenæ•°"""
        # å®ç°è·å–æ¯ä¸ªworkerå½“å‰å¤„ç†çš„tokenæ•°é‡
        pass
```

## å†…å­˜ç®¡ç†æ‰©å±•

### å†…å­˜å ç”¨æ§åˆ¶

```python
def release_memory_occupation(self, recv_req: ReleaseMemoryOccupationReqInput):
    """é‡Šæ”¾å†…å­˜å ç”¨"""
    # å§”æ‰˜ç»™tp_workerå¤„ç†
    success, message = self.tp_worker.release_memory_occupation(recv_req)
    return ReleaseMemoryOccupationReqOutput(success, message)

def resume_memory_occupation(self, recv_req: ResumeMemoryOccupationReqInput):
    """æ¢å¤å†…å­˜å ç”¨"""
    # å§”æ‰˜ç»™tp_workerå¤„ç†
    success, message = self.tp_worker.resume_memory_occupation(recv_req)
    return ResumeMemoryOccupationReqOutput(success, message)
```

### ç¼“å­˜ç®¡ç†

```python
def flush_cache(self):
    """åˆ·æ–°ç¼“å­˜"""
    if_success = True
    
    try:
        # æ¸…ç†è¿è¡Œæ‰¹æ¬¡
        if self.running_batch is not None:
            for req in self.running_batch.reqs:
                req.finished_reason = FINISH_ABORT()
            self.running_batch = None
            self.cur_batch = None
        
        # æ¸…ç†ç­‰å¾…é˜Ÿåˆ—
        for req in self.waiting_queue:
            req.finished_reason = FINISH_ABORT()
        self.waiting_queue = []
        
        # é‡ç½®å‰å‘è®¡æ•°å™¨
        self.forward_ct = 0
        
        # é‡ç½®åˆ†å—è¯·æ±‚
        if hasattr(self, 'chunked_req'):
            self.chunked_req = None
        
        # åˆ·æ–°GPUç¼“å­˜
        torch.cuda.empty_cache()
        
    except Exception as e:
        logger.error(f"Failed to flush cache: {e}")
        if_success = False
        
    return if_success
```

## å…¶ä»–æ‰©å±•åŠŸèƒ½

### æ…¢é€Ÿæ§åˆ¶

```python
def slow_down(self, recv_req: SlowDownReqInput):
    """æ§åˆ¶å‰å‘æ¨ç†é€Ÿåº¦"""
    t = recv_req.forward_sleep_time
    if t is not None and t <= 0:
        t = None
    self.forward_sleep_time = t
    return SlowDownReqOutput()
```

### ä¸“å®¶åˆ†å¸ƒè®°å½•

```python
def expert_distribution_handle(self, recv_req: ExpertDistributionReq):
    """å¤„ç†ä¸“å®¶åˆ†å¸ƒè¯·æ±‚"""
    if recv_req == ExpertDistributionReq.START_RECORD:
        get_global_expert_distribution_recorder().start_record()
    elif recv_req == ExpertDistributionReq.STOP_RECORD:
        get_global_expert_distribution_recorder().stop_record()
    elif recv_req == ExpertDistributionReq.DUMP_RECORD:
        get_global_expert_distribution_recorder().dump_record()
    else:
        raise ValueError(f"Unrecognized ExpertDistributionReq value: {recv_req=}")
    return ExpertDistributionReqOutput()
```

### è¯·æ±‚åˆ†å‘å™¨

è°ƒåº¦å™¨ä½¿ç”¨TypeBasedDispatcheræ¥å¤„ç†ä¸åŒç±»å‹çš„è¯·æ±‚ï¼š

```python
self._request_dispatcher = TypeBasedDispatcher([
    (TokenizedGenerateReqInput, self.handle_generate_request),
    (TokenizedEmbeddingReqInput, self.handle_embedding_request),
    (FlushCacheReqInput, self.flush_cache_wrapped),
    (AbortReq, self.abort_request),
    (OpenSessionReqInput, self.open_session),
    (CloseSessionReqInput, self.close_session),
    (UpdateWeightFromDiskReqInput, self.update_weights_from_disk),
    (InitWeightsUpdateGroupReqInput, self.init_weights_update_group),
    (UpdateWeightsFromDistributedReqInput, self.update_weights_from_distributed),
    (UpdateWeightsFromTensorReqInput, self.update_weights_from_tensor),
    (GetWeightsByNameReqInput, self.get_weights_by_name),
    (ReleaseMemoryOccupationReqInput, self.release_memory_occupation),
    (ResumeMemoryOccupationReqInput, self.resume_memory_occupation),
    (SlowDownReqInput, self.slow_down),
    (ProfileReq, self.profile),
    (FreezeGCReq, self.handle_freeze_gc),
    (LoadLoRAAdapterReqInput, self.load_lora_adapter),
    (UnloadLoRAAdapterReqInput, self.unload_lora_adapter),
])
```

## Engineå±‚é¢çš„é›†æˆ

Engineç±»æä¾›äº†é«˜å±‚çš„æ‰©å±•åŠŸèƒ½æ¥å£ï¼š

```python
class Engine:
    def update_weights_from_disk(self, model_path: str, load_format: Optional[str] = None):
        """ä»ç£ç›˜æ›´æ–°æƒé‡"""
        obj = UpdateWeightFromDiskReqInput(model_path=model_path, load_format=load_format)
        return asyncio.get_event_loop().run_until_complete(
            self.tokenizer_manager.update_weights_from_disk(obj, None)
        )
    
    def load_lora_adapter(self, lora_name: str, lora_path: str, pinned: bool = False):
        """åŠ è½½LoRAé€‚é…å™¨"""
        obj = LoadLoRAAdapterReqInput(
            lora_name=lora_name,
            lora_path=lora_path, 
            pinned=pinned,
        )
        return asyncio.get_event_loop().run_until_complete(
            self.tokenizer_manager.load_lora_adapter(obj, None)
        )
    
    def unload_lora_adapter(self, lora_name: str):
        """å¸è½½LoRAé€‚é…å™¨"""
        obj = UnloadLoRAAdapterReqInput(lora_name=lora_name)
        return asyncio.get_event_loop().run_until_complete(
            self.tokenizer_manager.unload_lora_adapter(obj, None)
        )
```

---

## ğŸ“ æ€»ç»“

SGLangè°ƒåº¦å™¨çš„æ‰©å±•åŠŸèƒ½ä½“ç°äº†ç°ä»£æ¨ç†ç³»ç»Ÿåœ¨çµæ´»æ€§å’Œå¯ç»´æŠ¤æ€§æ–¹é¢çš„ç²¾å¿ƒè®¾è®¡ï¼š

### ğŸ¯ æ ¸å¿ƒè®¾è®¡åŸåˆ™

**å§”æ‰˜æ¨¡å¼**: è°ƒåº¦å™¨å±‚é¢çš„æ‰©å±•åŠŸèƒ½å¤§å¤šç›´æ¥å§”æ‰˜ç»™tp_workerå¤„ç†ï¼Œä¿æŒäº†æ¶æ„çš„æ¸…æ™°æ€§å’ŒèŒè´£åˆ†ç¦»ã€‚

**Mixinæ¨¡å¼**: é€šè¿‡SchedulerUpdateWeightsMixinç­‰Mixinç±»ç»„ç»‡ä¸åŒçš„åŠŸèƒ½æ¨¡å—ï¼Œå®ç°äº†è‰¯å¥½çš„ä»£ç ç»„ç»‡å’Œå¯ç»´æŠ¤æ€§ã€‚

**ç®€å•æœ‰æ•ˆ**: é¿å…äº†è¿‡åº¦å¤æ‚çš„è®¾è®¡ï¼Œä¸“æ³¨äºæ ¸å¿ƒåŠŸèƒ½çš„å¯é å®ç°ï¼Œé€šè¿‡ç®€å•çš„æ¥å£æä¾›å¼ºå¤§çš„åŠŸèƒ½ã€‚

**åˆ†å±‚æ¶æ„**: Engineã€TokenizerManagerã€Schedulerã€ModelRunnerå„å±‚èŒè´£æ˜ç¡®ï¼Œæ‰©å±•åŠŸèƒ½åœ¨åˆé€‚çš„å±‚æ¬¡å®ç°ã€‚

### ğŸ”§ å®ç°ç‰¹è‰²

**æºç å‡†ç¡®æ€§**: æœ¬æ–‡æ¡£åŸºäºçœŸå®SGLangæºç ç¼–å†™ï¼Œæ‰€æœ‰æ‰©å±•åŠŸèƒ½å®ç°éƒ½æ¥è‡ªå®é™…ä»£ç ï¼Œç¡®ä¿æŠ€æœ¯å‡†ç¡®æ€§ã€‚

**æ•™å­¦ä¸å®è·µå¹¶é‡**: é‡‡ç”¨"æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ + æºç å®ç°ç»†èŠ‚"çš„åŒé‡ç»“æ„ï¼Œæ—¢ä¾¿äºç†è§£æ‰©å±•åŠŸèƒ½åŸç†ï¼Œåˆæä¾›å®ç°å‚è€ƒã€‚

**å¤æ‚æ€§é€æ˜**: æ˜ç¡®å±•ç¤ºäº†è°ƒåº¦å™¨å±‚é¢çš„ç®€æ´æ¥å£ä¸åº•å±‚å¤æ‚å®ç°çš„å·®å¼‚ï¼Œè®©å¼€å‘è€…äº†è§£å®é™…ç³»ç»Ÿçš„åˆ†å±‚è®¾è®¡ã€‚

### ğŸ“ˆ å…³é”®åŠŸèƒ½äº®ç‚¹

1. **å¤šæ ·åŒ–æƒé‡æ›´æ–°**: æ”¯æŒç£ç›˜ã€åˆ†å¸ƒå¼ã€å¼ é‡ä¸‰ç§æƒé‡æ›´æ–°æ–¹å¼ï¼Œæ»¡è¶³ä¸åŒéƒ¨ç½²éœ€æ±‚
2. **LoRAé€‚é…å™¨ç®¡ç†**: æä¾›åŠ¨æ€LoRAåŠ è½½/å¸è½½åŠŸèƒ½ï¼Œæ”¯æŒæ¨¡å‹èƒ½åŠ›çš„çµæ´»æ‰©å±•
3. **æ•°æ®å¹¶è¡Œæ§åˆ¶**: åŒ…å«è½®è¯¢ã€æœ€çŸ­é˜Ÿåˆ—ã€æœ€å°‘tokenç­‰å¤šç§è´Ÿè½½å‡è¡¡ç­–ç•¥
4. **å†…å­˜ç®¡ç†æ‰©å±•**: æ”¯æŒå†…å­˜å ç”¨çš„åŠ¨æ€é‡Šæ”¾å’Œæ¢å¤æ§åˆ¶
5. **ä¸“å®¶åˆ†å¸ƒè®°å½•**: ä¸ºMoEæ¨¡å‹æä¾›ä¸“å®¶ä½¿ç”¨æƒ…å†µçš„ç›‘æ§å’Œè®°å½•
6. **ç¼“å­˜ç®¡ç†**: æä¾›å®Œæ•´çš„ç¼“å­˜åˆ·æ–°å’ŒçŠ¶æ€é‡ç½®åŠŸèƒ½
7. **æ€§èƒ½æ§åˆ¶**: æ”¯æŒæ¨ç†é€Ÿåº¦çš„åŠ¨æ€è°ƒèŠ‚å’Œæ€§èƒ½è°ƒä¼˜

### ğŸš€ æ¶æ„ä¼˜åŠ¿

- **æ‰©å±•æ€§**: Mixinæ¨¡å¼ä½¿å¾—æ·»åŠ æ–°åŠŸèƒ½å˜å¾—ç®€å•ï¼Œä¸ä¼šå½±å“æ ¸å¿ƒè°ƒåº¦é€»è¾‘
- **å¯ç»´æŠ¤æ€§**: æ¸…æ™°çš„åˆ†å±‚æ¶æ„ä¾¿äºåŠŸèƒ½çš„ç‹¬ç«‹å¼€å‘å’Œç»´æŠ¤
- **å¯é æ€§**: é€šè¿‡å§”æ‰˜æ¨¡å¼å’Œé”™è¯¯å¤„ç†ç¡®ä¿äº†ç³»ç»Ÿçš„ç¨³å®šæ€§
- **çµæ´»æ€§**: æ”¯æŒå¤šç§éƒ¨ç½²åœºæ™¯å’Œä½¿ç”¨æ¨¡å¼çš„åŠ¨æ€é…ç½®

è¿™äº›æ‰©å±•åŠŸèƒ½è™½ç„¶å®ç°ç›¸å¯¹ç®€æ´ï¼Œä½†ä¸ºSGLangæä¾›äº†çµæ´»çš„æ¨¡å‹ç®¡ç†ã€ç³»ç»Ÿæ§åˆ¶å’Œæ€§èƒ½è°ƒä¼˜èƒ½åŠ›ï¼Œæ˜¯æ„å»ºç”Ÿäº§çº§æ¨ç†ç³»ç»Ÿçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚