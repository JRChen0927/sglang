# æ ¸å¿ƒæ•°æ®ç»“æ„

---

SGLangè°ƒåº¦å™¨çš„é«˜æ•ˆè¿è¡Œä¾èµ–äºä¸€ç³»åˆ—ç²¾å¿ƒè®¾è®¡çš„æ•°æ®ç»“æ„ã€‚è¿™äº›æ•°æ®ç»“æ„ä¸ä»…æ‰¿è½½ç€è¯·æ±‚çš„å„ç§ä¿¡æ¯ï¼Œè¿˜è´Ÿè´£æ‰¹æ¬¡ç®¡ç†ã€å†…å­˜åˆ†é…å’Œæ¨¡å‹æ¨ç†çš„åè°ƒã€‚ç†è§£è¿™äº›æ ¸å¿ƒæ•°æ®ç»“æ„æ˜¯æ·±å…¥æŒæ¡SGLangè°ƒåº¦å™¨å·¥ä½œåŸç†çš„åŸºç¡€ã€‚

---

## 1. æ•°æ®æµè½¬æ¶æ„

SGLangé‡‡ç”¨åˆ†å±‚çš„æ•°æ®å¤„ç†æ¶æ„ï¼Œè¯·æ±‚ä»æ¥æ”¶åˆ°æ‰§è¡Œç»å†äº†å››ä¸ªä¸»è¦çš„æ•°æ®ç»“æ„å±‚æ¬¡ï¼Œæ¯ä¸€å±‚éƒ½æœ‰æ˜ç¡®çš„èŒè´£åˆ†å·¥ï¼š

**è°ƒåº¦å™¨å±‚é¢çš„ScheduleBatch**è´Ÿè´£å­˜å‚¨è°ƒåº¦å™¨éœ€è¦çš„æ‰€æœ‰ä¿¡æ¯ï¼ŒåŒ…æ‹¬è¯·æ±‚åˆ—è¡¨ã€å†…å­˜æ± å¼•ç”¨ã€ç¼“å­˜ç®¡ç†ç­‰é«˜å±‚è°ƒåº¦å†³ç­–æ‰€éœ€çš„æ•°æ®ã€‚

**æ¨¡å‹å·¥ä½œå™¨å±‚é¢çš„ModelWorkerBatch**æ˜¯ScheduleBatchçš„ç®€åŒ–ç‰ˆæœ¬ï¼ŒåªåŒ…å«æ¨¡å‹å‰å‘æ¨ç†æ‰€éœ€çš„æ ¸å¿ƒæ•°æ®ï¼Œå»é™¤äº†è°ƒåº¦å™¨ç‰¹æœ‰çš„ç®¡ç†ä¿¡æ¯ã€‚

**æ¨¡å‹æ‰§è¡Œå™¨å±‚é¢çš„ForwardBatch**åŒ…å«æœ€åº•å±‚çš„GPUå¼ é‡æ•°æ®ï¼Œæ˜¯å®é™…åœ¨GPUä¸Šæ‰§è¡Œè®¡ç®—æ—¶ä½¿ç”¨çš„æ•°æ®æ ¼å¼ã€‚

### 1.1 æ•°æ®æµè½¬å¯è§†åŒ–

```mermaid
graph TD
    subgraph "ğŸ”„ æ•°æ®æµè½¬æ¶æ„"
        direction LR
        Req[Req<br/>è¯·æ±‚å¯¹è±¡] -->|ç»„ç»‡æˆæ‰¹æ¬¡| ScheduleBatch[ScheduleBatch<br/>è°ƒåº¦å™¨æ‰¹æ¬¡]
        ScheduleBatch -->|ç®€åŒ–ä¼ é€’| ModelWorkerBatch[ModelWorkerBatch<br/>æ¨¡å‹å·¥ä½œå™¨æ‰¹æ¬¡]
        ModelWorkerBatch -->|å¼ é‡è½¬æ¢| ForwardBatch[ForwardBatch<br/>GPUå‰å‘æ‰¹æ¬¡]
    end

    subgraph "ğŸ“Š æŠ½è±¡å±‚é¢"
        direction TB
        A["ğŸ“‹ è°ƒåº¦å™¨å±‚é¢"]
        B["âš™ï¸ æ¨¡å‹å·¥ä½œå™¨å±‚é¢"] 
        C["ğŸ”¥ GPUæ‰§è¡Œå±‚é¢"]
        A --> B --> C
    end
    
    A --> ScheduleBatch
    B --> ModelWorkerBatch
    C --> ForwardBatch

    subgraph "ğŸ” æ ¸å¿ƒå­—æ®µ"
        direction LR
        ScheduleBatch --> SB_Fields{"reqs: List[Req] | req_to_token_pool | tree_cache"}
        ModelWorkerBatch --> MWB_Fields{"input_ids: Tensor | seq_lens: Tensor | sampling_info"}
        ForwardBatch --> FB_Fields{"positions: Tensor | attn_backend | token_to_kv_pool"}
    end

    style A fill:#e3f2fd,color:#000000,stroke:#333
    style B fill:#f1f8e9,color:#000000,stroke:#333
    style C fill:#fff8e1,color:#000000,stroke:#333
    style Req fill:#f3e5f5,color:#000000,stroke:#333
    style ScheduleBatch fill:#e8f5e8,color:#000000,stroke:#333
    style ModelWorkerBatch fill:#fff3e0,color:#000000,stroke:#333
    style ForwardBatch fill:#ffebee,color:#000000,stroke:#333
    style SB_Fields fill:#f0f4c3,color:#000000,stroke:#333
    style MWB_Fields fill:#e8eaf6,color:#000000,stroke:#333
    style FB_Fields fill:#fce4ec,color:#000000,stroke:#333
```

**å›¾ç¤ºè¯´æ˜**ï¼šä½¿ç”¨å­å›¾ç»“æ„æ¸…æ™°å±•ç¤ºä¸‰ä¸ªç»´åº¦ï¼šæ•°æ®æµè½¬æ¶æ„ï¼ˆä¸»è¦æ•°æ®ç±»ï¼‰ã€æŠ½è±¡å±‚é¢ï¼ˆç³»ç»Ÿå±‚æ¬¡ï¼‰ã€æ ¸å¿ƒå­—æ®µï¼ˆå…³é”®å±æ€§ï¼‰ã€‚çŸ©å½¢èŠ‚ç‚¹è¡¨ç¤ºæ•°æ®ç±»ï¼Œè±å½¢èŠ‚ç‚¹è¡¨ç¤ºå­—æ®µé›†åˆï¼Œå®ç°äº†æ›´å¥½çš„è§†è§‰å±‚æ¬¡æ„Ÿã€‚

è¿™ç§åˆ†å±‚è®¾è®¡ç¡®ä¿äº†æ¯ä¸ªç»„ä»¶åªå¤„ç†ä¸å…¶èŒè´£ç›¸å…³çš„æ•°æ®ï¼Œæé«˜äº†ç³»ç»Ÿçš„æ¨¡å—åŒ–ç¨‹åº¦å’Œæ‰§è¡Œæ•ˆç‡ã€‚

### 1.2 è¯·æ±‚ç”Ÿå‘½å‘¨æœŸå¯è§†åŒ–

```mermaid
graph TD
    subgraph "ğŸ¯ è¯·æ±‚åˆ›å»ºé˜¶æ®µ"
        A1["ç”¨æˆ·è¯·æ±‚<br/>GenerateReqInput"]
        A2["TokenåŒ–å¤„ç†<br/>TokenizedGenerateReqInput"]
        A3["åˆ›å»ºReqå¯¹è±¡<br/>handle_generate_request()"]
    end

    subgraph "ğŸ“‹ é˜Ÿåˆ—ç®¡ç†é˜¶æ®µ"
        B1["ç­‰å¾…é˜Ÿåˆ—<br/>waiting_queue"]
        B2["è¯­æ³•é˜Ÿåˆ—<br/>grammar_queue"]
        B3["ä¼šè¯ç®¡ç†<br/>sessions"]
    end

    subgraph "ğŸ“¦ æ‰¹æ¬¡ç»„ç»‡é˜¶æ®µ"
        C1["PrefillAdder<br/>æ™ºèƒ½é€‰æ‹©"]
        C2["ScheduleBatch<br/>æ‰¹æ¬¡åˆ›å»º"]
        C3["å†…å­˜åˆ†é…<br/>alloc_req_slots()"]
    end

    subgraph "âš¡ æ¨¡å‹æ‰§è¡Œé˜¶æ®µ"
        D1["ModelWorkerBatch<br/>ç®€åŒ–ä¼ é€’"]
        D2["ForwardBatch<br/>GPUå¼ é‡"]
        D3["æ¨¡å‹å‰å‘<br/>forward()"]
    end

    subgraph "ğŸ“¤ ç»“æœå¤„ç†é˜¶æ®µ"
        E1["è¾“å‡ºç”Ÿæˆ<br/>logits processing"]
        E2["æµå¼è¾“å‡º<br/>stream_output()"]
        E3["è¯·æ±‚å®Œæˆ<br/>finished_reason"]
    end

    A1 --> A2 --> A3
    A3 --> B1
    A3 --> B2
    A3 --> B3
    B1 --> C1
    B2 --> C1
    C1 --> C2
    C2 --> C3
    C3 --> D1
    D1 --> D2
    D2 --> D3
    D3 --> E1
    E1 --> E2
    E2 --> E3

    style A1 fill:#e3f2fd,color:#000000,stroke:#333
    style A2 fill:#e3f2fd,color:#000000,stroke:#333
    style A3 fill:#e3f2fd,color:#000000,stroke:#333
    style B1 fill:#f1f8e9,color:#000000,stroke:#333
    style B2 fill:#f1f8e9,color:#000000,stroke:#333
    style B3 fill:#f1f8e9,color:#000000,stroke:#333
    style C1 fill:#fff3e0,color:#000000,stroke:#333
    style C2 fill:#fff3e0,color:#000000,stroke:#333
    style C3 fill:#fff3e0,color:#000000,stroke:#333
    style D1 fill:#ffebee,color:#000000,stroke:#333
    style D2 fill:#ffebee,color:#000000,stroke:#333
    style D3 fill:#ffebee,color:#000000,stroke:#333
    style E1 fill:#f3e5f5,color:#000000,stroke:#333
    style E2 fill:#f3e5f5,color:#000000,stroke:#333
    style E3 fill:#f3e5f5,color:#000000,stroke:#333
```

**å›¾ç¤ºè¯´æ˜**ï¼šè“è‰²è¡¨ç¤ºè¯·æ±‚åˆ›å»ºï¼Œç»¿è‰²è¡¨ç¤ºé˜Ÿåˆ—ç®¡ç†ï¼Œæ©™è‰²è¡¨ç¤ºæ‰¹æ¬¡ç»„ç»‡ï¼Œçº¢è‰²è¡¨ç¤ºæ¨¡å‹æ‰§è¡Œï¼Œç´«è‰²è¡¨ç¤ºç»“æœå¤„ç†ã€‚å±•ç¤ºäº†ä¸€ä¸ªReqä»åˆ›å»ºåˆ°å®Œæˆçš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸã€‚

---

## 2. Reqæ•°æ®ç»“æ„

**æ¶æ„å®šä½**ï¼šReqæ˜¯æ•´ä¸ªSGLangç³»ç»Ÿçš„åŸå­å•ä½ï¼Œæ˜¯æ‰€æœ‰ä¿¡æ¯ï¼ˆç”¨æˆ·è¾“å…¥ã€æ¨¡å‹å‚æ•°ã€å¤„ç†çŠ¶æ€ï¼‰çš„èµ·ç‚¹ã€‚åœ¨æ•°æ®æµè½¬æ¶æ„ä¸­ï¼ŒReqæ‰¿è½½ç€ä»ç”¨æˆ·è¯·æ±‚åˆ°æœ€ç»ˆè¾“å‡ºçš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸä¿¡æ¯ï¼Œæ˜¯åç»­ScheduleBatchã€ModelWorkerBatchã€ForwardBatchç­‰æ‰€æœ‰æ‰¹æ¬¡æ•°æ®ç»“æ„çš„åŸºç¡€æ„å»ºå—ã€‚

Reqç±»æ˜¯SGLangä¸­è¡¨ç¤ºå•ä¸ªè¯·æ±‚çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼ŒåŒ…å«äº†è¯·æ±‚ä»åˆ›å»ºåˆ°å®Œæˆçš„å…¨éƒ¨ä¿¡æ¯ã€‚

### 2.1 æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ

**Reqç±»çš„è®¾è®¡ç†å¿µ**ï¼šReqç±»æ˜¯SGLangä¸­è¡¨ç¤ºå•ä¸ªè¯·æ±‚çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼Œé‡‡ç”¨äº†ä¸°å¯Œçš„å‚æ•°è®¾è®¡æ¥æ”¯æŒç°ä»£å¤§è¯­è¨€æ¨¡å‹æ¨ç†çš„å¤æ‚éœ€æ±‚ã€‚å®ƒä¸ä»…æ‰¿è½½åŸºç¡€çš„è¾“å…¥è¾“å‡ºä¿¡æ¯ï¼Œè¿˜é›†æˆäº†å¤šæ¨¡æ€æ”¯æŒã€ä¼šè¯ç®¡ç†ã€LoRAé€‚é…å™¨ã€åˆ†ç¦»å¼æ¶æ„ã€æ€§èƒ½ä¼˜åŒ–ç­‰é«˜çº§åŠŸèƒ½ã€‚

**è®¾è®¡ç‰¹è‰²**ï¼š
- **ç”Ÿå‘½å‘¨æœŸå®Œæ•´æ€§**ï¼šä»è¯·æ±‚åˆ›å»ºåˆ°ç»“æœè¾“å‡ºçš„å…¨ç¨‹çŠ¶æ€è·Ÿè¸ª
- **å¤šåœºæ™¯å…¼å®¹**ï¼šç»Ÿä¸€æ”¯æŒæ–‡æœ¬ç”Ÿæˆã€åµŒå…¥è®¡ç®—ã€å¤šæ¨¡æ€æ¨ç†
- **æ€§èƒ½ä¼˜åŒ–é›†æˆ**ï¼šå†…ç½®å‰ç¼€ç¼“å­˜ã€åˆ†å—å¤„ç†ã€æµå¼è¾“å‡ºç­‰ä¼˜åŒ–æœºåˆ¶
- **åˆ†å¸ƒå¼æ¶æ„æ”¯æŒ**ï¼šåŸç”Ÿæ”¯æŒåˆ†ç¦»å¼æ¨ç†å’Œå¤šç§å¹¶è¡Œæ¨¡å¼
- **æ‰©å±•æ€§è®¾è®¡**ï¼šé€šè¿‡å¯é€‰å­—æ®µæ”¯æŒæœªæ¥åŠŸèƒ½æ‰©å±•

### 2.2 å®Œæ•´å­—æ®µå®šä¹‰

SGLangçš„Reqç±»åŒ…å«äº†æ”¯æŒç°ä»£å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ‰€éœ€çš„å…¨éƒ¨å­—æ®µï¼Œä»¥ä¸‹æ˜¯å®Œæ•´çš„å­—æ®µå®šä¹‰ï¼š

```python
class Req:
    """SGLangè¯·æ±‚å¯¹è±¡çš„å®Œæ•´å®ç°"""
    
    def __init__(
        self,
        rid: str,                                    # è¯·æ±‚å”¯ä¸€æ ‡è¯†ç¬¦
        origin_input_text: str,                      # åŸå§‹è¾“å…¥æ–‡æœ¬
        origin_input_ids: List[int],                 # åŸå§‹è¾“å…¥tokenåºåˆ—
        sampling_params: SamplingParams,             # é‡‡æ ·å‚æ•°é…ç½®
        return_logprob: bool = False,                # æ˜¯å¦è¿”å›å¯¹æ•°æ¦‚ç‡
        top_logprobs_num: int = 0,                  # top-kå¯¹æ•°æ¦‚ç‡æ•°é‡
        token_ids_logprob: List[int] = None,        # æŒ‡å®štokençš„å¯¹æ•°æ¦‚ç‡
        stream: bool = False,                        # æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
        origin_input_ids_unpadded: Optional[Tuple[int]] = None,  # æœªå¡«å……çš„åŸå§‹è¾“å…¥
        lora_id: Optional[str] = None,              # LoRAé€‚é…å™¨ID
        input_embeds: Optional[List[List[float]]] = None,  # è¾“å…¥åµŒå…¥å‘é‡
        token_type_ids: List[int] = None,           # è·¨ç¼–ç å™¨æ¨¡å‹tokenç±»å‹
        session_id: Optional[str] = None,           # ä¼šè¯ID
        custom_logit_processor: Optional[str] = None,  # è‡ªå®šä¹‰logitå¤„ç†å™¨
        return_hidden_states: bool = False,         # æ˜¯å¦è¿”å›éšè—çŠ¶æ€
        eos_token_ids: Optional[Set[int]] = None,   # ç»“æŸtokené›†åˆ
        bootstrap_host: Optional[str] = None,       # åˆ†ç¦»å¼æ¶æ„å¯åŠ¨ä¸»æœº
        bootstrap_port: Optional[int] = None,       # åˆ†ç¦»å¼æ¶æ„å¯åŠ¨ç«¯å£
        bootstrap_room: Optional[int] = None,       # åˆ†ç¦»å¼æ¶æ„æˆ¿é—´ID
        data_parallel_rank: Optional[int] = None,   # æ•°æ®å¹¶è¡Œrank
        vocab_size: Optional[int] = None,           # è¯æ±‡è¡¨å¤§å°
    ):
        # åŸºç¡€è¯·æ±‚ä¿¡æ¯
        self.rid = rid                              # è¯·æ±‚å”¯ä¸€æ ‡è¯†ç¬¦
        self.origin_input_text = origin_input_text  # åŸå§‹è¾“å…¥æ–‡æœ¬
        self.origin_input_ids_unpadded = (          # æœªå¡«å……çš„åŸå§‹è¾“å…¥
            origin_input_ids_unpadded if origin_input_ids_unpadded 
            else origin_input_ids
        )
        self.origin_input_ids = origin_input_ids    # åŸå§‹è¾“å…¥tokenåºåˆ—
        self.output_ids = []                        # æ¨¡å‹ç”Ÿæˆçš„è¾“å‡ºtokenåºåˆ—
        self.fill_ids = []                          # å®Œæ•´åºåˆ—ï¼ˆè¾“å…¥+è¾“å‡ºï¼‰
        self.session_id = session_id                # ä¼šè¯ID
        self.input_embeds = input_embeds            # è¾“å…¥åµŒå…¥å‘é‡
        
        # è·¨ç¼–ç å™¨æ¨¡å‹æ”¯æŒ
        self.token_type_ids = token_type_ids        # tokenç±»å‹æ ‡è¯†
        
        # å±€éƒ¨æ³¨æ„åŠ›ç›¸å…³
        self.evicted_seqlen_local = 0               # å±€éƒ¨æ³¨æ„åŠ›ä¸­è¢«ç§»é™¤çš„åºåˆ—é•¿åº¦
        
        # é‡‡æ ·é…ç½®
        if isinstance(sampling_params.custom_params, dict):
            sampling_params = copy.copy(sampling_params)
            sampling_params.custom_params = sampling_params.custom_params | {
                "__req__": self
            }
        self.sampling_params = sampling_params       # é‡‡æ ·å‚æ•°
        self.custom_logit_processor = custom_logit_processor  # è‡ªå®šä¹‰logitå¤„ç†å™¨
        self.return_hidden_states = return_hidden_states      # æ˜¯å¦è¿”å›éšè—çŠ¶æ€
        self.lora_id = lora_id                      # LoRAé€‚é…å™¨ID
        
        # å†…å­˜æ± ç®¡ç†
        self.req_pool_idx: Optional[int] = None     # è¯·æ±‚æ± ç´¢å¼•
        
        # çŠ¶æ€ç®¡ç†
        self.tokenizer = None                       # tokenizerå¼•ç”¨
        self.finished_reason = None                 # å®ŒæˆåŸå› 
        self.finished_output = None                 # æ˜¯å¦å®Œæˆè¾“å‡º
        self.to_abort = False                       # æ˜¯å¦éœ€è¦ä¸­æ­¢
        self.to_abort_message: str = None           # ä¸­æ­¢æ¶ˆæ¯
        self.stream = stream                        # æµå¼è¾“å‡ºæ ‡å¿—
        self.eos_token_ids = eos_token_ids         # ç»“æŸtokené›†åˆ
        self.vocab_size = vocab_size               # è¯æ±‡è¡¨å¤§å°
        
        # å¢é‡è§£ç æ”¯æŒ
        self.surr_offset = None                    # ç¯ç»•åç§»é‡
        self.read_offset = None                    # è¯»å–åç§»é‡
        self.decoded_text = ""                     # å·²è§£ç æ–‡æœ¬
        
        # å¤šæ¨¡æ€è¾“å…¥
        self.multimodal_inputs: Optional[MultimodalInputs] = None  # å¤šæ¨¡æ€è¾“å…¥
        
        # å‰ç¼€ç¼“å­˜ä¿¡æ¯
        self.prefix_indices: torch.Tensor = []     # å‰ç¼€ç¼“å­˜ç´¢å¼•
        self.extend_input_len = 0                  # éœ€è¦é¢„å¡«å……çš„tokenæ•°é‡
        self.extend_logprob_start_len = 0          # æ‰©å±•æ‰¹æ¬¡ä¸­çš„ç›¸å¯¹logprobèµ·å§‹é•¿åº¦
        self.last_node: Any = None                 # å‰ç¼€æ ‘æœ€åèŠ‚ç‚¹
        self.last_host_node: Any = None            # ä¸»æœºç«¯å‰ç¼€æ ‘æœ€åèŠ‚ç‚¹
        self.host_hit_length = 0                   # ä¸»æœºå‘½ä¸­é•¿åº¦
        self.swa_uuid_for_lock: Optional[int] = None  # SWAåŸºæ•°æ ‘é”å®šUUID
        
        # åˆ†å—å¤„ç†
        self.is_chunked = 0                        # åˆ†å—è®¡æ•°å™¨
        
        # å›é€€å¤„ç†
        self.is_retracted = False                  # æ˜¯å¦å·²å›é€€
        
        # å¢é‡æµå¼è¾“å‡º
        self.send_token_offset: int = 0            # å‘é€tokenåç§»é‡
        self.send_decode_id_offset: int = 0        # å‘é€è§£ç IDåç§»é‡
        self.send_output_token_logprobs_offset: int = 0  # å‘é€è¾“å‡ºtoken logprobsåç§»é‡
        
        # å¯¹æ•°æ¦‚ç‡å‚æ•°
        self.return_logprob = return_logprob       # æ˜¯å¦è¿”å›å¯¹æ•°æ¦‚ç‡
        self.logprob_start_len = 0                 # logprobè®¡ç®—èµ·å§‹ç´¢å¼•
        self.top_logprobs_num = top_logprobs_num   # top-k logprobsæ•°é‡
        self.token_ids_logprob = token_ids_logprob # æŒ‡å®štokençš„logprob
        self.temp_scaled_logprobs = False          # æ¸©åº¦ç¼©æ”¾logprobs
        self.top_p_normalized_logprobs = False     # top-på½’ä¸€åŒ–logprobs
        
        # å¯¹æ•°æ¦‚ç‡è¿”å›å€¼
        self.input_logprob_sent: bool = False      # è¾“å…¥logprobæ˜¯å¦å·²å‘é€
        self.input_token_logprobs_val: Optional[List[float]] = None     # è¾“å…¥token logprobså€¼
        self.input_token_logprobs_idx: Optional[List[int]] = None       # è¾“å…¥token logprobsç´¢å¼•
        self.input_top_logprobs_val: Optional[List[float]] = None       # è¾“å…¥top logprobså€¼
        self.input_top_logprobs_idx: Optional[List[int]] = None         # è¾“å…¥top logprobsç´¢å¼•
        self.input_token_ids_logprobs_val: Optional[List[float]] = None # è¾“å…¥æŒ‡å®štoken logprobså€¼
        self.input_token_ids_logprobs_idx: Optional[List[int]] = None   # è¾“å…¥æŒ‡å®štoken logprobsç´¢å¼•
        self.input_token_logprobs: Optional[List[Tuple[int]]] = None    # è¾“å…¥token logprobsä¸´æ—¶å­˜å‚¨
        self.temp_input_top_logprobs_val: Optional[List[torch.Tensor]] = None  # ä¸´æ—¶è¾“å…¥top logprobså€¼
        self.temp_input_top_logprobs_idx: Optional[List[int]] = None           # ä¸´æ—¶è¾“å…¥top logprobsç´¢å¼•
        self.temp_input_token_ids_logprobs_val: Optional[List[float]] = None   # ä¸´æ—¶è¾“å…¥æŒ‡å®štoken logprobså€¼
        self.temp_input_token_ids_logprobs_idx: Optional[List[int]] = None     # ä¸´æ—¶è¾“å…¥æŒ‡å®štoken logprobsç´¢å¼•
        
        # è¾“å‡ºå¯¹æ•°æ¦‚ç‡
        if return_logprob:
            self.output_token_logprobs_val = []     # è¾“å‡ºtoken logprobså€¼
            self.output_token_logprobs_idx = []     # è¾“å‡ºtoken logprobsç´¢å¼•
            self.output_top_logprobs_val = []       # è¾“å‡ºtop logprobså€¼
            self.output_top_logprobs_idx = []       # è¾“å‡ºtop logprobsç´¢å¼•
            self.output_token_ids_logprobs_val = [] # è¾“å‡ºæŒ‡å®štoken logprobså€¼
            self.output_token_ids_logprobs_idx = [] # è¾“å‡ºæŒ‡å®štoken logprobsç´¢å¼•
        else:
            self.output_token_logprobs_val = self.output_token_logprobs_idx = \
                self.output_top_logprobs_val = self.output_top_logprobs_idx = \
                self.output_token_ids_logprobs_val = self.output_token_ids_logprobs_idx = None
        
        # éšè—çŠ¶æ€
        self.hidden_states: List[List[float]] = []  # éšè—çŠ¶æ€åˆ—è¡¨
        self.hidden_states_tensor = None           # éšè—çŠ¶æ€å¼ é‡ï¼ˆPD + MTPæ—¶ä½¿ç”¨ï¼‰
        
        # åµŒå…¥å‘é‡
        self.embedding = None                      # åµŒå…¥å‘é‡ç»“æœ
        
        # çº¦æŸè§£ç 
        self.grammar: Optional[BaseGrammarObject] = None  # è¯­æ³•å¯¹è±¡
        self.grammar_wait_ct = 0                   # è¯­æ³•ç­‰å¾…è®¡æ•°
        
        # ç¼“å­˜ç»Ÿè®¡
        self.cached_tokens = 0                     # å·²ç¼“å­˜çš„tokenæ•°é‡
        self.already_computed = 0                  # å·²è®¡ç®—çš„tokenæ•°é‡
        
        # æŠ•æœºè§£ç 
        self.spec_verify_ct = 0                    # æŠ•æœºè§£ç éªŒè¯è®¡æ•°
        
        # æ€§èƒ½æŒ‡æ ‡
        self.time_stats: TimeStats = TimeStats()  # æ—¶é—´ç»Ÿè®¡
        self.has_log_time_stats: bool = False      # æ˜¯å¦å·²è®°å½•æ—¶é—´ç»Ÿè®¡
        self.queue_time_start = None               # é˜Ÿåˆ—å¼€å§‹æ—¶é—´
        self.queue_time_end = None                 # é˜Ÿåˆ—ç»“æŸæ—¶é—´
        
        # åˆ†ç¦»å¼æ¶æ„
        self.bootstrap_host: str = bootstrap_host  # å¯åŠ¨ä¸»æœº
        self.bootstrap_port: Optional[int] = bootstrap_port  # å¯åŠ¨ç«¯å£
        self.bootstrap_room: Optional[int] = bootstrap_room  # å¯åŠ¨æˆ¿é—´
        self.disagg_kv_sender: Optional[BaseKVSender] = None  # KVå‘é€å™¨
        
        # æ•°æ®å¹¶è¡Œ
        self.data_parallel_rank: Optional[int] = data_parallel_rank  # æ•°æ®å¹¶è¡Œrank
        
        # KVç¼“å­˜å‘é€ç®¡ç†
        self.start_send_idx: int = 0               # KVç¼“å­˜å‘é€èµ·å§‹ç´¢å¼•
        self.tmp_end_idx: int = -1                 # ä¸´æ—¶ç»“æŸç´¢å¼•
        self.metadata_buffer_index: int = -1       # å…ƒæ•°æ®ç¼“å†²åŒºç´¢å¼•
```

### 2.3 å…³é”®æ–¹æ³•

Reqç±»è¿˜åŒ…å«äº†ä¸€ç³»åˆ—é‡è¦çš„æ–¹æ³•æ¥ç®¡ç†è¯·æ±‚çš„ç”Ÿå‘½å‘¨æœŸï¼š

```python
class Req:
    @property
    def seqlen(self):
        """è®¡ç®—å½“å‰åºåˆ—æ€»é•¿åº¦"""
        return len(self.origin_input_ids) + len(self.output_ids)
    
    def extend_image_inputs(self, image_inputs):
        """æ‰©å±•å¤šæ¨¡æ€è¾“å…¥"""
        if self.multimodal_inputs is None:
            self.multimodal_inputs = image_inputs
        else:
            self.multimodal_inputs.merge(image_inputs)
    
    def finished(self) -> bool:
        """æ£€æŸ¥è¯·æ±‚æ˜¯å¦å·²å®Œæˆ"""
        return self.finished_reason is not None
    
    def init_next_round_input(self, tree_cache: Optional[BasePrefixCache] = None):
        """åˆå§‹åŒ–ä¸‹ä¸€è½®è¾“å…¥"""
        self.fill_ids = self.origin_input_ids + self.output_ids
        if tree_cache is not None:
            if isinstance(tree_cache, LoRARadixCache):
                (
                    self.prefix_indices,
                    self.last_node,
                    self.last_host_node,
                    self.host_hit_length,
                ) = tree_cache.match_prefix_with_lora_id(
                    key=LoRAKey(
                        lora_id=self.lora_id, token_ids=self.adjust_max_prefix_ids()
                    ),
                )
            else:
                (
                    self.prefix_indices,
                    self.last_node,
                    self.last_host_node,
                    self.host_hit_length,
                ) = tree_cache.match_prefix(
                    key=self.adjust_max_prefix_ids(),
                )
        self.extend_input_len = len(self.fill_ids) - len(self.prefix_indices)
    
    def adjust_max_prefix_ids(self):
        """è°ƒæ•´æœ€å¤§å‰ç¼€é•¿åº¦"""
        self.fill_ids = self.origin_input_ids + self.output_ids
        input_len = len(self.fill_ids)
        
        # ç¡®ä¿æ¯ä¸ªè¯·æ±‚è‡³å°‘æœ‰ä¸€ä¸ªtoken
        max_prefix_len = input_len - 1
        
        if self.sampling_params.max_new_tokens > 0:
            # éœ€è¦è‡³å°‘ä¸€ä¸ªtokenæ¥è®¡ç®—logits
            max_prefix_len = min(max_prefix_len, input_len - 1)
        
        if self.return_logprob:
            max_prefix_len = min(max_prefix_len, self.logprob_start_len)
        
        max_prefix_len = max(max_prefix_len, 0)
        return self.fill_ids[:max_prefix_len]
    
    def init_incremental_detokenize(self):
        """åˆå§‹åŒ–å¢é‡å»tokenåŒ–"""
        first_iter = self.surr_offset is None or self.read_offset is None
        
        if first_iter:
            self.read_offset = len(self.origin_input_ids_unpadded)
            self.surr_offset = max(
                self.read_offset - INIT_INCREMENTAL_DETOKENIZATION_OFFSET, 0
            )
        
        all_ids = self.origin_input_ids_unpadded + self.output_ids
        return all_ids[self.surr_offset :], self.read_offset - self.surr_offset
    
    def check_finished(self):
        """æ£€æŸ¥è¯·æ±‚æ˜¯å¦åº”è¯¥ç»“æŸ"""
        if self.finished():
            return
        
        if self.to_abort:
            self.finished_reason = FINISH_ABORT(
                message=self.to_abort_message,
            )
            return
        
        # æ£€æŸ¥å„ç§ç»“æŸæ¡ä»¶...
```





---

## 3. ScheduleBatchæ•°æ®ç»“æ„

**æ¶æ„å®šä½**ï¼šScheduleBatchæ˜¯æ•°æ®æµè½¬æ¶æ„ä¸­çš„è°ƒåº¦å™¨å±‚æŠ½è±¡ï¼Œè´Ÿè´£å°†å¤šä¸ªReqå¯¹è±¡ç»„ç»‡æˆæ‰¹æ¬¡å¹¶ç®¡ç†è°ƒåº¦ç›¸å…³çš„èµ„æºã€‚å®ƒæ˜¯è¿æ¥ä¸Šå±‚è°ƒåº¦å†³ç­–å’Œä¸‹å±‚æ¨¡å‹æ‰§è¡Œçš„å…³é”®æ¡¥æ¢ï¼ŒåŒ…å«äº†å†…å­˜æ± å¼•ç”¨ã€ç¼“å­˜ç®¡ç†ã€å¹¶è¡Œé…ç½®ç­‰è°ƒåº¦å™¨ç‰¹æœ‰çš„ç®¡ç†ä¿¡æ¯ã€‚

ScheduleBatchæ˜¯è°ƒåº¦å™¨å±‚é¢çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼Œè´Ÿè´£ç®¡ç†ä¸€ä¸ªæ‰¹æ¬¡ä¸­æ‰€æœ‰è¯·æ±‚çš„ä¿¡æ¯å’Œèµ„æºã€‚

### 3.1 æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ

**ScheduleBatchçš„è®¾è®¡ç†å¿µ**ï¼šScheduleBatchæ˜¯SGLangæ‰¹å¤„ç†ç³»ç»Ÿçš„æ ¸å¿ƒæŠ½è±¡ï¼Œå®ƒå°†å¤šä¸ªç‹¬ç«‹çš„Reqå¯¹è±¡ç»„ç»‡æˆä¸€ä¸ªç»Ÿä¸€çš„æ‰§è¡Œå•å…ƒï¼ŒåŒæ—¶ç®¡ç†æ‰€æœ‰ç›¸å…³çš„èµ„æºå¼•ç”¨å’Œæ‰§è¡Œé…ç½®ã€‚å…¶è®¾è®¡å……åˆ†ä½“ç°äº†ç°ä»£æ¨ç†ç³»ç»Ÿå¯¹æ€§èƒ½ã€å†…å­˜æ•ˆç‡å’ŒåŠŸèƒ½ä¸°å¯Œæ€§çš„è¦æ±‚ã€‚

**æ ¸å¿ƒè®¾è®¡åŸåˆ™**ï¼š
- **èµ„æºç»Ÿä¸€ç®¡ç†**ï¼šé›†ä¸­ç®¡ç†å†…å­˜æ± ã€ç¼“å­˜ã€åˆ†é…å™¨ç­‰å…³é”®èµ„æº
- **æ‰¹é‡åŒ–ä¼˜åŒ–**ï¼šå°†ä¸ªä½“è¯·æ±‚çš„æ ‡é‡æ•°æ®è½¬æ¢ä¸ºæ‰¹é‡å¼ é‡ï¼Œæ”¯æŒGPUå¹¶è¡Œè®¡ç®—
- **çŠ¶æ€åè°ƒ**ï¼šç»Ÿä¸€ç®¡ç†æ‰¹æ¬¡ä¸­æ‰€æœ‰è¯·æ±‚çš„æ‰§è¡ŒçŠ¶æ€å’Œé…ç½®
- **æ¨¡å—åŒ–æ¥å£**ï¼šé€šè¿‡æ¸…æ™°çš„æ¥å£ä¸è°ƒåº¦å™¨å…¶ä»–ç»„ä»¶åä½œ
- **æ‰©å±•æ€§è®¾è®¡**ï¼šæ”¯æŒå¤šç§å‰å‘æ¨¡å¼ã€å¹¶è¡Œç­–ç•¥å’Œé«˜çº§ä¼˜åŒ–åŠŸèƒ½

### 3.2 å®Œæ•´å­—æ®µå®šä¹‰

```python
@dataclasses.dataclass
class ScheduleBatch(ScheduleBatchDisaggregationDecodeMixin):
    """è°ƒåº¦å™¨æ‰¹æ¬¡çš„å®Œæ•´å®ç°"""
    
    # è¯·æ±‚ã€å†…å­˜æ± å’Œç¼“å­˜ç®¡ç†
    reqs: List[Req]                                           # æ‰¹æ¬¡ä¸­çš„è¯·æ±‚åˆ—è¡¨
    req_to_token_pool: ReqToTokenPool = None                 # è¯·æ±‚åˆ°tokenæ± æ˜ å°„
    token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator = None  # KVç¼“å­˜åˆ†é…å™¨
    tree_cache: BasePrefixCache = None                       # å‰ç¼€ç¼“å­˜æ ‘
    is_hybrid: bool = False                                  # æ˜¯å¦ä¸ºæ··åˆæ¨¡å¼
    
    # æ‰¹æ¬¡é…ç½®
    model_config: ModelConfig = None                         # æ¨¡å‹é…ç½®
    forward_mode: ForwardMode = None                         # å‰å‘æ¨¡å¼
    enable_overlap: bool = False                             # æ˜¯å¦å¯ç”¨é‡å å¤„ç†
    batch_is_full: bool = False                             # æ‰¹æ¬¡æ˜¯å¦å·²æ»¡
    
    # äº‹ä»¶åŒæ­¥
    launch_done: Optional[threading.Event] = None           # å¯åŠ¨å®Œæˆäº‹ä»¶
    
    # åˆ†å—é¢„å¡«å……
    chunked_req: Optional[Req] = None                       # å½“å‰åˆ†å—è¯·æ±‚
    
    # é‡‡æ ·ä¿¡æ¯
    sampling_info: SamplingBatchInfo = None                 # å½“å‰æ‰¹æ¬¡é‡‡æ ·ä¿¡æ¯
    next_batch_sampling_info: SamplingBatchInfo = None      # ä¸‹ä¸€æ‰¹æ¬¡é‡‡æ ·ä¿¡æ¯
    
    # ä¼ é€’ç»™æ¨¡å‹æ‰§è¡Œå™¨çš„æ‰¹é‡åŒ–å‚æ•°
    input_ids: torch.Tensor = None                          # è¾“å…¥token IDå¼ é‡ [b]
    input_embeds: torch.Tensor = None                       # è¾“å…¥åµŒå…¥å¼ é‡ [b, hidden_size]
    token_type_ids: torch.Tensor = None                     # tokenç±»å‹IDå¼ é‡ [b]
    req_pool_indices: torch.Tensor = None                   # è¯·æ±‚æ± ç´¢å¼•å¼ é‡ [b]
    seq_lens: torch.Tensor = None                           # åºåˆ—é•¿åº¦å¼ é‡ [b]
    out_cache_loc: torch.Tensor = None                      # KVç¼“å­˜è¾“å‡ºä½ç½®å¼ é‡ [b]
    output_ids: torch.Tensor = None                         # è¾“å‡ºtoken IDå¼ é‡ [b]
    
    # å¤šæ¨¡æ€è¾“å…¥
    multimodal_inputs: Optional[List] = None                # å¤šæ¨¡æ€è¾“å…¥åˆ—è¡¨
    
    # åºåˆ—é•¿åº¦ç»Ÿè®¡
    seq_lens_sum: int = None                                # æ‰€æœ‰åºåˆ—é•¿åº¦æ€»å’Œ
    orig_seq_lens: torch.Tensor = None                      # åŸå§‹åºåˆ—é•¿åº¦å¼ é‡ [b] (Qwen-1Mç›¸å…³)
    
    # DPæ³¨æ„åŠ›ä¼˜åŒ–
    global_num_tokens: Optional[List[int]] = None           # å…¨å±€tokenæ•°é‡
    global_num_tokens_for_logprob: Optional[List[int]] = None  # ç”¨äºlogprobçš„å…¨å±€tokenæ•°é‡
    is_extend_in_batch: bool = False                        # æ‰¹æ¬¡ä¸­æ˜¯å¦æœ‰æ‰©å±•è¯·æ±‚
    can_run_dp_cuda_graph: bool = False                     # æ˜¯å¦å¯è¿è¡ŒDP CUDAå›¾
    tbo_split_seq_index: Optional[int] = None               # TBOåˆ†å‰²åºåˆ—ç´¢å¼•
    global_forward_mode: Optional[ForwardMode] = None       # å…¨å±€å‰å‘æ¨¡å¼
    
    # å¯¹æ•°æ¦‚ç‡å¤„ç†
    return_logprob: bool = False                            # æ˜¯å¦è¿”å›å¯¹æ•°æ¦‚ç‡
    top_logprobs_nums: Optional[List[int]] = None           # top logprobsæ•°é‡åˆ—è¡¨
    token_ids_logprobs: Optional[List[List[int]]] = None    # token ID logprobsåˆ—è¡¨
    
    # logitså’Œlogprobåå¤„ç†
    temp_scaled_logprobs: bool = False                      # æ¸©åº¦ç¼©æ”¾logprobs
    top_p_normalized_logprobs: bool = False                 # top-på½’ä¸€åŒ–logprobs
    
    # æ‰©å±•å’Œæ··åˆåˆ†å—é¢„å¡«å……
    prefix_lens: List[int] = None                           # å‰ç¼€é•¿åº¦åˆ—è¡¨
    extend_lens: List[int] = None                           # æ‰©å±•é•¿åº¦åˆ—è¡¨
    extend_num_tokens: Optional[int] = None                 # æ‰©å±•tokenæ•°é‡
    decoding_reqs: List[Req] = None                         # è§£ç è¯·æ±‚åˆ—è¡¨
    extend_logprob_start_lens: List[int] = None             # æ‰©å±•logprobèµ·å§‹é•¿åº¦åˆ—è¡¨
    extend_input_logprob_token_ids: Optional[torch.Tensor] = None  # æ‰©å±•è¾“å…¥logprob token IDs
    
    # ç¼–ç å™¨-è§£ç å™¨æ¶æ„
    encoder_cached: Optional[List[bool]] = None             # ç¼–ç å™¨ç¼“å­˜çŠ¶æ€åˆ—è¡¨
    encoder_lens: Optional[torch.Tensor] = None             # ç¼–ç å™¨é•¿åº¦å¼ é‡
    encoder_lens_cpu: Optional[List[int]] = None            # CPUä¸Šçš„ç¼–ç å™¨é•¿åº¦åˆ—è¡¨
    encoder_out_cache_loc: Optional[torch.Tensor] = None    # ç¼–ç å™¨è¾“å‡ºç¼“å­˜ä½ç½®
    
    # æµå¼è¾“å‡º
    has_stream: bool = False                                # æ˜¯å¦æœ‰æµå¼è¯·æ±‚
    
    # è¯­æ³•çº¦æŸ
    has_grammar: bool = False                               # æ˜¯å¦æœ‰è¯­æ³•çº¦æŸ
    
    # è®¾å¤‡é…ç½®
    device: str = "cuda"                                    # è®¾å¤‡ç±»å‹
    
    # æŠ•æœºè§£ç 
    spec_algorithm: SpeculativeAlgorithm = None             # æŠ•æœºè§£ç ç®—æ³•
    spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]] = None  # æŠ•æœºè§£ç ä¿¡æ¯
    
    # éšè—çŠ¶æ€è¿”å›
    return_hidden_states: bool = False                      # æ˜¯å¦è¿”å›éšè—çŠ¶æ€
    
    # ä»…é¢„å¡«å……æ¨¡å¼
    is_prefill_only: bool = False                          # æ˜¯å¦ä¸ºä»…é¢„å¡«å……æ¨¡å¼
    
    # åˆ†å±‚ç¼“å­˜
    hicache_consumer_index: int = 0                         # åˆ†å±‚ç¼“å­˜æ¶ˆè´¹è€…ç´¢å¼•
```

### 3.3 æ ¸å¿ƒæ–¹æ³•

ScheduleBatchåŒ…å«äº†ä¸°å¯Œçš„æ–¹æ³•æ¥ç®¡ç†æ‰¹æ¬¡çš„ç”Ÿå‘½å‘¨æœŸï¼š

```python
class ScheduleBatch:
    @classmethod
    def init_new(
        cls,
        reqs: List[Req],
        req_to_token_pool: ReqToTokenPool,
        token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator,
        tree_cache: BasePrefixCache,
        model_config: ModelConfig,
        enable_overlap: bool,
        spec_algorithm: SpeculativeAlgorithm,
        chunked_req: Optional[Req] = None,
    ):
        """åˆ›å»ºæ–°çš„è°ƒåº¦æ‰¹æ¬¡"""
        return_logprob = any(req.return_logprob for req in reqs)
        
        is_hybrid = False
        if isinstance(token_to_kv_pool_allocator, SWATokenToKVPoolAllocator):
            assert (
                tree_cache is None
                or isinstance(tree_cache, SWARadixCache)
                or isinstance(tree_cache, SWAChunkCache)
            ), "SWARadixCache or SWAChunkCache is required for SWATokenToKVPoolAllocator"
            is_hybrid = True
        
        return cls(
            reqs=reqs,
            req_to_token_pool=req_to_token_pool,
            token_to_kv_pool_allocator=token_to_kv_pool_allocator,
            tree_cache=tree_cache,
            is_hybrid=is_hybrid,
            model_config=model_config,
            enable_overlap=enable_overlap,
            return_logprob=return_logprob,
            has_stream=any(req.stream for req in reqs),
            has_grammar=any(req.grammar for req in reqs),
            device=req_to_token_pool.device,
            spec_algorithm=spec_algorithm,
            chunked_req=chunked_req,
        )
    
    def alloc_req_slots(self, num_reqs: int):
        """åˆ†é…è¯·æ±‚æ§½ä½"""
        req_pool_indices = self.req_to_token_pool.alloc(num_reqs)
        if req_pool_indices is None:
            raise RuntimeError(
                "alloc_req_slots runs out of memory. "
                "Please set a smaller number for `--max-running-requests`. "
                f"{self.req_to_token_pool.available_size()=}, "
                f"{num_reqs=}, "
            )
        return req_pool_indices
    
    def alloc_token_slots(self, num_tokens: int, backup_state: bool = False):
        """åˆ†é…tokenæ§½ä½"""
        # å¦‚æœéœ€è¦ï¼Œå…ˆæ¸…ç†æ ‘ç¼“å­˜
        self._evict_tree_cache_if_needed(num_tokens)
        
        # å¤‡ä»½çŠ¶æ€ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if backup_state:
            state = self.token_to_kv_pool_allocator.backup_state()
        
        # åˆ†é…KVç¼“å­˜ç©ºé—´
        out_cache_loc = self.token_to_kv_pool_allocator.alloc(num_tokens)
        if out_cache_loc is None:
            phase_str = "Prefill" if self.forward_mode.is_extend() else "Decode"
            error_msg = (
                f"{phase_str} out of memory. Try to lower your batch size.\n"
                f"Try to allocate {num_tokens} tokens.\n"
            )
            logger.error(error_msg)
            raise RuntimeError(error_msg)
        
        if backup_state:
            return out_cache_loc, state
        else:
            return out_cache_loc
    
    def free_req_slots(self):
        """é‡Šæ”¾è¯·æ±‚æ§½ä½"""
        if self.req_pool_indices is not None:
            self.req_to_token_pool.free(self.req_pool_indices)
    
    def free_token_slots(self):
        """é‡Šæ”¾tokenæ§½ä½"""
        if self.out_cache_loc is not None:
            self.token_to_kv_pool_allocator.free(self.out_cache_loc)
    
    def prepare_for_extend(self):
        """å‡†å¤‡æ‰©å±•æ¨¡å¼"""
        self.forward_mode = ForwardMode.EXTEND
        
        # åˆ†é…è¯·æ±‚æ§½ä½
        bs = len(self.reqs)
        req_pool_indices = self.alloc_req_slots(bs)
        
        # åˆå§‹åŒ–å¼ é‡
        reqs = self.reqs
        input_ids = [r.fill_ids[len(r.prefix_indices) :] for r in reqs]
        extend_num_tokens = sum(len(ids) for ids in input_ids)
        seq_lens = [len(r.fill_ids) for r in reqs]
        orig_seq_lens = [max(len(r.fill_ids), len(r.origin_input_ids)) for r in reqs]
        prefix_lens = [len(r.prefix_indices) for r in reqs]
        extend_lens = [r.extend_input_len for r in reqs]
        
        # ... æ›´å¤šåˆå§‹åŒ–é€»è¾‘
    
    def prepare_for_decode(self):
        """å‡†å¤‡è§£ç æ¨¡å¼"""
        self.forward_mode = ForwardMode.DECODE
        
        # ... è§£ç æ¨¡å¼åˆå§‹åŒ–é€»è¾‘
```



---

## 4. ModelWorkerBatchæ•°æ®ç»“æ„

**æ¶æ„å®šä½**ï¼šModelWorkerBatchæ˜¯æ•°æ®æµè½¬æ¶æ„ä¸­çš„æ¨¡å‹å·¥ä½œå™¨å±‚æŠ½è±¡ï¼Œä½œä¸ºScheduleBatchå‘æ¨¡å‹æ‰§è¡Œå™¨ä¼ é€’çš„ä¸­é—´å±‚ã€‚å®ƒå»é™¤äº†è°ƒåº¦å™¨ç‰¹æœ‰çš„ç®¡ç†ä¿¡æ¯ï¼ˆå¦‚å†…å­˜æ± å¼•ç”¨ã€ç¼“å­˜ç®¡ç†ï¼‰ï¼Œåªä¿ç•™æ¨¡å‹æ¨ç†æ‰€éœ€çš„æ ¸å¿ƒæ•°æ®ã€‚

ModelWorkerBatchæ˜¯ScheduleBatchå‘æ¨¡å‹å·¥ä½œå™¨ä¼ é€’çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œå»é™¤äº†è°ƒåº¦å™¨ç‰¹æœ‰çš„ç®¡ç†ä¿¡æ¯ï¼Œä¸“æ³¨äºæ¨¡å‹æ¨ç†æ‰€éœ€çš„æ ¸å¿ƒæ•°æ®ã€‚

### 4.1 æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ

**ModelWorkerBatchçš„è®¾è®¡ç†å¿µ**ï¼šModelWorkerBatchæ˜¯æ•°æ®æµè½¬ä¸­çš„å…³é”®ä¸­é—´å±‚ï¼Œå®ƒä»ScheduleBatchä¸­æå–æ¨¡å‹æ¨ç†æ‰€éœ€çš„æ ¸å¿ƒä¿¡æ¯ï¼Œå»é™¤è°ƒåº¦å™¨ç‰¹æœ‰çš„ç®¡ç†å¼€é”€ã€‚è¿™ç§è®¾è®¡å®ç°äº†å…³æ³¨ç‚¹åˆ†ç¦»ï¼Œè®©æ¨¡å‹æ‰§è¡Œå™¨ä¸“æ³¨äºæ¨ç†è®¡ç®—ï¼Œè€Œä¸éœ€è¦äº†è§£è°ƒåº¦å™¨çš„å†…éƒ¨çŠ¶æ€ã€‚

**ç²¾ç®€åŒ–åŸåˆ™**ï¼š
- **ä¿¡æ¯è¿‡æ»¤**ï¼šåªä¿ç•™æ¨¡å‹æ¨ç†å¿…éœ€çš„æ•°æ®ï¼Œå»é™¤è°ƒåº¦ç®¡ç†ä¿¡æ¯
- **é›¶æ‹·è´ä¼ é€’**ï¼šå¼ é‡æ•°æ®é€šè¿‡å¼•ç”¨ä¼ é€’ï¼Œé¿å…ä¸å¿…è¦çš„å†…å­˜æ‹·è´
- **æ¥å£ç®€åŒ–**ï¼šä¸ºæ¨¡å‹æ‰§è¡Œå™¨æä¾›æ¸…æ™°ã€ç®€æ´çš„æ•°æ®æ¥å£
- **æ€§èƒ½ä¼˜åŒ–**ï¼šå‡å°‘æ•°æ®ä¼ é€’çš„å¼€é”€å’Œå¤æ‚æ€§
- **å‘ä¸‹å…¼å®¹**ï¼šä¿æŒä¸ForwardBatchè½¬æ¢çš„å…¼å®¹æ€§

### 4.2 å®Œæ•´å­—æ®µå®šä¹‰

```python
@dataclasses.dataclass
class ModelWorkerBatch:
    """æ¨¡å‹å·¥ä½œå™¨æ‰¹æ¬¡çš„å®Œæ•´å®ç°"""
    
    # æ‰¹æ¬¡æ ‡è¯†
    bid: int                                                # æ‰¹æ¬¡ID
    forward_mode: ForwardMode                               # å‰å‘æ¨¡å¼
    
    # æ ¸å¿ƒå¼ é‡
    input_ids: torch.Tensor                                 # è¾“å…¥token IDå¼ é‡
    req_pool_indices: torch.Tensor                          # è¯·æ±‚æ± ç´¢å¼•å¼ é‡
    seq_lens: torch.Tensor                                  # åºåˆ—é•¿åº¦å¼ é‡
    out_cache_loc: torch.Tensor                             # KVç¼“å­˜è¾“å‡ºä½ç½®å¼ é‡
    seq_lens_cpu: Optional[torch.Tensor]                    # CPUä¸Šçš„åºåˆ—é•¿åº¦å¼ é‡
    seq_lens_sum: int                                       # åºåˆ—é•¿åº¦æ€»å’Œ
    
    # å¯¹æ•°æ¦‚ç‡ç›¸å…³
    return_logprob: bool                                    # æ˜¯å¦è¿”å›å¯¹æ•°æ¦‚ç‡
    top_logprobs_nums: Optional[List[int]]                  # top logprobsæ•°é‡åˆ—è¡¨
    token_ids_logprobs: Optional[List[List[int]]]           # token ID logprobsåˆ—è¡¨
    
    # DPæ³¨æ„åŠ›ä¼˜åŒ–
    global_num_tokens: Optional[List[int]]                  # å…¨å±€tokenæ•°é‡
    global_num_tokens_for_logprob: Optional[List[int]]      # ç”¨äºlogprobçš„å…¨å±€tokenæ•°é‡
    is_extend_in_batch: bool                                # æ‰¹æ¬¡ä¸­æ˜¯å¦æœ‰æ‰©å±•è¯·æ±‚
    can_run_dp_cuda_graph: bool                             # æ˜¯å¦å¯è¿è¡ŒDP CUDAå›¾
    tbo_split_seq_index: Optional[int]                      # TBOåˆ†å‰²åºåˆ—ç´¢å¼•
    global_forward_mode: Optional[ForwardMode]              # å…¨å±€å‰å‘æ¨¡å¼
    
    # æ‰©å±•æ¨¡å¼ç›¸å…³
    extend_num_tokens: Optional[int]                        # æ‰©å±•tokenæ•°é‡
    extend_seq_lens: Optional[List[int]]                    # æ‰©å±•åºåˆ—é•¿åº¦åˆ—è¡¨
    extend_prefix_lens: Optional[List[int]]                 # æ‰©å±•å‰ç¼€é•¿åº¦åˆ—è¡¨
    extend_logprob_start_lens: Optional[List[int]]          # æ‰©å±•logprobèµ·å§‹é•¿åº¦åˆ—è¡¨
    extend_input_logprob_token_ids: Optional[torch.Tensor]  # æ‰©å±•è¾“å…¥logprob token IDs
    
    # å¤šæ¨¡æ€æ”¯æŒ
    multimodal_inputs: Optional[List[MultimodalInputs]]     # å¤šæ¨¡æ€è¾“å…¥åˆ—è¡¨
    
    # ç¼–ç å™¨-è§£ç å™¨æ¶æ„
    encoder_cached: Optional[List[bool]]                    # ç¼–ç å™¨ç¼“å­˜çŠ¶æ€åˆ—è¡¨
    encoder_lens: Optional[torch.Tensor]                    # ç¼–ç å™¨é•¿åº¦å¼ é‡
    encoder_lens_cpu: Optional[List[int]]                   # CPUä¸Šçš„ç¼–ç å™¨é•¿åº¦åˆ—è¡¨
    encoder_out_cache_loc: Optional[torch.Tensor]           # ç¼–ç å™¨è¾“å‡ºç¼“å­˜ä½ç½®
    
    # LoRAæ”¯æŒ
    lora_ids: Optional[List[str]]                           # LoRAé€‚é…å™¨IDåˆ—è¡¨
    
    # é‡‡æ ·ä¿¡æ¯
    sampling_info: SamplingBatchInfo                        # é‡‡æ ·æ‰¹æ¬¡ä¿¡æ¯
    
    # å¯é€‰å­—æ®µ
    orig_seq_lens: Optional[torch.Tensor] = None            # åŸå§‹åºåˆ—é•¿åº¦ (Qwen-1Mç›¸å…³)
    input_embeds: Optional[torch.Tensor] = None             # è¾“å…¥åµŒå…¥å¼ é‡
    token_type_ids: Optional[torch.Tensor] = None           # è·¨ç¼–ç å™¨æ¨¡å‹tokenç±»å‹ID
    
    # æŠ•æœºè§£ç 
    spec_algorithm: SpeculativeAlgorithm = None             # æŠ•æœºè§£ç ç®—æ³•
    spec_info: Optional[Union[EagleVerifyInput, EagleDraftInput]] = None  # æŠ•æœºè§£ç ä¿¡æ¯
    capture_hidden_mode: CaptureHiddenMode = None           # éšè—çŠ¶æ€æ•è·æ¨¡å¼
    hicache_consumer_index: int = 0                         # åˆ†å±‚ç¼“å­˜æ¶ˆè´¹è€…ç´¢å¼•
    
    # é‡å äº‹ä»¶
    launch_done: Optional[threading.Event] = None           # å¯åŠ¨å®Œæˆäº‹ä»¶
```



---

## 5. ForwardBatchæ•°æ®ç»“æ„

**æ¶æ„å®šä½**ï¼šForwardBatchæ˜¯æ•°æ®æµè½¬æ¶æ„çš„æœ€åº•å±‚ï¼Œä»£è¡¨GPUæ‰§è¡Œå±‚æŠ½è±¡ã€‚å®ƒå°†ModelWorkerBatchçš„é«˜å±‚æ•°æ®è¿›ä¸€æ­¥è½¬æ¢ä¸ºGPUå‹å¥½çš„å¼ é‡æ ¼å¼ï¼ŒåŒ…å«äº†æ¨¡å‹åœ¨GPUä¸Šæ‰§è¡Œå‰å‘ä¼ æ’­æ‰€éœ€çš„æ‰€æœ‰å¼ é‡æ•°æ®å’Œè®¡ç®—èµ„æºå¼•ç”¨ã€‚

ForwardBatchæ˜¯æ•°æ®æµè½¬çš„æœ€åº•å±‚ï¼ŒåŒ…å«GPUæ¨¡å‹æ‰§è¡Œæ—¶çš„æ‰€æœ‰å¼ é‡æ•°æ®ï¼Œæ˜¯å®é™…åœ¨GPUä¸Šæ‰§è¡Œè®¡ç®—çš„æ•°æ®æ ¼å¼ã€‚

### 5.1 æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ

**ForwardBatchçš„è®¾è®¡ç†å¿µ**ï¼šForwardBatchæ˜¯æ•°æ®æµè½¬æ¶æ„çš„æœ€åº•å±‚æŠ½è±¡ï¼Œä¸“é—¨ä¸ºGPUè®¡ç®—ä¼˜åŒ–è®¾è®¡ã€‚å®ƒå°†é«˜å±‚çš„æ‰¹æ¬¡ä¿¡æ¯è½¬æ¢ä¸ºGPUå‹å¥½çš„å¼ é‡æ ¼å¼ï¼ŒåŒ…å«äº†æ¨¡å‹å‰å‘ä¼ æ’­æ‰€éœ€çš„æ‰€æœ‰è®¡ç®—èµ„æºå¼•ç”¨å’Œå¼ é‡æ•°æ®ã€‚

**GPUä¼˜åŒ–åŸåˆ™**ï¼š
- **å¼ é‡åŒ–æ•°æ®**ï¼šæ‰€æœ‰è®¡ç®—è¾“å…¥éƒ½ç»„ç»‡ä¸ºè¿ç»­çš„GPUå¼ é‡
- **å†…å­˜æ•ˆç‡**ï¼šé€šè¿‡å¼•ç”¨ä¼ é€’é¿å…æ•°æ®æ‹·è´ï¼Œä¼˜åŒ–GPUå†…å­˜ä½¿ç”¨
- **è®¡ç®—å‹å¥½**ï¼šæ•°æ®å¸ƒå±€ä¸“é—¨é’ˆå¯¹GPUå¹¶è¡Œè®¡ç®—ä¼˜åŒ–
- **èµ„æºé›†æˆ**ï¼šé›†æˆKVç¼“å­˜ã€æ³¨æ„åŠ›åç«¯ç­‰GPUè®¡ç®—èµ„æº
- **å¤šæ¨¡æ€æ”¯æŒ**ï¼šç»Ÿä¸€å¤„ç†æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰ä¸åŒæ¨¡æ€çš„å¼ é‡æ•°æ®

**ä¸ä¸Šå±‚å·®å¼‚**ï¼š
- **å»é™¤é«˜å±‚æŠ½è±¡**ï¼šä¸åŒ…å«Reqå¯¹è±¡åˆ—è¡¨ï¼Œåªä¿ç•™å¼ é‡æ•°æ®
- **æ·»åŠ GPUèµ„æº**ï¼šåŒ…å«æ³¨æ„åŠ›åç«¯ã€KVç¼“å­˜æ± ç­‰GPUç‰¹æœ‰èµ„æº
- **ä¼˜åŒ–æ•°æ®å¸ƒå±€**ï¼šé’ˆå¯¹GPUè®¡ç®—æ¨¡å¼ä¼˜åŒ–çš„å¼ é‡ç»„ç»‡

### 5.2 å®Œæ•´å­—æ®µå®šä¹‰

ForwardBatchçš„å®Œæ•´å®šä¹‰ä½äº`sglang/srt/model_executor/forward_batch_info.py`ä¸­ï¼ŒåŒ…å«äº†GPUæ¨¡å‹æ‰§è¡Œæ‰€éœ€çš„å…¨éƒ¨å¼ é‡æ•°æ®ï¼š

```python
@dataclass
class ForwardBatch:
    """GPUå‰å‘è®¡ç®—æ‰¹æ¬¡çš„å®Œæ•´å®ç°"""
    
    # åŸºç¡€å‰å‘ä¿¡æ¯
    forward_mode: ForwardMode                               # å‰å‘æ¨¡å¼
    batch_size: int                                         # æ‰¹æ¬¡å¤§å°
    
    # æ ¸å¿ƒè¾“å…¥å¼ é‡
    input_ids: torch.Tensor                                 # è¾“å…¥token IDå¼ é‡ [batch_size]
    req_pool_indices: torch.Tensor                          # è¯·æ±‚æ± ç´¢å¼•å¼ é‡ [batch_size]
    seq_lens: torch.Tensor                                  # åºåˆ—é•¿åº¦å¼ é‡ [batch_size]
    out_cache_loc: torch.Tensor                             # KVç¼“å­˜è¾“å‡ºä½ç½®å¼ é‡ [batch_size]
    seq_lens_sum: int                                       # åºåˆ—é•¿åº¦æ€»å’Œ
    
    # ä½ç½®å’Œæ³¨æ„åŠ›ä¿¡æ¯
    positions: torch.Tensor = None                          # ä½ç½®ç¼–ç å¼ é‡
    seq_lens_cpu: Optional[torch.Tensor] = None             # CPUä¸Šçš„åºåˆ—é•¿åº¦å¼ é‡
    
    # é¢„å¡«å……æ¨¡å¼ä¸“ç”¨
    extend_num_tokens: Optional[int] = None                  # æ‰©å±•tokenæ•°é‡
    extend_seq_lens: Optional[torch.Tensor] = None           # æ‰©å±•åºåˆ—é•¿åº¦å¼ é‡
    extend_start_loc: Optional[torch.Tensor] = None          # æ‰©å±•èµ·å§‹ä½ç½®å¼ é‡
    extend_prefix_lens: Optional[torch.Tensor] = None       # æ‰©å±•å‰ç¼€é•¿åº¦å¼ é‡
    
    # å¤šæ¨¡æ€æ”¯æŒ
    multimodal_inputs: Optional[List] = None                 # å¤šæ¨¡æ€è¾“å…¥æ•°æ®
    input_embeds: Optional[torch.Tensor] = None              # è¾“å…¥åµŒå…¥å¼ é‡
    
    # KVç¼“å­˜å’Œæ³¨æ„åŠ›åç«¯
    req_to_token_pool: ReqToTokenPool = None                # è¯·æ±‚åˆ°tokenæ± æ˜ å°„
    token_to_kv_pool: KVCache = None                        # KVç¼“å­˜æ± å¼•ç”¨
    attn_backend: AttentionBackend = None                   # æ³¨æ„åŠ›åç«¯
    
    # DPæ³¨æ„åŠ›ä¼˜åŒ–
    global_num_tokens_gpu: Optional[torch.Tensor] = None    # å…¨å±€tokenæ•°GPUå¼ é‡
    dp_padding_mode: Optional[DpPaddingMode] = None         # DPå¡«å……æ¨¡å¼
    global_num_tokens: Optional[List[int]] = None           # å…¨å±€tokenæ•°é‡åˆ—è¡¨
    global_num_tokens_for_logprob: Optional[List[int]] = None  # ç”¨äºlogprobçš„å…¨å±€tokenæ•°é‡
    is_extend_in_batch: bool = False                        # æ‰¹æ¬¡ä¸­æ˜¯å¦æœ‰æ‰©å±•è¯·æ±‚
    can_run_dp_cuda_graph: bool = False                     # æ˜¯å¦å¯è¿è¡ŒDP CUDAå›¾
    tbo_split_seq_index: Optional[int] = None               # TBOåˆ†å‰²åºåˆ—ç´¢å¼•
    global_forward_mode: Optional[ForwardMode] = None       # å…¨å±€å‰å‘æ¨¡å¼
    
    # å¯¹æ•°æ¦‚ç‡ç›¸å…³
    return_logprob: bool = False                            # æ˜¯å¦è¿”å›å¯¹æ•°æ¦‚ç‡
    top_logprobs_nums: Optional[List[int]] = None           # top logprobsæ•°é‡åˆ—è¡¨
    token_ids_logprobs: Optional[List[List[int]]] = None    # token ID logprobsåˆ—è¡¨
    extend_logprob_start_lens: Optional[List[int]] = None   # æ‰©å±•logprobèµ·å§‹é•¿åº¦åˆ—è¡¨
    extend_input_logprob_token_ids: Optional[torch.Tensor] = None  # æ‰©å±•è¾“å…¥logprob token IDs
    
    # ç¼–ç å™¨-è§£ç å™¨æ¶æ„
    encoder_cached: Optional[List[bool]] = None             # ç¼–ç å™¨ç¼“å­˜çŠ¶æ€åˆ—è¡¨
    encoder_lens: Optional[torch.Tensor] = None             # ç¼–ç å™¨é•¿åº¦å¼ é‡
    encoder_lens_cpu: Optional[List[int]] = None            # CPUä¸Šçš„ç¼–ç å™¨é•¿åº¦åˆ—è¡¨
    encoder_out_cache_loc: Optional[torch.Tensor] = None    # ç¼–ç å™¨è¾“å‡ºç¼“å­˜ä½ç½®
    
    # LoRAæ”¯æŒ
    lora_ids: Optional[List[str]] = None                    # LoRAé€‚é…å™¨IDåˆ—è¡¨
    
    # é‡‡æ ·ä¿¡æ¯
    sampling_info: SamplingBatchInfo = None                 # é‡‡æ ·æ‰¹æ¬¡ä¿¡æ¯
    
    # æŠ•æœºè§£ç æ”¯æŒ
    spec_algorithm: SpeculativeAlgorithm = None             # æŠ•æœºè§£ç ç®—æ³•
    spec_info: Optional[Union[EagleVerifyInput, EagleDraftInput]] = None  # æŠ•æœºè§£ç ä¿¡æ¯
    
    # éšè—çŠ¶æ€æ•è·
    capture_hidden_mode: CaptureHiddenMode = None           # éšè—çŠ¶æ€æ•è·æ¨¡å¼
    
    # åŸå§‹åºåˆ—é•¿åº¦ (Qwen-1Mç›¸å…³)
    orig_seq_lens: Optional[torch.Tensor] = None            # åŸå§‹åºåˆ—é•¿åº¦å¼ é‡
    
    # è·¨ç¼–ç å™¨æ¨¡å‹æ”¯æŒ
    token_type_ids: Optional[torch.Tensor] = None           # tokenç±»å‹IDå¼ é‡
    
    # åˆ†å±‚ç¼“å­˜
    hicache_consumer_index: int = 0                         # åˆ†å±‚ç¼“å­˜æ¶ˆè´¹è€…ç´¢å¼•
    
    # é‡å è°ƒåº¦
    launch_done: Optional[threading.Event] = None           # å¯åŠ¨å®Œæˆäº‹ä»¶
```

### 5.3 ForwardBatchåˆ›å»ºæ–¹æ³•

```python
@classmethod
def init_new(cls, batch: ModelWorkerBatch, model_runner: ModelRunner):
    """ä»ModelWorkerBatchåˆ›å»ºForwardBatch"""
    return cls(
        # åŸºç¡€ä¿¡æ¯ä»ModelWorkerBatchç›´æ¥å¤åˆ¶
        forward_mode=batch.forward_mode,                    # å‰å‘æ¨¡å¼
        batch_size=len(batch.seq_lens),                     # æ‰¹æ¬¡å¤§å°
        input_ids=batch.input_ids,                          # è¾“å…¥tokenå¼ é‡
        req_pool_indices=batch.req_pool_indices,            # è¯·æ±‚æ± ç´¢å¼•
        seq_lens=batch.seq_lens,                            # åºåˆ—é•¿åº¦
        out_cache_loc=batch.out_cache_loc,                  # è¾“å‡ºç¼“å­˜ä½ç½®
        seq_lens_sum=batch.seq_lens_sum,                    # åºåˆ—é•¿åº¦æ€»å’Œ
        
        # å¤šæ¨¡æ€å’Œç¼–ç å™¨æ”¯æŒ
        multimodal_inputs=batch.multimodal_inputs,          # å¤šæ¨¡æ€è¾“å…¥
        encoder_cached=batch.encoder_cached,                # ç¼–ç å™¨ç¼“å­˜çŠ¶æ€
        encoder_lens=batch.encoder_lens,                    # ç¼–ç å™¨é•¿åº¦
        encoder_lens_cpu=batch.encoder_lens_cpu,            # CPUç¼–ç å™¨é•¿åº¦
        encoder_out_cache_loc=batch.encoder_out_cache_loc,  # ç¼–ç å™¨è¾“å‡ºç¼“å­˜ä½ç½®
        
        # é‡‡æ ·å’Œå¯¹æ•°æ¦‚ç‡
        return_logprob=batch.return_logprob,                # æ˜¯å¦è¿”å›å¯¹æ•°æ¦‚ç‡
        top_logprobs_nums=batch.top_logprobs_nums,          # top-kå¯¹æ•°æ¦‚ç‡æ•°é‡
        token_ids_logprobs=batch.token_ids_logprobs,        # tokenå¯¹æ•°æ¦‚ç‡
        
        # LoRAå’ŒæŠ•æœºè§£ç 
        lora_ids=batch.lora_ids,                            # LoRAé€‚é…å™¨IDåˆ—è¡¨
        sampling_info=batch.sampling_info,                  # é‡‡æ ·ä¿¡æ¯
        spec_algorithm=batch.spec_algorithm,                # æŠ•æœºè§£ç ç®—æ³•
        spec_info=batch.spec_info,                          # æŠ•æœºè§£ç ä¿¡æ¯
        
        # ä»model_runnerè·å–èµ„æºå¼•ç”¨
        req_to_token_pool=model_runner.req_to_token_pool,   # è¯·æ±‚åˆ°tokenæ± 
        token_to_kv_pool=model_runner.token_to_kv_pool,     # KVç¼“å­˜æ± 
        attn_backend=model_runner.attn_backend,             # æ³¨æ„åŠ›åç«¯
        
        # DPæ³¨æ„åŠ›ç›¸å…³
        global_num_tokens=batch.global_num_tokens,          # å…¨å±€tokenæ•°é‡
        global_num_tokens_for_logprob=batch.global_num_tokens_for_logprob,
        is_extend_in_batch=batch.is_extend_in_batch,        # æ‰¹æ¬¡ä¸­æ˜¯å¦æœ‰æ‰©å±•
        can_run_dp_cuda_graph=batch.can_run_dp_cuda_graph,  # æ˜¯å¦å¯è¿è¡ŒDP CUDAå›¾
        tbo_split_seq_index=batch.tbo_split_seq_index,      # TBOåˆ†å‰²åºåˆ—ç´¢å¼•
        global_forward_mode=batch.global_forward_mode,      # å…¨å±€å‰å‘æ¨¡å¼
        
        # æ‰©å±•æ¨¡å¼ç›¸å…³
        extend_num_tokens=batch.extend_num_tokens,          # æ‰©å±•tokenæ•°é‡
        extend_seq_lens=batch.extend_seq_lens,              # æ‰©å±•åºåˆ—é•¿åº¦
        extend_prefix_lens=batch.extend_prefix_lens,        # æ‰©å±•å‰ç¼€é•¿åº¦
        extend_logprob_start_lens=batch.extend_logprob_start_lens,  # æ‰©å±•logprobèµ·å§‹é•¿åº¦
        extend_input_logprob_token_ids=batch.extend_input_logprob_token_ids,  # æ‰©å±•è¾“å…¥logprob token IDs
        
        # å…¶ä»–å­—æ®µ
        orig_seq_lens=batch.orig_seq_lens,                  # åŸå§‹åºåˆ—é•¿åº¦
        input_embeds=batch.input_embeds,                    # è¾“å…¥åµŒå…¥
        token_type_ids=batch.token_type_ids,                # tokenç±»å‹ID
        capture_hidden_mode=batch.capture_hidden_mode,      # éšè—çŠ¶æ€æ•è·æ¨¡å¼
        hicache_consumer_index=batch.hicache_consumer_index, # åˆ†å±‚ç¼“å­˜æ¶ˆè´¹è€…ç´¢å¼•
        launch_done=batch.launch_done,                      # å¯åŠ¨å®Œæˆäº‹ä»¶
    )
```



---

## 6. å†…å­˜ç®¡ç†æ•°æ®ç»“æ„

### 6.1 å†…å­˜ç®¡ç†ç»„ä»¶è®¾è®¡è¯¦è§£

**è®¾è®¡èƒŒæ™¯ä¸æŒ‘æˆ˜**ï¼šå¤§è¯­è¨€æ¨¡å‹æ¨ç†é¢ä¸´ç€ä¸¥å³»çš„å†…å­˜æŒ‘æˆ˜ï¼šKVç¼“å­˜å ç”¨å·¨å¤§ã€è¯·æ±‚é•¿åº¦ä¸ä¸€ã€åŠ¨æ€æ‰¹å¤„ç†éœ€æ±‚ã€‚SGLangçš„å†…å­˜ç®¡ç†ç³»ç»Ÿé€šè¿‡ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶çš„åä½œï¼Œå®ç°äº†é«˜æ•ˆã€çµæ´»çš„å†…å­˜ç®¡ç†ç­–ç•¥ã€‚

**ReqToTokenPool - è¯·æ±‚æ˜ å°„ç®¡ç†å™¨**ï¼š

**è®¾è®¡äº®ç‚¹**ï¼š
- **ç´¢å¼•æŠ½è±¡**ï¼šå°†å¤æ‚çš„å†…å­˜åœ°å€ç®¡ç†æŠ½è±¡ä¸ºç®€å•çš„ç´¢å¼•æ“ä½œ
- **åŠ¨æ€åˆ†é…**ï¼šæ”¯æŒè¯·æ±‚çš„åŠ¨æ€æ·»åŠ å’Œç§»é™¤ï¼Œé€‚åº”è¿ç»­æ‰¹å¤„ç†çš„éœ€æ±‚
- **å†…å­˜å¤ç”¨**ï¼šé€šè¿‡æ± åŒ–æŠ€æœ¯å®ç°å†…å­˜çš„é«˜æ•ˆå¤ç”¨ï¼Œå‡å°‘åˆ†é…/é‡Šæ”¾å¼€é”€
- **å¹¶å‘å®‰å…¨**ï¼šæ”¯æŒå¤šçº¿ç¨‹ç¯å¢ƒä¸‹çš„å®‰å…¨è®¿é—®å’Œä¿®æ”¹

**BaseTokenToKVPoolAllocator - KVç¼“å­˜åˆ†é…å™¨**ï¼š

**è®¾è®¡äº®ç‚¹**ï¼š
- **åˆ†é¡µç®¡ç†**ï¼šé‡‡ç”¨åˆ†é¡µæœºåˆ¶ç®¡ç†KVç¼“å­˜ï¼Œæ”¯æŒçµæ´»çš„å†…å­˜åˆ†é…ç­–ç•¥
- **ç­–ç•¥å¤šæ ·æ€§**ï¼šæŠ½è±¡åŸºç±»è®¾è®¡ï¼Œæ”¯æŒä¸åŒçš„åˆ†é…ç®—æ³•ï¼ˆLRUã€FIFOã€æ··åˆç­–ç•¥ç­‰ï¼‰
- **å†…å­˜é¢„ä¼°**ï¼šæä¾›ç²¾ç¡®çš„å†…å­˜éœ€æ±‚é¢„ä¼°ï¼Œé¿å…OOMé”™è¯¯
- **å›æ”¶ä¼˜åŒ–**ï¼šæ™ºèƒ½çš„å†…å­˜å›æ”¶æœºåˆ¶ï¼Œæ”¯æŒå»¶è¿Ÿé‡Šæ”¾å’Œæ‰¹é‡å›æ”¶

**BasePrefixCache - å‰ç¼€ç¼“å­˜ç³»ç»Ÿ**ï¼š

**è®¾è®¡äº®ç‚¹**ï¼š
- **è®¡ç®—å¤ç”¨**ï¼šè¯†åˆ«å’Œå¤ç”¨è¯·æ±‚é—´çš„å…¬å…±å‰ç¼€ï¼Œæ˜¾è‘—å‡å°‘é‡å¤è®¡ç®—
- **æ ‘å½¢ç»“æ„**ï¼šé‡‡ç”¨é«˜æ•ˆçš„æ ‘å½¢æ•°æ®ç»“æ„ï¼ˆå¦‚Radix Treeï¼‰ç®¡ç†å‰ç¼€å…³ç³»
- **å¼•ç”¨è®¡æ•°**ï¼šé€šè¿‡å¼•ç”¨è®¡æ•°æœºåˆ¶ç¡®ä¿ç¼“å­˜æ•°æ®çš„æ­£ç¡®ç”Ÿå‘½å‘¨æœŸç®¡ç†
- **å¹¶å‘æ§åˆ¶**ï¼šæ”¯æŒå¤šè¯·æ±‚å¹¶å‘è®¿é—®æ—¶çš„ç¼“å­˜ä¸€è‡´æ€§ä¿è¯

### 6.2 å†…å­˜ç®¡ç†åä½œå¯è§†åŒ–

```mermaid
graph TD
    subgraph "ğŸ”„ å†…å­˜åˆ†é…æµç¨‹"
        A1["æ–°è¯·æ±‚åˆ°è¾¾<br/>Reqåˆ›å»º"]
        A2["è¯·æ±‚æ§½ä½åˆ†é…<br/>alloc_req_slots()"]
        A3["Tokenæ§½ä½åˆ†é…<br/>alloc_token_slots()"]
        A4["å‰ç¼€ç¼“å­˜æ£€æŸ¥<br/>tree_cache.match_prefix()"]
    end

    subgraph "ğŸ’¾ å†…å­˜æ± åä½œ"
        B1["ReqToTokenPool<br/>è¯·æ±‚â†’Tokenæ˜ å°„"]
        B2["BaseTokenToKVPoolAllocator<br/>KVç¼“å­˜åˆ†é…å™¨"]
        B3["BasePrefixCache<br/>å‰ç¼€ç¼“å­˜"]
    end

    subgraph "ğŸ—„ï¸ å†…å­˜é‡Šæ”¾æµç¨‹"
        C1["è¯·æ±‚å®Œæˆ<br/>finished_reasonè®¾ç½®"]
        C2["é‡Šæ”¾Tokenæ§½ä½<br/>free_token_slots()"]
        C3["é‡Šæ”¾è¯·æ±‚æ§½ä½<br/>free_req_slots()"]
        C4["ç¼“å­˜å¼•ç”¨å‡å°‘<br/>dec_lock_ref()"]
    end

    A1 --> A2
    A2 --> A3
    A3 --> A4
    A2 --> B1
    A3 --> B2
    A4 --> B3
    C1 --> C2
    C2 --> C3
    C3 --> C4
    C2 --> B2
    C3 --> B1
    C4 --> B3

    style A1 fill:#e3f2fd,color:#000000,stroke:#333
    style A2 fill:#e3f2fd,color:#000000,stroke:#333
    style A3 fill:#e3f2fd,color:#000000,stroke:#333
    style A4 fill:#e3f2fd,color:#000000,stroke:#333
    style B1 fill:#f1f8e9,color:#000000,stroke:#333
    style B2 fill:#f1f8e9,color:#000000,stroke:#333
    style B3 fill:#f1f8e9,color:#000000,stroke:#333
    style C1 fill:#fff3e0,color:#000000,stroke:#333
    style C2 fill:#fff3e0,color:#000000,stroke:#333
    style C3 fill:#fff3e0,color:#000000,stroke:#333
    style C4 fill:#fff3e0,color:#000000,stroke:#333
```

**å›¾ç¤ºè¯´æ˜**ï¼šè“è‰²è¡¨ç¤ºå†…å­˜åˆ†é…æµç¨‹ï¼Œç»¿è‰²è¡¨ç¤ºå†…å­˜æ± åä½œï¼Œæ©™è‰²è¡¨ç¤ºå†…å­˜é‡Šæ”¾æµç¨‹ã€‚å±•ç¤ºäº†SGLangå†…å­˜ç®¡ç†ç»„ä»¶ä¹‹é—´çš„åä½œå…³ç³»å’Œå®Œæ•´çš„å†…å­˜ç”Ÿå‘½å‘¨æœŸã€‚

### 6.3 å†…å­˜åˆ†é…åä½œæœºåˆ¶

**çœŸå®çš„å†…å­˜åˆ†é…æ–¹æ³•**ï¼šSGLangé€šè¿‡ScheduleBatchç±»çš„çœŸå®æ–¹æ³•æ¥ç®¡ç†å†…å­˜åˆ†é…å’Œé‡Šæ”¾ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹å±•ç¤ºçœŸå®çš„å†…å­˜åˆ†é…æ–¹æ³•ï¼Œçœç•¥äº†éƒ¨åˆ†é”™è¯¯å¤„ç†é€»è¾‘ã€‚å®Œæ•´å®ç°è¯·å‚è€ƒ `sglang/srt/managers/schedule_batch.py`ã€‚

```python
# æ¥è‡ªScheduleBatchçš„çœŸå®å†…å­˜åˆ†é…æ–¹æ³•
def alloc_req_slots(self, num_reqs: int):
    """åˆ†é…è¯·æ±‚æ§½ä½çš„çœŸå®æ–¹æ³•"""
    req_pool_indices = self.req_to_token_pool.alloc(num_reqs)  # åˆ†é…è¯·æ±‚æ± ç´¢å¼•
    if req_pool_indices is None:
        raise RuntimeError(
            "alloc_req_slots runs out of memory. "
            "Please set a smaller number for `--max-running-requests`. "
            f"{self.req_to_token_pool.available_size()=}, "
            f"{num_reqs=}, "
        )
    return req_pool_indices

def alloc_token_slots(self, num_tokens: int, backup_state: bool = False):
    """åˆ†é…tokenæ§½ä½çš„çœŸå®æ–¹æ³•"""
    # å¦‚æœéœ€è¦ï¼Œå…ˆæ¸…ç†æ ‘ç¼“å­˜
    self._evict_tree_cache_if_needed(num_tokens)
    
    # å¤‡ä»½çŠ¶æ€ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if backup_state:
        state = self.token_to_kv_pool_allocator.backup_state()
    
    # åˆ†é…KVç¼“å­˜ç©ºé—´
    out_cache_loc = self.token_to_kv_pool_allocator.alloc(num_tokens)
    if out_cache_loc is None:
        phase_str = "Prefill" if self.forward_mode.is_extend() else "Decode"
        error_msg = (
            f"{phase_str} out of memory. Try to lower your batch size.\n"
            f"Try to allocate {num_tokens} tokens.\n"
        )
        logger.error(error_msg)
        raise RuntimeError(error_msg)
    
    if backup_state:
        return out_cache_loc, state
    else:
        return out_cache_loc

# æ¥è‡ªPrefillAdderçš„çœŸå®è¯·æ±‚æ·»åŠ é€»è¾‘
def add_one_req(self, req: Req, has_chunked_req: bool):
    """PrefillAdderæ·»åŠ è¯·æ±‚çš„çœŸå®æ–¹æ³•ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    # åˆå§‹åŒ–è¯·æ±‚çš„ä¸‹ä¸€è½®è¾“å…¥
    req.init_next_round_input(self.tree_cache)
    
    # è®¡ç®—å‰ç¼€é•¿åº¦å’Œè¾“å…¥tokenæ•°
    prefix_len = len(req.prefix_indices)
    input_tokens = req.extend_input_len
    
    # æ£€æŸ¥æ˜¯å¦å¯ä»¥æ·»åŠ è¯·æ±‚
    if self.rem_chunk_tokens is None or input_tokens <= self.rem_chunk_tokens:
        # éåˆ†å—é¢„å¡«å……
        self.can_run_list.append(req)  # æ·»åŠ åˆ°å¯è¿è¡Œåˆ—è¡¨
        if self.is_hybrid:  # SWAæ··åˆç¼“å­˜
            swa_uuid_for_lock = self.tree_cache.inc_lock_ref(req.last_node)
            req.swa_uuid_for_lock = swa_uuid_for_lock
        else:
            self.tree_cache.inc_lock_ref(req.last_node)  # å¢åŠ é”å¼•ç”¨
        
        # æ›´æ–°é¢„å¡«å……é¢„ç®—
        self._update_prefill_budget(
            prefix_len, 
            input_tokens,
            min(req.sampling_params.max_new_tokens, CLIP_MAX_NEW_TOKENS)
        )
    else:
        # åˆ†å—é¢„å¡«å……å¤„ç†
        trunc_len = self.rem_chunk_tokens - self.page_size + 1
        req.extend_input_len = trunc_len
        req.fill_ids = req.fill_ids[:len(req.prefix_indices) + trunc_len]
        
        self.can_run_list.append(req)
        self.new_chunked_req = req  # è®¾ç½®ä¸ºæ–°çš„åˆ†å—è¯·æ±‚
    
    return self.budget_state()  # è¿”å›é¢„ç®—çŠ¶æ€
```

## 7. å¤šæ¨¡æ€æ•°æ®ç»“æ„

SGLangæ”¯æŒå¤šæ¨¡æ€è¾“å…¥ï¼Œé€šè¿‡ä¸“é—¨çš„æ•°æ®ç»“æ„æ¥ç»Ÿä¸€ç®¡ç†å›¾åƒã€è§†é¢‘ã€éŸ³é¢‘ç­‰ä¸åŒæ¨¡æ€çš„æ•°æ®ã€‚

### 7.1 MultimodalInputså®Œæ•´å®šä¹‰

```python
@dataclasses.dataclass
class MultimodalInputs:
    """å¤šæ¨¡æ€è¾“å…¥æ•°æ®ç»“æ„çš„å®Œæ•´å®ç°"""
    
    # æ ¸å¿ƒæ•°æ®é¡¹
    mm_items: List[MultimodalDataItem]                      # å¤šæ¨¡æ€æ•°æ®é¡¹åˆ—è¡¨
    image_pad_len: Optional[list] = None                    # å›¾åƒå¡«å……é•¿åº¦
    num_image_tokens: Optional[int] = None                  # å›¾åƒtokenæ•°é‡
    
    # å›¾åƒç›¸å…³token ID
    im_token_id: Optional[int] = None                       # å›¾åƒtoken ID
    im_start_id: Optional[int] = None                       # å›¾åƒå¼€å§‹token ID
    im_end_id: Optional[int] = None                         # å›¾åƒç»“æŸtoken ID
    slice_start_id: Optional[int] = None                    # åˆ‡ç‰‡å¼€å§‹token ID
    slice_end_id: Optional[int] = None                      # åˆ‡ç‰‡ç»“æŸtoken ID
    
    # è§†é¢‘ç›¸å…³token ID
    video_token_id: Optional[int] = None                    # è§†é¢‘token ID
    
    # éŸ³é¢‘ç›¸å…³token ID
    audio_token_id: Optional[int] = None                    # éŸ³é¢‘token ID
    audio_start_id: Optional[int] = None                    # éŸ³é¢‘å¼€å§‹token ID
    audio_end_id: Optional[int] = None                      # éŸ³é¢‘ç»“æŸtoken ID
    
    # QWen2-VLç›¸å…³ä½ç½®ç¼–ç 
    mrope_positions: Optional[torch.Tensor] = None          # å¤šç»´ä½ç½®ç¼–ç 
    mrope_position_delta: Optional[torch.Tensor] = None     # ä½ç½®ç¼–ç å¢é‡
    
    @staticmethod
    def from_dict(obj: dict):
        """ä»å­—å…¸åˆ›å»ºMultimodalInputs"""
        ret = MultimodalInputs(
            mm_items=obj["mm_items"],
        )
        
        assert isinstance(ret.mm_items, list)
        ret.mm_items = [item for item in ret.mm_items if item.is_valid()]
        for item in ret.mm_items:
            item.set_pad_value()
        
        optional_args = [
            "mrope_positions", "mrope_position_delta",
            "im_token_id", "im_start_id", "im_end_id",
            "video_token_id", "slice_start_id", "slice_end_id",
            "audio_start_id", "audio_end_id", "audio_token_id",
        ]
        for arg in optional_args:
            if arg in obj:
                setattr(ret, arg, obj[arg])
        
        return ret
    
    def contains_image_inputs(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾åƒè¾“å…¥"""
        return any(item.is_image() for item in self.mm_items)
    
    def contains_video_inputs(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«è§†é¢‘è¾“å…¥"""
        return any(item.is_video() for item in self.mm_items)
    
    def contains_audio_inputs(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«éŸ³é¢‘è¾“å…¥"""
        return any(item.is_audio() for item in self.mm_items)
    
    def contains_mm_input(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«å¤šæ¨¡æ€è¾“å…¥"""
        return any(True for item in self.mm_items if item.is_valid())
    
    def merge(self, other: MultimodalInputs):
        """åˆå¹¶å¤šæ¨¡æ€è¾“å…¥"""
        # éœ€è¦åˆå¹¶çš„å‚æ•°
        optional_args = ["mm_items", "image_pad_len"]
        for arg in optional_args:
            self_arg = getattr(self, arg, None)
            if self_arg is not None:
                setattr(self, arg, self_arg + getattr(other, arg))
        
        # å¤„ç†mrope_positions
        mrope_positions = self.mrope_positions
        if mrope_positions is not None:
            if other.mrope_positions is None:
                self.mrope_positions = mrope_positions
            else:
                self.mrope_positions = torch.cat(
                    [self.mrope_positions, other.mrope_positions], dim=1
                )
        
        # å¤„ç†mrope_position_delta
        mrope_position_delta = self.mrope_position_delta
        if mrope_position_delta is not None:
            if other.mrope_position_delta is None:
                self.mrope_position_delta = mrope_position_delta
            else:
                self.mrope_position_delta = torch.cat(
                    [self.mrope_position_delta, other.mrope_position_delta], dim=0
                )
        
        # è®¾ç½®token ID
        for key, val in other.__dict__.items():
            if "_id" in key:
                if getattr(self, key, None) is None:
                    setattr(self, key, getattr(other, key, None))
```

### 7.2 MultimodalDataItemå®Œæ•´å®šä¹‰

```python
@dataclasses.dataclass
class MultimodalDataItem:
    """å¤šæ¨¡æ€æ•°æ®é¡¹çš„å®Œæ•´å®ç°"""
    
    modality: Modality                                      # æ¨¡æ€ç±»å‹ï¼ˆå›¾åƒ/è§†é¢‘/éŸ³é¢‘ï¼‰
    hash: int = None                                        # æ•°æ®å“ˆå¸Œå€¼
    pad_value: int = None                                   # å¡«å……å€¼
    offsets: Optional[list] = None                          # åç§»é‡åˆ—è¡¨
    
    # åŸå§‹ç‰¹å¾æ•°æ®ï¼ˆäºŒé€‰ä¸€ï¼‰
    feature: Union[torch.Tensor, np.ndarray] = None         # åŸå§‹ç‰¹å¾ï¼ˆå¦‚pixel_valuesï¼‰
    precomputed_embeddings: Optional[Union[torch.Tensor, np.ndarray]] = None  # é¢„è®¡ç®—åµŒå…¥
    
    # æ¨¡å‹ç‰¹å®šæ•°æ®
    model_specific_data: dict[str, Any] = dataclasses.field(default_factory=dict)
    
    def __getattr__(self, name: str):
        """åŠ¨æ€è®¿é—®æ¨¡å‹ç‰¹å®šæ•°æ®"""
        if (
            "model_specific_data" in self.__dict__
            and name in self.__dict__["model_specific_data"]
        ):
            return self.__dict__["model_specific_data"][name]
        else:
            raise AttributeError(
                f"'{self.__class__.__name__}' object has no attribute '{name}'"
            )
    
    def __setitem__(self, key: str, value: Any):
        """è®¾ç½®æ¨¡å‹ç‰¹å®šæ•°æ®"""
        if key in self.__dict__:
            self.__dict__[key] = value
        else:
            self.model_specific_data[key] = value
    
    def set(self, key: str, value: Any):
        """è®¾ç½®æ¨¡å‹ç‰¹å®šæ•°æ®"""
        self.__setitem__(key, value)
    
    @staticmethod
    def is_empty_list(l):
        """æ£€æŸ¥åˆ—è¡¨æ˜¯å¦ä¸ºç©º"""
        if l is None:
            return True
        return len([item for item in flatten_nested_list(l) if item is not None]) == 0
    
    def set_pad_value(self):
        """è®¾ç½®å¡«å……å€¼"""
        from sglang.srt.managers.mm_utils import hash_feature
        
        if self.hash is None:
            if self.feature is not None:
                hashed_feature = self.feature
            else:
                hashed_feature = self.precomputed_embeddings
            self.hash = hash_feature(hashed_feature)
        assert self.hash is not None
        self.pad_value = self.hash % (1 << 30)
    
    def is_modality(self, modality: Modality) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæŒ‡å®šæ¨¡æ€"""
        return self.modality == modality
    
    def is_audio(self):
        """æ£€æŸ¥æ˜¯å¦ä¸ºéŸ³é¢‘æ¨¡æ€"""
        return self.modality == Modality.AUDIO
    
    def is_image(self):
        """æ£€æŸ¥æ˜¯å¦ä¸ºå›¾åƒæ¨¡æ€"""
        return self.modality in [Modality.IMAGE, Modality.MULTI_IMAGES]
    
    def is_video(self):
        """æ£€æŸ¥æ˜¯å¦ä¸ºè§†é¢‘æ¨¡æ€"""
        return self.modality == Modality.VIDEO
    
    def is_valid(self) -> bool:
        """æ£€æŸ¥æ•°æ®é¡¹æ˜¯å¦æœ‰æ•ˆ"""
        return self.is_image() or self.is_video() or self.is_audio()
    
    def validate(self):
        """éªŒè¯æ•°æ®é¡¹"""
        # TODO: å®ç°éªŒè¯é€»è¾‘
        pass
    
    @staticmethod
    def from_dict(obj: dict):
        """ä»å­—å…¸åˆ›å»ºMultimodalDataItem"""
        kwargs = dict(obj)
        modality = kwargs.pop("modality")
        if isinstance(modality, str):
            modality = Modality[modality]
        ret = MultimodalDataItem(modality=modality, **kwargs)
        ret.validate()
        return ret
    
    def merge(self, other):
        """åˆå¹¶å¤šæ¨¡æ€æ•°æ®é¡¹"""
        self.feature += other.feature
        self.offsets += other.offsets
        self.hash = hash((self.hash, other.hash))
        self.set_pad_value()
```

## 8. æ‰¹é‡è¯·æ±‚æ•°æ®ç»“æ„

SGLangæ”¯æŒæ‰¹é‡è¯·æ±‚å¤„ç†ä»¥ä¼˜åŒ–ç½‘ç»œä¼ è¾“æ•ˆç‡ã€‚

### 8.1 BatchTokenizedGenerateReqInput

```python
@dataclass
class BatchTokenizedGenerateReqInput:
    """æ‰¹é‡ç”Ÿæˆè¯·æ±‚è¾“å…¥çš„å®Œæ•´å®ç°"""
    
    batch: List[TokenizedGenerateReqInput]                  # æ‰¹é‡å·²tokenizeçš„è¯·æ±‚åˆ—è¡¨
    
    def __len__(self):
        """è¿”å›æ‰¹æ¬¡å¤§å°"""
        return len(self.batch)
    
    def __getitem__(self, i):
        """æ”¯æŒç´¢å¼•è®¿é—®"""
        return self.batch[i]
    
    def __iter__(self):
        """æ”¯æŒè¿­ä»£è®¿é—®"""
        return iter(self.batch)
```

### 8.2 BatchTokenizedEmbeddingReqInput

```python
@dataclass
class BatchTokenizedEmbeddingReqInput:
    """æ‰¹é‡åµŒå…¥è¯·æ±‚è¾“å…¥çš„å®Œæ•´å®ç°"""
    
    batch: List[TokenizedEmbeddingReqInput]                 # æ‰¹é‡åµŒå…¥è¯·æ±‚åˆ—è¡¨
    
    def __len__(self):
        """è¿”å›æ‰¹æ¬¡å¤§å°"""
        return len(self.batch)
    
    def __getitem__(self, i):
        """æ”¯æŒç´¢å¼•è®¿é—®"""
        return self.batch[i]
    
    def __iter__(self):
        """æ”¯æŒè¿­ä»£è®¿é—®"""
        return iter(self.batch)
```

## 9. åˆ†ç¦»å¼æ¶æ„æ•°æ®ç»“æ„

SGLangæ”¯æŒé¢„å¡«å……/è§£ç åˆ†ç¦»éƒ¨ç½²æ¨¡å¼ï¼Œé€šè¿‡ä¸“é—¨çš„å­—æ®µç®¡ç†è·¨èŠ‚ç‚¹çš„KVç¼“å­˜ä¼ è¾“å’Œè¯·æ±‚è·¯ç”±ã€‚

### 9.1 åˆ†ç¦»å¼æ¶æ„æ ¸å¿ƒå­—æ®µ

Reqç±»åŒ…å«äº†ä¸“é—¨çš„åˆ†ç¦»å¼æ¶æ„å­—æ®µï¼š

```python
class Req:
    """Reqç±»ä¸­çš„åˆ†ç¦»å¼æ¶æ„ç›¸å…³å­—æ®µ"""
    
    # KVç¼“å­˜ä¼ è¾“ç»„ä»¶
    disagg_kv_sender: Optional[BaseKVSender] = None      # KVå‘é€å™¨ï¼Œç”¨äºå‘é€é¢„å¡«å……çš„KVç¼“å­˜
    disagg_kv_receiver: Optional[BaseKVReceiver] = None  # KVæ¥æ”¶å™¨ï¼Œç”¨äºæ¥æ”¶è¿œç¨‹KVç¼“å­˜
    
    # Bootstrapè¿æ¥é…ç½®
    bootstrap_host: Optional[str] = None                 # å¯åŠ¨ä¸»æœºåœ°å€ï¼Œç”¨äºèŠ‚ç‚¹å‘ç°
    bootstrap_port: Optional[int] = None                 # å¯åŠ¨ç«¯å£å·ï¼Œç”¨äºæœåŠ¡è¿æ¥
    bootstrap_room: Optional[str] = None                 # å¯åŠ¨æˆ¿é—´IDï¼Œç”¨äºè¯·æ±‚è·¯ç”±å’Œéš”ç¦»
    
    # æ•°æ®å¹¶è¡Œé…ç½®
    data_parallel_rank: Optional[int] = None             # æ•°æ®å¹¶è¡Œrankï¼Œç”¨äºè´Ÿè½½å‡è¡¡
    
    # SWAæ··åˆç¼“å­˜ç›¸å…³å­—æ®µ
    swa_uuid_for_lock: Optional[str] = None             # SWAé”å®šUUIDï¼Œé˜²æ­¢å¹¶å‘è®¿é—®å†²çª
    
    # KVç¼“å­˜å‘é€ç®¡ç†
    start_send_idx: int = 0                             # KVç¼“å­˜å‘é€èµ·å§‹ç´¢å¼•
    tmp_end_idx: int = -1                               # ä¸´æ—¶ç»“æŸç´¢å¼•
    metadata_buffer_index: int = -1                     # å…ƒæ•°æ®ç¼“å†²åŒºç´¢å¼•
```

## 10. æ•°æ®ç»“æ„åä½œå…³ç³»è¯¦è§£

### 10.1 åä½œå…³ç³»å¯è§†åŒ–

```mermaid
graph TD
    subgraph "ğŸ”„ æ•°æ®ç»„ç»‡å±‚æ¬¡"
        A1["Req<br/>åŸå­å•ä½"]
        A2["List[Req]<br/>è¯·æ±‚é›†åˆ"]
        A3["ScheduleBatch<br/>æ‰¹æ¬¡æŠ½è±¡"]
    end

    subgraph "ğŸ’¾ èµ„æºç®¡ç†å±‚"
        B1["ReqToTokenPool<br/>è¯·æ±‚æ˜ å°„"]
        B2["BaseTokenToKVPoolAllocator<br/>KVåˆ†é…å™¨"]
        B3["BasePrefixCache<br/>å‰ç¼€ç¼“å­˜"]
    end

    subgraph "ğŸ”„ æ•°æ®è½¬æ¢å±‚"
        C1["ModelWorkerBatch<br/>æ¨¡å‹å±‚"]
        C2["ForwardBatch<br/>æ‰§è¡Œå±‚"]
        C3["GPU Tensors<br/>è®¡ç®—è½½ä½“"]
    end

    subgraph "ğŸŒ æ‰©å±•åŠŸèƒ½å±‚"
        D1["MultimodalInputs<br/>å¤šæ¨¡æ€"]
        D2["BatchRequests<br/>æ‰¹é‡ä¼˜åŒ–"]
        D3["Disaggregation<br/>åˆ†ç¦»å¼"]
    end

    A1 --> A2
    A2 --> A3
    A3 --> B1
    A3 --> B2
    A3 --> B3
    A3 --> C1
    C1 --> C2
    C2 --> C3
    A1 --> D1
    A2 --> D2
    A3 --> D3

    style A1 fill:#e3f2fd,color:#000000,stroke:#333
    style A2 fill:#e3f2fd,color:#000000,stroke:#333
    style A3 fill:#e3f2fd,color:#000000,stroke:#333
    style B1 fill:#f1f8e9,color:#000000,stroke:#333
    style B2 fill:#f1f8e9,color:#000000,stroke:#333
    style B3 fill:#f1f8e9,color:#000000,stroke:#333
    style C1 fill:#fff3e0,color:#000000,stroke:#333
    style C2 fill:#fff3e0,color:#000000,stroke:#333
    style C3 fill:#fff3e0,color:#000000,stroke:#333
    style D1 fill:#ffebee,color:#000000,stroke:#333
    style D2 fill:#ffebee,color:#000000,stroke:#333
    style D3 fill:#ffebee,color:#000000,stroke:#333
```

**å›¾ç¤ºè¯´æ˜**ï¼šè“è‰²è¡¨ç¤ºæ•°æ®ç»„ç»‡å±‚æ¬¡ï¼Œç»¿è‰²è¡¨ç¤ºèµ„æºç®¡ç†ï¼Œæ©™è‰²è¡¨ç¤ºæ•°æ®è½¬æ¢ï¼Œçº¢è‰²è¡¨ç¤ºæ‰©å±•åŠŸèƒ½ã€‚å±•ç¤ºäº†SGLangæ•°æ®ç»“æ„çš„å®Œæ•´åä½œç½‘ç»œã€‚

### 10.2 æ ¸å¿ƒåä½œæœºåˆ¶

**æ•°æ®æµè½¬çš„åä½œè¿‡ç¨‹**ï¼šå„æ•°æ®ç»“æ„é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ¥å£è¿›è¡Œåä½œï¼Œç¡®ä¿é«˜æ•ˆçš„æ•°æ®æµè½¬å’Œèµ„æºç®¡ç†ã€‚

**æ ¸å¿ƒåä½œå…³ç³»**ï¼š
- **Reqå¯¹è±¡**ï¼šä½œä¸ºæœ€åŸºæœ¬çš„æ•°æ®å•å…ƒï¼Œæ‰¿è½½è¯·æ±‚çš„å®Œæ•´ä¿¡æ¯
- **ScheduleBatch**ï¼šå°†å¤šä¸ªReqç»„ç»‡æˆæ‰¹æ¬¡ï¼Œç®¡ç†èµ„æºæ± å¼•ç”¨å’Œè°ƒåº¦çŠ¶æ€
- **å†…å­˜ç®¡ç†ç»„ä»¶**ï¼šReqToTokenPoolã€BaseTokenToKVPoolAllocatorã€BasePrefixCacheååŒå·¥ä½œï¼Œç¡®ä¿å†…å­˜çš„é«˜æ•ˆåˆ†é…å’Œå›æ”¶
- **æ•°æ®è½¬æ¢**ï¼šScheduleBatchâ†’ModelWorkerBatchâ†’ForwardBatchçš„é€å±‚ç®€åŒ–ï¼Œæ¯å±‚ä¸“æ³¨äºç‰¹å®šçš„æŠ½è±¡çº§åˆ«

---

## 11. æ ¸å¿ƒè®¾è®¡åŸåˆ™

SGLangçš„æ•°æ®ç»“æ„è®¾è®¡ä½“ç°äº†å‡ ä¸ªé‡è¦çš„è®¾è®¡åŸåˆ™ï¼š

**åˆ†å±‚æŠ½è±¡**: é€šè¿‡Reqâ†’ScheduleBatchâ†’ModelWorkerBatchçš„åˆ†å±‚è®¾è®¡ï¼Œç³»ç»Ÿèƒ½å¤Ÿåœ¨ä¸åŒæŠ½è±¡å±‚é¢è¿›è¡Œä¼˜åŒ–ï¼Œè°ƒåº¦å™¨å…³æ³¨é«˜å±‚å†³ç­–ï¼Œæ¨¡å‹æ‰§è¡Œå™¨å…³æ³¨åº•å±‚è®¡ç®—ã€‚

**æ¨¡å—åŒ–è®¾è®¡**: å„ä¸ªæ•°æ®ç»“æ„èŒè´£æ¸…æ™°ï¼Œç›¸äº’ä¹‹é—´é€šè¿‡æ˜ç¡®çš„æ¥å£è¿›è¡Œäº¤äº’ï¼Œæé«˜äº†ç³»ç»Ÿçš„å¯ç»´æŠ¤æ€§å’Œå¯æµ‹è¯•æ€§ã€‚

**æ€§èƒ½ä¼˜åŒ–**: æ•°æ®ç»“æ„å……åˆ†è€ƒè™‘äº†æ€§èƒ½å› ç´ ï¼š
- æ‰¹é‡åŒ–å¤„ç†å‡å°‘å‡½æ•°è°ƒç”¨å¼€é”€
- å¼ é‡åŒ–æ•°æ®æ”¯æŒGPUå¹¶è¡Œè®¡ç®—
- å†…å­˜æ± è®¾è®¡æé«˜å†…å­˜å±€éƒ¨æ€§
- ç¼“å­˜å‹å¥½çš„æ•°æ®å¸ƒå±€

**æ‰©å±•æ€§**: é€šè¿‡æŠ½è±¡åŸºç±»å’Œmixinæ¨¡å¼ï¼Œæ•°æ®ç»“æ„å…·å¤‡è‰¯å¥½çš„æ‰©å±•æ€§ï¼Œèƒ½å¤Ÿæ”¯æŒæ–°åŠŸèƒ½çš„æ·»åŠ å’Œç°æœ‰åŠŸèƒ½çš„ä¼˜åŒ–ã€‚

### 11.1 å®ç°ç‰¹è‰²

**æºç å‡†ç¡®æ€§**ï¼šæœ¬æ–‡æ¡£åŸºäºçœŸå®SGLangæºç ç¼–å†™ï¼Œæ‰€æœ‰æ•°æ®ç»“æ„å®šä¹‰éƒ½æ¥è‡ªå®é™…å®ç°ï¼Œç¡®ä¿æŠ€æœ¯å‡†ç¡®æ€§ã€‚

**æ¶æ„æ¸…æ™°æ€§**ï¼šé‡‡ç”¨"æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ + å®Œæ•´å­—æ®µå®šä¹‰ + å…³é”®æ–¹æ³•å®ç°"çš„ç»“æ„ï¼Œæ—¢ä¾¿äºç†è§£è®¾è®¡æ€æƒ³ï¼Œåˆæä¾›å…·ä½“å®ç°å‚è€ƒã€‚

**å®Œæ•´æ€§ä¿è¯**ï¼šå±•ç¤ºäº†SGLangæ•°æ®ç»“æ„çš„å®Œæ•´å­—æ®µå’Œæ–¹æ³•ï¼Œè®©å¼€å‘è€…äº†è§£å®é™…ç³»ç»Ÿçš„å¤æ‚æ€§å’ŒåŠŸèƒ½ä¸°å¯Œæ€§ã€‚

### 11.2 æ¼”è¿›è¶‹åŠ¿

SGLangçš„æ•°æ®ç»“æ„å±•ç°äº†ç°ä»£æ¨ç†ç³»ç»Ÿçš„æ¼”è¿›æ–¹å‘ï¼š
- **å¤šæ¨¡æ€æ”¯æŒ**: ä»çº¯æ–‡æœ¬æ‰©å±•åˆ°å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘
- **åˆ†ç¦»å¼æ¶æ„**: æ”¯æŒé¢„å¡«å……/è§£ç åˆ†ç¦»çš„å¤§è§„æ¨¡éƒ¨ç½²
- **é«˜çº§ä¼˜åŒ–**: æŠ•æœºè§£ç ã€æ··åˆç¼“å­˜ã€DPæ³¨æ„åŠ›ç­‰å‰æ²¿æŠ€æœ¯
- **äº§ä¸šåŒ–éœ€æ±‚**: ä¼šè¯ç®¡ç†ã€LoRAé€‚é…å™¨ã€ç›‘æ§è°ƒè¯•ç­‰å·¥ç¨‹ç‰¹æ€§

ç†è§£è¿™äº›æ ¸å¿ƒæ•°æ®ç»“æ„åŠå…¶ç›¸äº’å…³ç³»ï¼Œæ˜¯æ·±å…¥æŒæ¡SGLangè°ƒåº¦å™¨å·¥ä½œæœºåˆ¶çš„å…³é”®ã€‚è¿™äº›æ•°æ®ç»“æ„ä¸ä»…æ‰¿è½½ç€ç³»ç»Ÿçš„æ ¸å¿ƒä¿¡æ¯ï¼Œè¿˜ä½“ç°äº†SGLangåœ¨æ€§èƒ½ã€å¯ç»´æŠ¤æ€§å’Œæ‰©å±•æ€§æ–¹é¢çš„è®¾è®¡è€ƒé‡ã€‚

**æ‰¿ä¸Šå¯ä¸‹**ï¼šåœ¨ç¬¬ä¸€ç« æˆ‘ä»¬äº†è§£äº†è°ƒåº¦å™¨çš„æ•´ä½“æ¶æ„å’Œè®¾è®¡ç†å¿µï¼Œæœ¬ç« æ·±å…¥å‰–æäº†æ”¯æ’‘è¿™äº›æ¶æ„çš„æ ¸å¿ƒæ•°æ®æŠ½è±¡ã€‚æœ‰äº†è¿™äº›åŸºç¡€ï¼Œæˆ‘ä»¬å°±ä¸ºæ·±å…¥æ¢è®¨è°ƒåº¦å™¨åœ¨å®é™…è¿è¡Œä¸­çš„è¯·æ±‚å¤„ç†æœºåˆ¶ã€æ‰¹æ¬¡è°ƒåº¦ç­–ç•¥å’Œå†…å­˜ç®¡ç†ç®—æ³•å¥ å®šäº†åšå®åŸºç¡€ã€‚æ¥ä¸‹æ¥çš„ç« èŠ‚å°†å±•ç¤ºè¿™äº›æ•°æ®ç»“æ„æ˜¯å¦‚ä½•åœ¨å…·ä½“çš„è°ƒåº¦æµç¨‹ä¸­å‘æŒ¥ä½œç”¨çš„ã€‚
