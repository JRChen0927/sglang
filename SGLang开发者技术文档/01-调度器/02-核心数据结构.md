# æ ¸å¿ƒæ•°æ®ç»“æ„

---

SGLangè°ƒåº¦å™¨çš„é«˜æ•ˆè¿è¡Œä¾èµ–äºä¸€ç³»åˆ—ç²¾å¿ƒè®¾è®¡çš„æ•°æ®ç»“æ„ã€‚è¿™äº›æ•°æ®ç»“æ„ä¸ä»…æ‰¿è½½ç€è¯·æ±‚çš„å„ç§ä¿¡æ¯ï¼Œè¿˜è´Ÿè´£æ‰¹æ¬¡ç®¡ç†ã€å†…å­˜åˆ†é…å’Œæ¨¡å‹æ¨ç†çš„åè°ƒã€‚ç†è§£è¿™äº›æ ¸å¿ƒæ•°æ®ç»“æ„æ˜¯æ·±å…¥æŒæ¡SGLangè°ƒåº¦å™¨å·¥ä½œåŸç†çš„åŸºç¡€ã€‚

---

## 1. æ•°æ®æµè½¬æ¶æ„

SGLangé‡‡ç”¨åˆ†å±‚çš„æ•°æ®å¤„ç†æ¶æ„ï¼Œè¯·æ±‚ä»æ¥æ”¶åˆ°æ‰§è¡Œç»å†äº†å››ä¸ªä¸»è¦çš„æ•°æ®ç»“æ„å±‚æ¬¡ï¼Œæ¯ä¸€å±‚éƒ½æœ‰æ˜ç¡®çš„èŒè´£åˆ†å·¥ï¼š

**è°ƒåº¦å™¨å±‚é¢çš„ScheduleBatch**è´Ÿè´£å­˜å‚¨è°ƒåº¦å™¨éœ€è¦çš„æ‰€æœ‰ä¿¡æ¯ï¼ŒåŒ…æ‹¬è¯·æ±‚åˆ—è¡¨ã€å†…å­˜æ± å¼•ç”¨ã€ç¼“å­˜ç®¡ç†ç­‰é«˜å±‚è°ƒåº¦å†³ç­–æ‰€éœ€çš„æ•°æ®ã€‚

**æ¨¡å‹å·¥ä½œå™¨å±‚é¢çš„ModelWorkerBatch**æ˜¯ScheduleBatchçš„ç®€åŒ–ç‰ˆæœ¬ï¼ŒåªåŒ…å«æ¨¡å‹å‰å‘æ¨ç†æ‰€éœ€çš„æ ¸å¿ƒæ•°æ®ï¼Œå»é™¤äº†è°ƒåº¦å™¨ç‰¹æœ‰çš„ç®¡ç†ä¿¡æ¯ã€‚

**æ¨¡å‹æ‰§è¡Œå™¨å±‚é¢çš„ForwardBatch**åŒ…å«æœ€åº•å±‚çš„GPUå¼ é‡æ•°æ®ï¼Œæ˜¯å®é™…åœ¨GPUä¸Šæ‰§è¡Œè®¡ç®—æ—¶ä½¿ç”¨çš„æ•°æ®æ ¼å¼ã€‚

### 1.1 æ•°æ®æµè½¬å¯è§†åŒ–

```mermaid
graph TD
    subgraph "ğŸ”„ æ•°æ®æµè½¬æ¶æ„"
        direction LR
        Req[Req<br/>è¯·æ±‚å¯¹è±¡] -->|ç»„ç»‡æˆæ‰¹æ¬¡| ScheduleBatch[ScheduleBatch<br/>è°ƒåº¦å™¨æ‰¹æ¬¡]
        ScheduleBatch -->|ç®€åŒ–ä¼ é€’| ModelWorkerBatch[ModelWorkerBatch<br/>æ¨¡å‹å·¥ä½œå™¨æ‰¹æ¬¡]
        ModelWorkerBatch -->|å¼ é‡è½¬æ¢| ForwardBatch[ForwardBatch<br/>GPUå‰å‘æ‰¹æ¬¡]
    end

    subgraph "ğŸ“Š æŠ½è±¡å±‚é¢"
        direction TB
        A["ğŸ“‹ è°ƒåº¦å™¨å±‚é¢"]
        B["âš™ï¸ æ¨¡å‹å·¥ä½œå™¨å±‚é¢"] 
        C["ğŸ”¥ GPUæ‰§è¡Œå±‚é¢"]
        A --> B --> C
    end
    
    A --> ScheduleBatch
    B --> ModelWorkerBatch
    C --> ForwardBatch

    subgraph "ğŸ” æ ¸å¿ƒå­—æ®µ"
        direction LR
        ScheduleBatch --> SB_Fields{"reqs: List[Req] | req_to_token_pool | tree_cache"}
        ModelWorkerBatch --> MWB_Fields{"input_ids: Tensor | seq_lens: Tensor | sampling_info"}
        ForwardBatch --> FB_Fields{"positions: Tensor | attn_backend | token_to_kv_pool"}
    end

    style A fill:#e3f2fd,color:#000000,stroke:#333
    style B fill:#f1f8e9,color:#000000,stroke:#333
    style C fill:#fff8e1,color:#000000,stroke:#333
    style Req fill:#f3e5f5,color:#000000,stroke:#333
    style ScheduleBatch fill:#e8f5e8,color:#000000,stroke:#333
    style ModelWorkerBatch fill:#fff3e0,color:#000000,stroke:#333
    style ForwardBatch fill:#ffebee,color:#000000,stroke:#333
    style SB_Fields fill:#f0f4c3,color:#000000,stroke:#333
    style MWB_Fields fill:#e8eaf6,color:#000000,stroke:#333
    style FB_Fields fill:#fce4ec,color:#000000,stroke:#333
```

**å›¾ç¤ºè¯´æ˜**ï¼šä½¿ç”¨å­å›¾ç»“æ„æ¸…æ™°å±•ç¤ºä¸‰ä¸ªç»´åº¦ï¼šæ•°æ®æµè½¬æ¶æ„ï¼ˆä¸»è¦æ•°æ®ç±»ï¼‰ã€æŠ½è±¡å±‚é¢ï¼ˆç³»ç»Ÿå±‚æ¬¡ï¼‰ã€æ ¸å¿ƒå­—æ®µï¼ˆå…³é”®å±æ€§ï¼‰ã€‚çŸ©å½¢èŠ‚ç‚¹è¡¨ç¤ºæ•°æ®ç±»ï¼Œè±å½¢èŠ‚ç‚¹è¡¨ç¤ºå­—æ®µé›†åˆï¼Œå®ç°äº†æ›´å¥½çš„è§†è§‰å±‚æ¬¡æ„Ÿã€‚

è¿™ç§åˆ†å±‚è®¾è®¡ç¡®ä¿äº†æ¯ä¸ªç»„ä»¶åªå¤„ç†ä¸å…¶èŒè´£ç›¸å…³çš„æ•°æ®ï¼Œæé«˜äº†ç³»ç»Ÿçš„æ¨¡å—åŒ–ç¨‹åº¦å’Œæ‰§è¡Œæ•ˆç‡ã€‚

### 1.2 è¯·æ±‚ç”Ÿå‘½å‘¨æœŸå¯è§†åŒ–

```mermaid
graph TD
    subgraph "ğŸ¯ è¯·æ±‚åˆ›å»ºé˜¶æ®µ"
        A1["ç”¨æˆ·è¯·æ±‚<br/>GenerateReqInput"]
        A2["TokenåŒ–å¤„ç†<br/>TokenizedGenerateReqInput"]
        A3["åˆ›å»ºReqå¯¹è±¡<br/>handle_generate_request()"]
    end

    subgraph "ğŸ“‹ é˜Ÿåˆ—ç®¡ç†é˜¶æ®µ"
        B1["ç­‰å¾…é˜Ÿåˆ—<br/>waiting_queue"]
        B2["è¯­æ³•é˜Ÿåˆ—<br/>grammar_queue"]
        B3["ä¼šè¯ç®¡ç†<br/>sessions"]
    end

    subgraph "ğŸ“¦ æ‰¹æ¬¡ç»„ç»‡é˜¶æ®µ"
        C1["PrefillAdder<br/>æ™ºèƒ½é€‰æ‹©"]
        C2["ScheduleBatch<br/>æ‰¹æ¬¡åˆ›å»º"]
        C3["å†…å­˜åˆ†é…<br/>alloc_req_slots()"]
    end

    subgraph "âš¡ æ¨¡å‹æ‰§è¡Œé˜¶æ®µ"
        D1["ModelWorkerBatch<br/>ç®€åŒ–ä¼ é€’"]
        D2["ForwardBatch<br/>GPUå¼ é‡"]
        D3["æ¨¡å‹å‰å‘<br/>forward()"]
    end

    subgraph "ğŸ“¤ ç»“æœå¤„ç†é˜¶æ®µ"
        E1["è¾“å‡ºç”Ÿæˆ<br/>logits processing"]
        E2["æµå¼è¾“å‡º<br/>stream_output()"]
        E3["è¯·æ±‚å®Œæˆ<br/>finished_reason"]
    end

    A1 --> A2 --> A3
    A3 --> B1
    A3 --> B2
    A3 --> B3
    B1 --> C1
    B2 --> C1
    C1 --> C2
    C2 --> C3
    C3 --> D1
    D1 --> D2
    D2 --> D3
    D3 --> E1
    E1 --> E2
    E2 --> E3

    style A1 fill:#e3f2fd,color:#000000,stroke:#333
    style A2 fill:#e3f2fd,color:#000000,stroke:#333
    style A3 fill:#e3f2fd,color:#000000,stroke:#333
    style B1 fill:#f1f8e9,color:#000000,stroke:#333
    style B2 fill:#f1f8e9,color:#000000,stroke:#333
    style B3 fill:#f1f8e9,color:#000000,stroke:#333
    style C1 fill:#fff3e0,color:#000000,stroke:#333
    style C2 fill:#fff3e0,color:#000000,stroke:#333
    style C3 fill:#fff3e0,color:#000000,stroke:#333
    style D1 fill:#ffebee,color:#000000,stroke:#333
    style D2 fill:#ffebee,color:#000000,stroke:#333
    style D3 fill:#ffebee,color:#000000,stroke:#333
    style E1 fill:#f3e5f5,color:#000000,stroke:#333
    style E2 fill:#f3e5f5,color:#000000,stroke:#333
    style E3 fill:#f3e5f5,color:#000000,stroke:#333
```

**å›¾ç¤ºè¯´æ˜**ï¼šè“è‰²è¡¨ç¤ºè¯·æ±‚åˆ›å»ºï¼Œç»¿è‰²è¡¨ç¤ºé˜Ÿåˆ—ç®¡ç†ï¼Œæ©™è‰²è¡¨ç¤ºæ‰¹æ¬¡ç»„ç»‡ï¼Œçº¢è‰²è¡¨ç¤ºæ¨¡å‹æ‰§è¡Œï¼Œç´«è‰²è¡¨ç¤ºç»“æœå¤„ç†ã€‚å±•ç¤ºäº†ä¸€ä¸ªReqä»åˆ›å»ºåˆ°å®Œæˆçš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸã€‚

---

## 2. Reqæ•°æ®ç»“æ„

**æ¶æ„å®šä½**ï¼šReqæ˜¯æ•´ä¸ªSGLangç³»ç»Ÿçš„åŸå­å•ä½ï¼Œæ˜¯æ‰€æœ‰ä¿¡æ¯ï¼ˆç”¨æˆ·è¾“å…¥ã€æ¨¡å‹å‚æ•°ã€å¤„ç†çŠ¶æ€ï¼‰çš„èµ·ç‚¹ã€‚åœ¨æ•°æ®æµè½¬æ¶æ„ä¸­ï¼ŒReqæ‰¿è½½ç€ä»ç”¨æˆ·è¯·æ±‚åˆ°æœ€ç»ˆè¾“å‡ºçš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸä¿¡æ¯ï¼Œæ˜¯åç»­ScheduleBatchã€ModelWorkerBatchã€ForwardBatchç­‰æ‰€æœ‰æ‰¹æ¬¡æ•°æ®ç»“æ„çš„åŸºç¡€æ„å»ºå—ã€‚

Reqç±»æ˜¯SGLangä¸­è¡¨ç¤ºå•ä¸ªè¯·æ±‚çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼ŒåŒ…å«äº†è¯·æ±‚ä»åˆ›å»ºåˆ°å®Œæˆçš„å…¨éƒ¨ä¿¡æ¯ã€‚

### 2.1 æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ

**Reqç±»çš„è®¾è®¡ç†å¿µ**ï¼šReqç±»æ˜¯SGLangä¸­è¡¨ç¤ºå•ä¸ªè¯·æ±‚çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼ŒåŒ…å«äº†ä»è¾“å…¥åˆ°è¾“å‡ºçš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸä¿¡æ¯ã€‚è®¾è®¡ä¸Šé‡‡ç”¨äº†ä¸°å¯Œçš„å‚æ•°æ”¯æŒï¼Œèƒ½å¤Ÿå¤„ç†æ–‡æœ¬ç”Ÿæˆã€åµŒå…¥è®¡ç®—ã€å¤šæ¨¡æ€è¾“å…¥ç­‰å¤šç§åœºæ™¯ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹ä¸ºReqç±»çš„æ ¸å¿ƒå±æ€§ç®€åŒ–ç‰ˆæœ¬ï¼Œçªå‡ºä¸»è¦æ¦‚å¿µã€‚çœŸå®å®ç°åŒ…å«40+ä¸ªå±æ€§ï¼Œæ”¯æŒæ›´å¤šé«˜çº§åŠŸèƒ½ã€‚

```python
class Req:
    """è¯·æ±‚å¯¹è±¡ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    def __init__(self, rid: str, origin_input_text: str, origin_input_ids: List[int],
                 sampling_params: SamplingParams, return_logprob: bool = False,
                 stream: bool = False, lora_id: Optional[str] = None):
        # åŸºæœ¬è¯·æ±‚ä¿¡æ¯
        self.rid = rid                          # è¯·æ±‚å”¯ä¸€æ ‡è¯†ç¬¦
        self.origin_input_text = origin_input_text    # åŸå§‹è¾“å…¥æ–‡æœ¬
        self.origin_input_ids = origin_input_ids      # åŸå§‹è¾“å…¥tokenåºåˆ—
        self.output_ids = []                         # è¾“å‡ºtokenåºåˆ—
        
        # å¤„ç†é…ç½®
        self.sampling_params = sampling_params        # é‡‡æ ·å‚æ•°é…ç½®
        self.return_logprob = return_logprob         # æ˜¯å¦è¿”å›å¯¹æ•°æ¦‚ç‡
        self.stream = stream                        # æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
        self.lora_id = lora_id                      # LoRAé€‚é…å™¨ID
        
        # çŠ¶æ€ç®¡ç†
        self.finished_reason = None                  # å®ŒæˆåŸå› 
        self.req_pool_idx = None                    # å†…å­˜æ± ç´¢å¼•
```

### 2.2 æºç å®ç°ç»†èŠ‚

**çœŸå®Reqç±»çš„å®Œæ•´å‚æ•°**ï¼šç”Ÿäº§ç¯å¢ƒä¸­çš„Reqç±»æ”¯æŒä¸°å¯Œçš„å‚æ•°é…ç½®ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€è¾“å…¥ã€LoRAé€‚é…å™¨ã€ä¼šè¯ç®¡ç†ã€åˆ†ç¦»å¼æ¶æ„ç­‰é«˜çº§åŠŸèƒ½ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹å±•ç¤ºçœŸå®Reqç±»çš„ä¸»è¦å‚æ•°ï¼Œçœç•¥äº†éƒ¨åˆ†å†…éƒ¨å®ç°ç»†èŠ‚ã€‚å®Œæ•´å®ç°è¯·å‚è€ƒ `sglang/srt/managers/schedule_batch.py`ã€‚

```python
class Req:
    """çœŸå®çš„SGLang Reqç±»å®ç°"""
    
    def __init__(
        self,
        rid: str,                              # è¯·æ±‚IDï¼ˆrequest idï¼‰
        origin_input_text: str,               # åŸå§‹è¾“å…¥æ–‡æœ¬
        origin_input_ids: List[int],          # åŸå§‹è¾“å…¥tokenåºåˆ—
        sampling_params: SamplingParams,      # é‡‡æ ·å‚æ•°é…ç½®
        return_logprob: bool = False,         # æ˜¯å¦è¿”å›å¯¹æ•°æ¦‚ç‡
        stream: bool = False,                 # æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
        lora_id: Optional[str] = None,        # LoRAé€‚é…å™¨ID
        session_id: Optional[str] = None,     # ä¼šè¯ID
        # ... è¿˜æœ‰20+ä¸ªå‚æ•°æ”¯æŒå¤šæ¨¡æ€ã€åˆ†ç¦»å¼æ¶æ„ç­‰é«˜çº§åŠŸèƒ½
    ):
        # åŸºç¡€è¯·æ±‚ä¿¡æ¯
        self.rid = rid                        # è¯·æ±‚å”¯ä¸€æ ‡è¯†ç¬¦
        self.origin_input_text = origin_input_text  # åŸå§‹è¾“å…¥æ–‡æœ¬
        self.origin_input_ids = origin_input_ids    # åŸå§‹tokenåºåˆ—
        self.output_ids = []                  # è¾“å‡ºtokenåºåˆ—ï¼ˆoutput token idsï¼‰
        self.fill_ids = []                    # å®Œæ•´tokenåºåˆ—ï¼ˆinput + outputï¼‰
        
        # å¤„ç†é…ç½®
        self.sampling_params = sampling_params      # é‡‡æ ·å‚æ•°
        self.stream = stream                        # æµå¼è¾“å‡ºæ ‡å¿—
        self.lora_id = lora_id                     # LoRAé€‚é…å™¨ID
        self.session_id = session_id               # ä¼šè¯ID
        
        # çŠ¶æ€ç®¡ç†
        self.finished_reason = None           # å®ŒæˆåŸå› ï¼ˆfinish reasonï¼‰
        self.req_pool_idx: Optional[int] = None  # è¯·æ±‚æ± ç´¢å¼•
        self.to_abort = False                 # æ˜¯å¦éœ€è¦ä¸­æ­¢
        
        # å¤šæ¨¡æ€æ”¯æŒ
        self.multimodal_inputs: Optional[MultimodalInputs] = None  # å¤šæ¨¡æ€è¾“å…¥
        self.input_embeds = input_embeds      # è¾“å…¥åµŒå…¥å‘é‡
        
        # å‰ç¼€ç¼“å­˜ä¼˜åŒ–
        self.prefix_indices: torch.Tensor = []      # å‰ç¼€ç¼“å­˜ç´¢å¼•
        self.extend_input_len = 0                   # éœ€è¦é¢„å¡«å……çš„tokenæ•°é‡
        
        # è¿˜æœ‰30+ä¸ªå­—æ®µæ”¯æŒå¢é‡è§£ç ã€åˆ†ç¦»å¼æ¶æ„ã€æ€§èƒ½ä¼˜åŒ–ç­‰åŠŸèƒ½...

ğŸ’¡ **å®ç°è¯´æ˜**: çœŸå®çš„Reqç±»æœ‰50+ä¸ªå­—æ®µï¼Œæ”¯æŒå¤šæ¨¡æ€è¾“å…¥ã€ä¼šè¯ç®¡ç†ã€LoRAé€‚é…å™¨ã€åˆ†ç¦»å¼æ¶æ„ã€å¢é‡è§£ç ã€å‰ç¼€ç¼“å­˜ç­‰é«˜çº§åŠŸèƒ½ã€‚æ•™å­¦ç‰ˆæœ¬çªå‡ºæ ¸å¿ƒçš„"è¾“å…¥â†’å¤„ç†â†’è¾“å‡º"æµç¨‹ã€‚
```

### 2.3 å…³é”®å­—æ®µåˆ†ç±»ä¸ä¸šåŠ¡å«ä¹‰

| å­—æ®µç±»åˆ« | å­—æ®µå | æ•°æ®ç±»å‹ | ä¸šåŠ¡å«ä¹‰ | GitHubæºç  |
|----------|--------|----------|----------|------------|
| **åŸºç¡€ä¿¡æ¯** | `rid` | `str` | è¯·æ±‚å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œç”¨äºå…¨é“¾è·¯è¿½è¸ª | [schedule_batch.py#L411](https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/managers/schedule_batch.py#L411) |
| | `origin_input_text` | `str` | åŸå§‹è¾“å…¥æ–‡æœ¬ï¼Œä¿ç•™ç”¨æˆ·åŸå§‹è¾“å…¥ | |
| | `origin_input_ids` | `List[int]` | TokenåŒ–åçš„è¾“å…¥åºåˆ— | |
| | `output_ids` | `List[int]` | æ¨¡å‹ç”Ÿæˆçš„è¾“å‡ºtokenåºåˆ— | |
| **å¤„ç†é…ç½®** | `sampling_params` | `SamplingParams` | é‡‡æ ·ç­–ç•¥é…ç½®ï¼ˆæ¸©åº¦ã€top-pç­‰ï¼‰ | |
| | `stream` | `bool` | æ˜¯å¦å¯ç”¨æµå¼è¾“å‡ºï¼Œå½±å“ç»“æœè¿”å›æ–¹å¼ | |
| | `return_logprob` | `bool` | æ˜¯å¦è¿”å›å¯¹æ•°æ¦‚ç‡ï¼Œç”¨äºç½®ä¿¡åº¦åˆ†æ | |
| | `lora_id` | `Optional[str]` | LoRAé€‚é…å™¨æ ‡è¯†ï¼Œæ”¯æŒå¤šæ¨¡å‹åˆ‡æ¢ | |
| **çŠ¶æ€ç®¡ç†** | `finished_reason` | `Optional[str]` | è¯·æ±‚å®ŒæˆåŸå› ï¼ˆé•¿åº¦é™åˆ¶/åœæ­¢è¯/é”™è¯¯ï¼‰ | |
| | `to_abort` | `bool` | æ˜¯å¦éœ€è¦ä¸­æ­¢ï¼Œç”¨äºè¯·æ±‚å–æ¶ˆ | |
| | `req_pool_idx` | `Optional[int]` | åœ¨å†…å­˜æ± ä¸­çš„ç´¢å¼•ä½ç½® | |
| **å¤šæ¨¡æ€** | `multimodal_inputs` | `Optional[MultimodalInputs]` | å›¾åƒ/è§†é¢‘/éŸ³é¢‘ç­‰éæ–‡æœ¬è¾“å…¥ | |
| | `input_embeds` | `Optional[Tensor]` | é¢„è®¡ç®—çš„è¾“å…¥åµŒå…¥å‘é‡ | |
| | `token_type_ids` | `Optional[Tensor]` | è·¨ç¼–ç å™¨æ¨¡å‹çš„tokenç±»å‹æ ‡è¯† | |
| **ä¼šè¯æ”¯æŒ** | `session_id` | `Optional[str]` | ä¼šè¯æ ‡è¯†ï¼Œæ”¯æŒè¿ç»­å¯¹è¯ | |
| | `bootstrap_host` | `Optional[str]` | åˆ†ç¦»å¼æ¨ç†çš„å¯åŠ¨ä¸»æœºåœ°å€ | |
| | `bootstrap_port` | `Optional[int]` | åˆ†ç¦»å¼æ¨ç†çš„å¯åŠ¨ç«¯å£å· | |
| | `bootstrap_room` | `Optional[str]` | åˆ†ç¦»å¼æ¨ç†çš„æˆ¿é—´IDï¼Œç”¨äºè¯·æ±‚è·¯ç”± | |
| **æ€§èƒ½ä¼˜åŒ–** | `prefix_indices` | `torch.Tensor` | å‰ç¼€ç¼“å­˜å‘½ä¸­çš„tokenç´¢å¼•ï¼Œå‡å°‘é‡å¤è®¡ç®— | |
| | `extend_input_len` | `int` | éœ€è¦é¢„å¡«å……çš„tokenæ•°é‡ï¼Œç”¨äºåˆ†å—å¤„ç† | |
| | `surr_offset` | `Optional[int]` | ç¯ç»•åç§»é‡ï¼Œç”¨äºæ»‘åŠ¨çª—å£æ³¨æ„åŠ›ä¼˜åŒ– | |
| **åˆ†ç¦»å¼æ¶æ„** | `disagg_kv_sender` | `Optional[BaseKVSender]` | KVç¼“å­˜å‘é€å™¨ï¼Œç”¨äºé¢„å¡«å……/è§£ç åˆ†ç¦» | |
| | `disagg_kv_receiver` | `Optional[BaseKVReceiver]` | KVç¼“å­˜æ¥æ”¶å™¨ï¼Œæ¥æ”¶è¿œç¨‹KVæ•°æ® | |
| | `swa_uuid_for_lock` | `Optional[str]` | SWAæ··åˆç¼“å­˜çš„é”å®šUUIDï¼Œé˜²æ­¢å¹¶å‘å†²çª | |

### 2.4 çŠ¶æ€ç®¡ç†

Reqç±»ç»´æŠ¤ç€è¯·æ±‚åœ¨å¤„ç†è¿‡ç¨‹ä¸­çš„å„ç§çŠ¶æ€ä¿¡æ¯ï¼š

**è¾“å‡ºç®¡ç†**  
output_idsåˆ—è¡¨è®°å½•äº†æ¨¡å‹ç”Ÿæˆçš„æ‰€æœ‰tokenï¼Œfill_idsæ˜¯origin_input_idså’Œoutput_idsçš„ç»„åˆï¼Œè¡¨ç¤ºå½“å‰çš„å®Œæ•´tokenåºåˆ—ã€‚

**ç”Ÿæˆæ§åˆ¶**  
finished_reasonè®°å½•è¯·æ±‚å®Œæˆçš„åŸå› ï¼Œå¯èƒ½æ˜¯è¾¾åˆ°æœ€å¤§é•¿åº¦ã€é‡åˆ°åœæ­¢tokenæˆ–å…¶ä»–æ¡ä»¶ã€‚å„ç§é•¿åº¦é™åˆ¶å’Œæ§åˆ¶å‚æ•°ç¡®ä¿ç”Ÿæˆè¿‡ç¨‹æŒ‰é¢„æœŸè¿›è¡Œã€‚

**å†…å­˜æ˜ å°„**  
req_pool_indiceså’Œå…¶ä»–ç´¢å¼•ä¿¡æ¯ç»´æŠ¤ç€è¯·æ±‚åœ¨å„ç§å†…å­˜æ± ä¸­çš„ä½ç½®ï¼Œè¿™å¯¹äºå†…å­˜ç®¡ç†å’Œç¼“å­˜æœºåˆ¶è‡³å…³é‡è¦ã€‚

### 2.5 å¤šæ¨¡æ€æ”¯æŒ

Reqç±»è¿˜æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ï¼ŒåŒ…æ‹¬å›¾åƒã€éŸ³é¢‘ç­‰éæ–‡æœ¬æ•°æ®ï¼š

```python
# æ¥è‡ªReqç±»çš„çœŸå®å¤šæ¨¡æ€å­—æ®µ
self.input_embeds = input_embeds                              # è¾“å…¥åµŒå…¥å‘é‡
self.multimodal_inputs: Optional[MultimodalInputs] = None    # å¤šæ¨¡æ€è¾“å…¥ç»Ÿä¸€æ¥å£
```

è¿™ç§è®¾è®¡ä½¿å¾—SGLangèƒ½å¤Ÿå¤„ç†ä¸ä»…ä»…æ˜¯æ–‡æœ¬çš„å¤šç§æ¨¡æ€è¾“å…¥ï¼Œä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æä¾›äº†åŸºç¡€æ”¯æŒã€‚

---

## 3. ScheduleBatchæ•°æ®ç»“æ„

**æ¶æ„å®šä½**ï¼šScheduleBatchæ˜¯æ•°æ®æµè½¬æ¶æ„ä¸­çš„è°ƒåº¦å™¨å±‚æŠ½è±¡ï¼Œè´Ÿè´£å°†å¤šä¸ªReqå¯¹è±¡ç»„ç»‡æˆæ‰¹æ¬¡å¹¶ç®¡ç†è°ƒåº¦ç›¸å…³çš„èµ„æºã€‚å®ƒæ˜¯è¿æ¥ä¸Šå±‚è°ƒåº¦å†³ç­–å’Œä¸‹å±‚æ¨¡å‹æ‰§è¡Œçš„å…³é”®æ¡¥æ¢ï¼ŒåŒ…å«äº†å†…å­˜æ± å¼•ç”¨ã€ç¼“å­˜ç®¡ç†ã€å¹¶è¡Œé…ç½®ç­‰è°ƒåº¦å™¨ç‰¹æœ‰çš„ç®¡ç†ä¿¡æ¯ã€‚

ScheduleBatchæ˜¯è°ƒåº¦å™¨å±‚é¢çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼Œè´Ÿè´£ç®¡ç†ä¸€ä¸ªæ‰¹æ¬¡ä¸­æ‰€æœ‰è¯·æ±‚çš„ä¿¡æ¯å’Œèµ„æºã€‚

### 3.1 æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ

```python
@dataclasses.dataclass
class ScheduleBatch:
    """æ‰¹æ¬¡æ•°æ®ç»“æ„çš„æ ¸å¿ƒæ¦‚å¿µ"""
    # è¯·æ±‚å’Œèµ„æºç®¡ç†
    reqs: List[Req]                          # æ‰¹æ¬¡ä¸­çš„è¯·æ±‚åˆ—è¡¨
    req_to_token_pool: ReqToTokenPool        # è¯·æ±‚åˆ°tokenæ± çš„æ˜ å°„
    token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator  # KVç¼“å­˜åˆ†é…å™¨
    tree_cache: BasePrefixCache              # å‰ç¼€ç¼“å­˜æ ‘
    
    # æ‰¹æ¬¡é…ç½®
    forward_mode: ForwardMode                # å‰å‘æ¨¡å¼ï¼ˆé¢„å¡«å……/è§£ç ï¼‰
    enable_overlap: bool = False             # æ˜¯å¦å¯ç”¨é‡å å¤„ç†
    batch_is_full: bool = False             # æ‰¹æ¬¡æ˜¯å¦å·²æ»¡
    
    # GPUå¼ é‡æ•°æ®
    input_ids: torch.Tensor = None          # è¾“å…¥token IDå¼ é‡
    seq_lens: torch.Tensor = None           # åºåˆ—é•¿åº¦å¼ é‡
    req_pool_indices: torch.Tensor = None   # è¯·æ±‚æ± ç´¢å¼•å¼ é‡
```

### 3.2 æºç å®ç°ç»†èŠ‚

**çœŸå®ScheduleBatchçš„å®Œæ•´ç»“æ„**ï¼šç”Ÿäº§ç¯å¢ƒä¸­çš„ScheduleBatchåŒ…å«äº†æ‰¹æ¬¡ç®¡ç†æ‰€éœ€çš„å…¨éƒ¨ä¿¡æ¯ï¼Œä»åŸºç¡€çš„è¯·æ±‚åˆ—è¡¨åˆ°å¤æ‚çš„GPUå¼ é‡æ•°æ®ï¼Œæ”¯æŒå¤šç§å‰å‘æ¨¡å¼å’Œä¼˜åŒ–ç­–ç•¥ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹å±•ç¤ºçœŸå®ScheduleBatchçš„ä¸»è¦å±æ€§ï¼Œçœç•¥äº†éƒ¨åˆ†å†…éƒ¨æ–¹æ³•å®ç°ã€‚å®Œæ•´å®ç°è¯·å‚è€ƒ `sglang/srt/managers/schedule_batch.py`ã€‚

```python
@dataclasses.dataclass
class ScheduleBatch(ScheduleBatchDisaggregationDecodeMixin):
    """çœŸå®çš„SGLang ScheduleBatchå®ç°"""
    
    # æ ¸å¿ƒç»„ç»‡
    reqs: List[Req]                                    # æ‰¹æ¬¡ä¸­çš„è¯·æ±‚åˆ—è¡¨
    req_to_token_pool: ReqToTokenPool = None          # è¯·æ±‚åˆ°tokenæ± æ˜ å°„
    token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator = None  # KVç¼“å­˜åˆ†é…å™¨
    tree_cache: BasePrefixCache = None                # å‰ç¼€ç¼“å­˜
    
    # æ‰¹æ¬¡æ§åˆ¶
    forward_mode: ForwardMode = None                  # å‰å‘æ¨¡å¼ï¼ˆé¢„å¡«å……/è§£ç ï¼‰
    batch_is_full: bool = False                       # æ‰¹æ¬¡æ»¡æ ‡å¿—
    enable_overlap: bool = False                      # æ˜¯å¦å¯ç”¨é‡å å¤„ç†
    launch_done: Optional[threading.Event] = None    # é‡å äº‹ä»¶å¾ªç¯åŒæ­¥
    
    # GPUå¼ é‡æ•°æ®
    input_ids: torch.Tensor = None                    # è¾“å…¥token IDå¼ é‡ [b]
    seq_lens: torch.Tensor = None                     # åºåˆ—é•¿åº¦å¼ é‡ [b]
    req_pool_indices: torch.Tensor = None             # è¯·æ±‚æ± ç´¢å¼•å¼ é‡ [b]
    out_cache_loc: torch.Tensor = None                # KVç¼“å­˜è¾“å‡ºä½ç½® [b]
    
    # é‡‡æ ·ä¿¡æ¯
    sampling_info: SamplingBatchInfo = None           # å½“å‰æ‰¹æ¬¡é‡‡æ ·ä¿¡æ¯
    next_batch_sampling_info: SamplingBatchInfo = None  # ä¸‹ä¸€æ‰¹æ¬¡é‡‡æ ·ä¿¡æ¯
    
    # å¤šæ¨¡æ€ä¸é«˜çº§åŠŸèƒ½
    multimodal_inputs: Optional[List] = None          # å¤šæ¨¡æ€è¾“å…¥æ•°æ®
    spec_algorithm: SpeculativeAlgorithm = None       # æŠ•æœºè§£ç ç®—æ³•
    has_stream: bool = False                          # æ˜¯å¦æœ‰æµå¼è¯·æ±‚
    has_grammar: bool = False                         # æ˜¯å¦æœ‰è¯­æ³•çº¦æŸ
    
    # ... è¿˜æœ‰40+ä¸ªå­—æ®µæ”¯æŒDPæ³¨æ„åŠ›ã€åˆ†å±‚ç¼“å­˜ã€åˆ†ç¦»å¼æ¶æ„ç­‰é«˜çº§åŠŸèƒ½

ğŸ’¡ **å®ç°è¯´æ˜**: çœŸå®çš„ScheduleBatchæœ‰60+ä¸ªå­—æ®µï¼Œæ”¯æŒå¤šæ¨¡æ€ã€DPæ³¨æ„åŠ›ã€æŠ•æœºè§£ç ã€åˆ†å±‚ç¼“å­˜ã€åˆ†ç¦»å¼æ¶æ„ç­‰å¤æ‚åŠŸèƒ½ã€‚ä¸Šè¿°ä»£ç çªå‡º"è¯·æ±‚â†’èµ„æºâ†’å¼ é‡"çš„æ ¸å¿ƒç»„ç»‡ç»“æ„ã€‚
```

### 3.3 å­—æ®µåŠŸèƒ½åˆ†ç±»ä¸ç»Ÿä¸€è¯´æ˜

| å­—æ®µç±»åˆ« | å­—æ®µå | ç»§æ‰¿å…³ç³» | ä¸šåŠ¡å«ä¹‰è¯´æ˜ |
|----------|--------|----------|--------------|
| **æ ¸å¿ƒç»„ç»‡** | `reqs` | æ–°å¢ | æ‰¹æ¬¡ä¸­çš„Reqå¯¹è±¡åˆ—è¡¨ï¼Œæ˜¯æ‰¹æ¬¡çš„æ ¸å¿ƒæ•°æ® |
| | `req_to_token_pool` | æ–°å¢ | è¯·æ±‚åˆ°tokenæ± çš„æ˜ å°„ç®¡ç†å™¨ |
| | `token_to_kv_pool_allocator` | æ–°å¢ | KVç¼“å­˜åˆ†é…å™¨ï¼Œç®¡ç†GPUå†…å­˜ |
| | `tree_cache` | æ–°å¢ | å‰ç¼€ç¼“å­˜æ ‘ï¼Œä¼˜åŒ–é‡å¤è®¡ç®— |
| **æ‰¹æ¬¡æ§åˆ¶** | `forward_mode` | ç»§æ‰¿åˆ°ä¸‹å±‚ | å‰å‘æ¨¡å¼ï¼Œä¸Reqä¸­å«ä¹‰ç›¸åŒï¼Œæ§åˆ¶æ‰¹æ¬¡æ‰§è¡Œæ¨¡å¼ |
| | `batch_is_full` | è°ƒåº¦å™¨ç‰¹æœ‰ | æ‰¹æ¬¡æ»¡æ ‡å¿—ï¼Œç”¨äºè°ƒåº¦å†³ç­–ï¼Œä¸ä¼ é€’åˆ°ä¸‹å±‚ |
| | `enable_overlap` | ç»§æ‰¿åˆ°ä¸‹å±‚ | é‡å æ‰§è¡Œæ ‡å¿—ï¼Œå½±å“GPU-CPUå¹¶è¡Œç­–ç•¥ |
| | `launch_done` | è°ƒåº¦å™¨ç‰¹æœ‰ | çº¿ç¨‹åŒæ­¥äº‹ä»¶ï¼Œç”¨äºé‡å è°ƒåº¦ï¼Œä¸ä¼ é€’åˆ°ä¸‹å±‚ |
| **GPUæ•°æ®** | `input_ids` | ç»§æ‰¿åˆ°ä¸‹å±‚ | ä¸Reqä¸­å«ä¹‰ç›¸åŒï¼Œä½†ç»„ç»‡ä¸ºæ‰¹é‡å¼ é‡æ ¼å¼ |
| | `seq_lens` | ç»§æ‰¿åˆ°ä¸‹å±‚ | åºåˆ—é•¿åº¦å¼ é‡ï¼Œç”¨äºæ³¨æ„åŠ›æ©ç è®¡ç®— |
| | `req_pool_indices` | ç»§æ‰¿åˆ°ä¸‹å±‚ | è¯·æ±‚æ± ç´¢å¼•å¼ é‡ï¼Œç”¨äºå†…å­˜æ˜ å°„ |
| | `out_cache_loc` | ç»§æ‰¿åˆ°ä¸‹å±‚ | KVç¼“å­˜è¾“å‡ºä½ç½®ï¼ŒæŒ‡å‘GPUå†…å­˜åœ°å€ |
| **é«˜çº§åŠŸèƒ½** | `spec_algorithm` | ç»§æ‰¿åˆ°ä¸‹å±‚ | æŠ•æœºè§£ç ç®—æ³•ç±»å‹ï¼ˆEagle/Medusaç­‰ï¼‰ |
| | `can_run_dp_cuda_graph` | è°ƒåº¦å™¨ç‰¹æœ‰ | æ˜¯å¦å¯è¿è¡ŒDP CUDAå›¾ä¼˜åŒ– |
| | `hicache_consumer_index` | ç»§æ‰¿åˆ°ä¸‹å±‚ | åˆ†å±‚ç¼“å­˜æ¶ˆè´¹è€…ç´¢å¼•ï¼Œç”¨äºCPU-GPUæ•°æ®åŒæ­¥ |
| **ç‰¹æ®ŠåŠŸèƒ½** | `has_stream` | æ‰¹æ¬¡ç‰¹æ€§ | æ‰¹æ¬¡ä¸­æ˜¯å¦åŒ…å«æµå¼è¯·æ±‚ |
| | `has_grammar` | æ‰¹æ¬¡ç‰¹æ€§ | æ‰¹æ¬¡ä¸­æ˜¯å¦åŒ…å«è¯­æ³•çº¦æŸè¯·æ±‚ |
| | `is_prefill_only` | æ‰¹æ¬¡ç‰¹æ€§ | æ˜¯å¦ä¸ºçº¯é¢„å¡«å……æ‰¹æ¬¡ï¼ˆmax_new_tokens=0ï¼‰ |
| | `chunked_req` | è°ƒåº¦å™¨ç‰¹æœ‰ | å½“å‰åˆ†å—è¯·æ±‚å¼•ç”¨ï¼Œç”¨äºè¶…é•¿åºåˆ—å¤„ç† |

### 3.4 GPUå¼ é‡æ•°æ®

ScheduleBatchåŒ…å«äº†ä¼ é€’ç»™æ¨¡å‹æ‰§è¡Œå™¨çš„æ‰¹é‡åŒ–å¼ é‡æ•°æ®ï¼š

```python
# æ¥è‡ªScheduleBatchçš„çœŸå®GPUå¼ é‡å­—æ®µ
class ScheduleBatch:
# Batched arguments to model runner
    input_ids: torch.Tensor = None            # shape: [b], int64 - è¾“å…¥token IDå¼ é‡
    seq_lens: torch.Tensor = None             # shape: [b], int64 - åºåˆ—é•¿åº¦å¼ é‡
    req_pool_indices: torch.Tensor = None     # shape: [b], int64 - è¯·æ±‚æ± ç´¢å¼•å¼ é‡
    out_cache_loc: torch.Tensor = None        # shape: [b], int64 - KVç¼“å­˜è¾“å‡ºä½ç½®
    output_ids: torch.Tensor = None           # shape: [b], int64 - è¾“å‡ºtokenå¼ é‡
    
    # é‡‡æ ·ä¿¡æ¯
sampling_info: SamplingBatchInfo = None           # å½“å‰æ‰¹æ¬¡é‡‡æ ·ä¿¡æ¯
next_batch_sampling_info: SamplingBatchInfo = None # ä¸‹ä¸€æ‰¹æ¬¡é‡‡æ ·ä¿¡æ¯
```

è¿™äº›å¼ é‡åŒ–çš„æ•°æ®æ˜¯GPUè®¡ç®—çš„ç›´æ¥è¾“å…¥ï¼Œå°†æ‰¹æ¬¡ä¸­æ‰€æœ‰è¯·æ±‚çš„ç›¸å…³ä¿¡æ¯ç»„ç»‡æˆäº†é€‚åˆå¹¶è¡Œå¤„ç†çš„æ ¼å¼ã€‚

---

## 4. ModelWorkerBatchæ•°æ®ç»“æ„

**æ¶æ„å®šä½**ï¼šModelWorkerBatchæ˜¯æ•°æ®æµè½¬æ¶æ„ä¸­çš„æ¨¡å‹å·¥ä½œå™¨å±‚æŠ½è±¡ï¼Œä½œä¸ºScheduleBatchå‘æ¨¡å‹æ‰§è¡Œå™¨ä¼ é€’çš„ä¸­é—´å±‚ã€‚å®ƒå»é™¤äº†è°ƒåº¦å™¨ç‰¹æœ‰çš„ç®¡ç†ä¿¡æ¯ï¼ˆå¦‚å†…å­˜æ± å¼•ç”¨ã€ç¼“å­˜ç®¡ç†ï¼‰ï¼Œåªä¿ç•™æ¨¡å‹æ¨ç†æ‰€éœ€çš„æ ¸å¿ƒæ•°æ®ã€‚

ModelWorkerBatchæ˜¯ScheduleBatchå‘æ¨¡å‹å·¥ä½œå™¨ä¼ é€’çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œå»é™¤äº†è°ƒåº¦å™¨ç‰¹æœ‰çš„ç®¡ç†ä¿¡æ¯ï¼Œä¸“æ³¨äºæ¨¡å‹æ¨ç†æ‰€éœ€çš„æ ¸å¿ƒæ•°æ®ã€‚

### 4.1 æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ

```python
@dataclasses.dataclass
class ModelWorkerBatch:
    """æ¨¡å‹å·¥ä½œå™¨æ‰¹æ¬¡çš„æ ¸å¿ƒæ¦‚å¿µ"""
    bid: int                               # æ‰¹æ¬¡ID
    forward_mode: ForwardMode              # å‰å‘æ¨¡å¼ï¼ˆé¢„å¡«å……/è§£ç ï¼‰
    input_ids: torch.Tensor               # è¾“å…¥tokenå¼ é‡
    req_pool_indices: torch.Tensor        # è¯·æ±‚æ± ç´¢å¼•
    seq_lens: torch.Tensor                # åºåˆ—é•¿åº¦
    out_cache_loc: torch.Tensor           # è¾“å‡ºç¼“å­˜ä½ç½®
    sampling_info: SamplingBatchInfo      # é‡‡æ ·ä¿¡æ¯
```

### 4.2 æºç å®ç°ç»†èŠ‚

```python
@dataclasses.dataclass
class ModelWorkerBatch:
    """çœŸå®çš„SGLang ModelWorkerBatchå®ç°"""
    
    # Basic batch info
    bid: int                               # æ‰¹æ¬¡ID
    forward_mode: ForwardMode              # å‰å‘æ¨¡å¼
    
    # Core tensors
    input_ids: torch.Tensor               # è¾“å…¥tokenå¼ é‡
    req_pool_indices: torch.Tensor        # è¯·æ±‚æ± ç´¢å¼•
    seq_lens: torch.Tensor                # åºåˆ—é•¿åº¦
    out_cache_loc: torch.Tensor           # KVç¼“å­˜è¾“å‡ºä½ç½®
    seq_lens_cpu: Optional[torch.Tensor]  # CPUä¸Šçš„åºåˆ—é•¿åº¦å¼ é‡
    seq_lens_sum: int                     # åºåˆ—é•¿åº¦æ€»å’Œ

    # For logprob
    return_logprob: bool
    top_logprobs_nums: Optional[List[int]]
    token_ids_logprobs: Optional[List[List[int]]]

    # For DP attention
    global_num_tokens: Optional[List[int]]
    global_num_tokens_for_logprob: Optional[List[int]]
    is_extend_in_batch: bool
    can_run_dp_cuda_graph: bool
    tbo_split_seq_index: Optional[int]
    global_forward_mode: Optional[ForwardMode]

    # For extend mode
    extend_num_tokens: Optional[int]
    extend_seq_lens: Optional[List[int]]
    extend_prefix_lens: Optional[List[int]]
    extend_logprob_start_lens: Optional[List[int]]
    extend_input_logprob_token_ids: Optional[torch.Tensor]

    # For multimodal
    multimodal_inputs: Optional[List[MultimodalInputs]]

    # For encoder-decoder
    encoder_cached: Optional[List[bool]]
    encoder_lens: Optional[torch.Tensor]
    encoder_lens_cpu: Optional[List[int]]
    encoder_out_cache_loc: Optional[torch.Tensor]

    # For LoRA
    lora_ids: Optional[List[str]]

    # Sampling info
    sampling_info: SamplingBatchInfo

    # Additional data
    orig_seq_lens: Optional[torch.Tensor] = None  # Qwen-1Mç›¸å…³
    input_embeds: Optional[torch.Tensor] = None   # è¾“å…¥åµŒå…¥
    token_type_ids: Optional[torch.Tensor] = None # è·¨ç¼–ç å™¨æ¨¡å‹

    # Speculative decoding
    spec_algorithm: SpeculativeAlgorithm = None
    spec_info: Optional[Union[EagleVerifyInput, EagleDraftInput]] = None

ğŸ’¡ **å®ç°è¯´æ˜**: çœŸå®çš„ModelWorkerBatchæœ‰30+ä¸ªå­—æ®µï¼ŒåŒ…å«DPæ³¨æ„åŠ›ã€ç¼–ç å™¨-è§£ç å™¨ã€LoRAã€æŠ•æœºè§£ç ç­‰å¤æ‚åŠŸèƒ½çš„æ”¯æŒã€‚æ•™å­¦ç‰ˆæœ¬çªå‡ºæ ¸å¿ƒçš„"è¾“å…¥â†’æ¨ç†â†’è¾“å‡º"æ•°æ®æµã€‚
```

è¿™ç§ç®€åŒ–ç¡®ä¿äº†æ¨¡å‹å·¥ä½œå™¨åªéœ€è¦å…³æ³¨æ¨ç†ç›¸å…³çš„ä¿¡æ¯ï¼Œæé«˜äº†æ•°æ®ä¼ é€’çš„æ•ˆç‡ã€‚

---

## 5. ForwardBatchæ•°æ®ç»“æ„

**æ¶æ„å®šä½**ï¼šForwardBatchæ˜¯æ•°æ®æµè½¬æ¶æ„çš„æœ€åº•å±‚ï¼Œä»£è¡¨GPUæ‰§è¡Œå±‚æŠ½è±¡ã€‚å®ƒå°†ModelWorkerBatchçš„é«˜å±‚æ•°æ®è¿›ä¸€æ­¥è½¬æ¢ä¸ºGPUå‹å¥½çš„å¼ é‡æ ¼å¼ï¼ŒåŒ…å«äº†æ¨¡å‹åœ¨GPUä¸Šæ‰§è¡Œå‰å‘ä¼ æ’­æ‰€éœ€çš„æ‰€æœ‰å¼ é‡æ•°æ®å’Œè®¡ç®—èµ„æºå¼•ç”¨ã€‚

ForwardBatchæ˜¯æ•°æ®æµè½¬çš„æœ€åº•å±‚ï¼ŒåŒ…å«GPUæ¨¡å‹æ‰§è¡Œæ—¶çš„æ‰€æœ‰å¼ é‡æ•°æ®ï¼Œæ˜¯å®é™…åœ¨GPUä¸Šæ‰§è¡Œè®¡ç®—çš„æ•°æ®æ ¼å¼ã€‚

### 5.1 æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ

**GPUè®¡ç®—çš„æ•°æ®è½½ä½“**ï¼šForwardBatchå°†ModelWorkerBatchè¿›ä¸€æ­¥è½¬æ¢ä¸ºGPUå‹å¥½çš„å¼ é‡æ ¼å¼ï¼Œä¸“æ³¨äºæ¨¡å‹æ¨ç†çš„æ ¸å¿ƒè®¡ç®—éœ€æ±‚ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹ä¸ºForwardBatchçš„æ ¸å¿ƒå¼ é‡ç®€åŒ–ç‰ˆæœ¬ï¼Œçªå‡ºGPUè®¡ç®—è¦ç´ ã€‚çœŸå®å®ç°åŒ…å«50+ä¸ªå­—æ®µï¼Œæ”¯æŒå„ç§é«˜çº§ä¼˜åŒ–ã€‚

```python
@dataclass
class ForwardBatch:
    """GPUå‰å‘è®¡ç®—æ‰¹æ¬¡ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    # åŸºç¡€ä¿¡æ¯
    forward_mode: ForwardMode           # å‰å‘æ¨¡å¼ï¼ˆé¢„å¡«å……/è§£ç ï¼‰
    batch_size: int                     # æ‰¹æ¬¡å¤§å°
    
    # æ ¸å¿ƒå¼ é‡æ•°æ®
    input_ids: torch.Tensor            # è¾“å…¥token IDå¼ é‡ [batch_size]
    req_pool_indices: torch.Tensor     # è¯·æ±‚æ± ç´¢å¼• [batch_size]
    seq_lens: torch.Tensor             # åºåˆ—é•¿åº¦ [batch_size]
    out_cache_loc: torch.Tensor        # KVç¼“å­˜è¾“å‡ºä½ç½® [batch_size]
    
    # ä½ç½®å’Œé•¿åº¦ä¿¡æ¯
    positions: torch.Tensor            # ä½ç½®ç¼–ç  [total_tokens]
    seq_lens_sum: int                  # æ‰€æœ‰åºåˆ—é•¿åº¦æ€»å’Œ
```

### 5.2 æºç å®ç°ç»†èŠ‚

**çœŸå®ForwardBatchçš„å®Œæ•´ç»“æ„**ï¼šç”Ÿäº§ç¯å¢ƒä¸­çš„ForwardBatchåŒ…å«äº†GPUæ¨¡å‹æ‰§è¡Œæ‰€éœ€çš„å…¨éƒ¨å¼ é‡æ•°æ®ï¼Œæ”¯æŒå¤šæ¨¡æ€ã€æŠ•æœºè§£ç ã€DPæ³¨æ„åŠ›ç­‰é«˜çº§åŠŸèƒ½ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹å±•ç¤ºçœŸå®ForwardBatchçš„ä¸»è¦å¼ é‡ï¼Œçœç•¥äº†éƒ¨åˆ†ä¼˜åŒ–ç›¸å…³å­—æ®µã€‚å®Œæ•´å®ç°è¯·å‚è€ƒ `sglang/srt/model_executor/forward_batch_info.py`ã€‚

```python
@dataclass
class ForwardBatch:
    """çœŸå®çš„SGLang ForwardBatchå®ç°"""
    
    # åŸºç¡€å‰å‘ä¿¡æ¯
    forward_mode: ForwardMode           # å‰å‘æ¨¡å¼
    batch_size: int                     # æ‰¹æ¬¡å¤§å°
    
    # æ ¸å¿ƒè¾“å…¥å¼ é‡
    input_ids: torch.Tensor            # è¾“å…¥token ID [batch_size]
    req_pool_indices: torch.Tensor     # è¯·æ±‚æ± ç´¢å¼• [batch_size]
    seq_lens: torch.Tensor             # åºåˆ—é•¿åº¦ [batch_size]
    out_cache_loc: torch.Tensor        # KVç¼“å­˜è¾“å‡ºä½ç½® [batch_size]
    seq_lens_sum: int                  # åºåˆ—é•¿åº¦æ€»å’Œ
    
    # ä½ç½®å’Œæ³¨æ„åŠ›ä¿¡æ¯
    positions: torch.Tensor = None     # ä½ç½®ç¼–ç å¼ é‡
    seq_lens_cpu: Optional[torch.Tensor] = None  # CPUä¸Šçš„åºåˆ—é•¿åº¦
    
    # é¢„å¡«å……æ¨¡å¼ä¸“ç”¨
    extend_num_tokens: Optional[int] = None      # æ‰©å±•tokenæ•°é‡
    extend_seq_lens: Optional[torch.Tensor] = None  # æ‰©å±•åºåˆ—é•¿åº¦
    extend_start_loc: Optional[torch.Tensor] = None  # æ‰©å±•èµ·å§‹ä½ç½®
    
    # å¤šæ¨¡æ€æ”¯æŒ
    multimodal_inputs: Optional[List] = None     # å¤šæ¨¡æ€è¾“å…¥æ•°æ®
    input_embeds: Optional[torch.Tensor] = None  # è¾“å…¥åµŒå…¥å¼ é‡
    
    # KVç¼“å­˜å’Œæ³¨æ„åŠ›åç«¯
    token_to_kv_pool: KVCache = None            # KVç¼“å­˜æ± å¼•ç”¨
    attn_backend: AttentionBackend = None       # æ³¨æ„åŠ›åç«¯
    
    # DPæ³¨æ„åŠ›ä¼˜åŒ–
    global_num_tokens_gpu: Optional[torch.Tensor] = None  # å…¨å±€tokenæ•°GPUå¼ é‡
    dp_padding_mode: Optional[DpPaddingMode] = None       # DPå¡«å……æ¨¡å¼
    
    # æŠ•æœºè§£ç æ”¯æŒ
    spec_algorithm: SpeculativeAlgorithm = None  # æŠ•æœºè§£ç ç®—æ³•
    spec_info: Optional[Union[EagleVerifyInput, EagleDraftInput]] = None
```

### 5.3 æ•°æ®è½¬æ¢æµç¨‹è¯¦è§£

**æ•°æ®ç»“æ„è½¬æ¢çš„æ ¸å¿ƒæœºåˆ¶**ï¼šSGLangé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„è½¬æ¢æµç¨‹ï¼Œå°†é«˜å±‚çš„è°ƒåº¦ä¿¡æ¯é€æ­¥è½¬æ¢ä¸ºGPUå¯æ‰§è¡Œçš„å¼ é‡æ•°æ®ã€‚

#### 5.3.1 ScheduleBatch â†’ ModelWorkerBatchè½¬æ¢

**è°ƒåº¦å™¨åˆ°æ¨¡å‹å·¥ä½œå™¨çš„æ•°æ®ç®€åŒ–**ï¼šå»é™¤è°ƒåº¦å™¨ç‰¹æœ‰çš„ç®¡ç†ä¿¡æ¯ï¼Œä¿ç•™æ¨¡å‹æ¨ç†å¿…éœ€çš„æ ¸å¿ƒæ•°æ®ã€‚

```python
# çœŸå®çš„ScheduleBatchåˆ°ModelWorkerBatchè½¬æ¢é€»è¾‘
def create_model_worker_batch(schedule_batch: ScheduleBatch) -> ModelWorkerBatch:
    """ä»ScheduleBatchåˆ›å»ºModelWorkerBatchï¼ˆçœŸå®è½¬æ¢é€»è¾‘ï¼‰"""
    return ModelWorkerBatch(
        # åŸºç¡€æ‰¹æ¬¡ä¿¡æ¯ï¼ˆç›´æ¥å¤åˆ¶ï¼‰
        bid=schedule_batch.bid,
        forward_mode=schedule_batch.forward_mode,
        
        # æ ¸å¿ƒå¼ é‡ï¼ˆç›´æ¥å¼•ç”¨ï¼Œé›¶æ‹·è´ï¼‰
        input_ids=schedule_batch.input_ids,
        req_pool_indices=schedule_batch.req_pool_indices,
        seq_lens=schedule_batch.seq_lens,
        out_cache_loc=schedule_batch.out_cache_loc,
        seq_lens_cpu=schedule_batch.seq_lens_cpu,
        seq_lens_sum=schedule_batch.seq_lens_sum,
        
        # é‡‡æ ·ä¿¡æ¯ï¼ˆç›´æ¥å¼•ç”¨ï¼‰
        sampling_info=schedule_batch.sampling_info,
        
        # å¤šæ¨¡æ€æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        multimodal_inputs=schedule_batch.multimodal_inputs,
        
        # LoRAä¿¡æ¯ï¼ˆä»è¯·æ±‚ä¸­æå–ï¼‰
        lora_ids=[req.lora_id for req in schedule_batch.reqs],
        
        # å¯¹æ•°æ¦‚ç‡ç›¸å…³ï¼ˆä»æ‰¹æ¬¡é…ç½®ä¸­è·å–ï¼‰
        return_logprob=schedule_batch.return_logprob,
        top_logprobs_nums=[req.top_logprobs_num for req in schedule_batch.reqs],
        
        # æ³¨æ„ï¼šå»é™¤äº†ä»¥ä¸‹è°ƒåº¦å™¨ç‰¹æœ‰ä¿¡æ¯
        # - req_to_token_poolï¼ˆå†…å­˜æ± å¼•ç”¨ï¼‰
        # - token_to_kv_pool_allocatorï¼ˆåˆ†é…å™¨å¼•ç”¨ï¼‰
        # - tree_cacheï¼ˆå‰ç¼€ç¼“å­˜å¼•ç”¨ï¼‰
        # - batch_is_fullï¼ˆè°ƒåº¦çŠ¶æ€ï¼‰
        # - launch_doneï¼ˆåŒæ­¥äº‹ä»¶ï¼‰
    )
```

#### 5.3.2 ModelWorkerBatch â†’ ForwardBatchè½¬æ¢

**æ¨¡å‹å·¥ä½œå™¨åˆ°GPUæ‰§è¡Œçš„å¼ é‡è½¬æ¢**ï¼š

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹å±•ç¤ºçœŸå®çš„ForwardBatchåˆ›å»ºè¿‡ç¨‹ï¼ŒåŸºäºå®é™…çš„`init_new`ç±»æ–¹æ³•ã€‚å®Œæ•´å®ç°è¯·å‚è€ƒ `sglang/srt/model_executor/forward_batch_info.py`ã€‚

```python
@classmethod
def init_new(cls, batch: ModelWorkerBatch, model_runner: ModelRunner):
    """ä»ModelWorkerBatchåˆ›å»ºForwardBatchçš„çœŸå®æ–¹æ³•"""
    return cls(
        # åŸºç¡€ä¿¡æ¯ä»ModelWorkerBatchç›´æ¥å¤åˆ¶
        forward_mode=batch.forward_mode,           # å‰å‘æ¨¡å¼
        batch_size=len(batch.seq_lens),           # æ‰¹æ¬¡å¤§å°
        input_ids=batch.input_ids,                # è¾“å…¥tokenå¼ é‡
        req_pool_indices=batch.req_pool_indices,  # è¯·æ±‚æ± ç´¢å¼•
        seq_lens=batch.seq_lens,                  # åºåˆ—é•¿åº¦
        out_cache_loc=batch.out_cache_loc,        # è¾“å‡ºç¼“å­˜ä½ç½®
        seq_lens_sum=batch.seq_lens_sum,          # åºåˆ—é•¿åº¦æ€»å’Œ
        
        # å¤šæ¨¡æ€å’Œç¼–ç å™¨æ”¯æŒ
        multimodal_inputs=batch.multimodal_inputs,      # å¤šæ¨¡æ€è¾“å…¥
        encoder_cached=batch.encoder_cached,            # ç¼–ç å™¨ç¼“å­˜çŠ¶æ€
        encoder_lens=batch.encoder_lens,                # ç¼–ç å™¨é•¿åº¦
        
        # é‡‡æ ·å’Œå¯¹æ•°æ¦‚ç‡
        return_logprob=batch.return_logprob,            # æ˜¯å¦è¿”å›å¯¹æ•°æ¦‚ç‡
        top_logprobs_nums=batch.top_logprobs_nums,      # top-kå¯¹æ•°æ¦‚ç‡æ•°é‡
        token_ids_logprobs=batch.token_ids_logprobs,    # tokenå¯¹æ•°æ¦‚ç‡
        
        # LoRAå’ŒæŠ•æœºè§£ç 
        lora_ids=batch.lora_ids,                        # LoRAé€‚é…å™¨IDåˆ—è¡¨
        sampling_info=batch.sampling_info,              # é‡‡æ ·ä¿¡æ¯
        spec_algorithm=batch.spec_algorithm,            # æŠ•æœºè§£ç ç®—æ³•
        
        # ä»model_runnerè·å–èµ„æºå¼•ç”¨
        req_to_token_pool=model_runner.req_to_token_pool,    # è¯·æ±‚åˆ°tokenæ± 
        token_to_kv_pool=model_runner.token_to_kv_pool,      # KVç¼“å­˜æ± 
        attn_backend=model_runner.attn_backend,              # æ³¨æ„åŠ›åç«¯
    )
```

---

## 6. å†…å­˜ç®¡ç†æ•°æ®ç»“æ„

### 6.1 å†…å­˜ç®¡ç†ç»„ä»¶

**ReqToTokenPool**ï¼šç®¡ç†è¯·æ±‚åˆ°tokenä½ç½®çš„æ˜ å°„å…³ç³»ï¼Œç»´æŠ¤æ¯ä¸ªè¯·æ±‚åœ¨å†…å­˜ä¸­çš„tokenä½ç½®ä¿¡æ¯ï¼Œæ”¯æŒåŠ¨æ€çš„å†…å­˜åˆ†é…å’Œå›æ”¶ã€‚

**BaseTokenToKVPoolAllocator**ï¼šKVç¼“å­˜åˆ†é…å™¨çš„æŠ½è±¡åŸºç±»ï¼Œè´Ÿè´£ç®¡ç†KVç¼“å­˜çš„åˆ†é…ã€å›æ”¶å’Œä¼˜åŒ–ã€‚ä¸åŒå®ç°å¯ä»¥é‡‡ç”¨ä¸åŒçš„åˆ†é…ç­–ç•¥ã€‚

**BasePrefixCache**ï¼šå‰ç¼€ç¼“å­˜çš„é€šç”¨æ¥å£ï¼Œå…·ä½“å®ç°å¦‚RadixCacheèƒ½å¤Ÿè¯†åˆ«å’Œå¤ç”¨è¯·æ±‚é—´çš„å…¬å…±å‰ç¼€ï¼Œæ˜¾è‘—å‡å°‘è®¡ç®—å¼€é”€ã€‚

### 6.2 å†…å­˜ç®¡ç†åä½œå¯è§†åŒ–

```mermaid
graph TD
    subgraph "ğŸ”„ å†…å­˜åˆ†é…æµç¨‹"
        A1["æ–°è¯·æ±‚åˆ°è¾¾<br/>Reqåˆ›å»º"]
        A2["è¯·æ±‚æ§½ä½åˆ†é…<br/>alloc_req_slots()"]
        A3["Tokenæ§½ä½åˆ†é…<br/>alloc_token_slots()"]
        A4["å‰ç¼€ç¼“å­˜æ£€æŸ¥<br/>tree_cache.match_prefix()"]
    end

    subgraph "ğŸ’¾ å†…å­˜æ± åä½œ"
        B1["ReqToTokenPool<br/>è¯·æ±‚â†’Tokenæ˜ å°„"]
        B2["BaseTokenToKVPoolAllocator<br/>KVç¼“å­˜åˆ†é…å™¨"]
        B3["BasePrefixCache<br/>å‰ç¼€ç¼“å­˜"]
    end

    subgraph "ğŸ—„ï¸ å†…å­˜é‡Šæ”¾æµç¨‹"
        C1["è¯·æ±‚å®Œæˆ<br/>finished_reasonè®¾ç½®"]
        C2["é‡Šæ”¾Tokenæ§½ä½<br/>free_token_slots()"]
        C3["é‡Šæ”¾è¯·æ±‚æ§½ä½<br/>free_req_slots()"]
        C4["ç¼“å­˜å¼•ç”¨å‡å°‘<br/>dec_lock_ref()"]
    end

    A1 --> A2
    A2 --> A3
    A3 --> A4
    A2 --> B1
    A3 --> B2
    A4 --> B3
    C1 --> C2
    C2 --> C3
    C3 --> C4
    C2 --> B2
    C3 --> B1
    C4 --> B3

    style A1 fill:#e3f2fd,color:#000000,stroke:#333
    style A2 fill:#e3f2fd,color:#000000,stroke:#333
    style A3 fill:#e3f2fd,color:#000000,stroke:#333
    style A4 fill:#e3f2fd,color:#000000,stroke:#333
    style B1 fill:#f1f8e9,color:#000000,stroke:#333
    style B2 fill:#f1f8e9,color:#000000,stroke:#333
    style B3 fill:#f1f8e9,color:#000000,stroke:#333
    style C1 fill:#fff3e0,color:#000000,stroke:#333
    style C2 fill:#fff3e0,color:#000000,stroke:#333
    style C3 fill:#fff3e0,color:#000000,stroke:#333
    style C4 fill:#fff3e0,color:#000000,stroke:#333
```

**å›¾ç¤ºè¯´æ˜**ï¼šè“è‰²è¡¨ç¤ºå†…å­˜åˆ†é…æµç¨‹ï¼Œç»¿è‰²è¡¨ç¤ºå†…å­˜æ± åä½œï¼Œæ©™è‰²è¡¨ç¤ºå†…å­˜é‡Šæ”¾æµç¨‹ã€‚å±•ç¤ºäº†SGLangå†…å­˜ç®¡ç†ç»„ä»¶ä¹‹é—´çš„åä½œå…³ç³»å’Œå®Œæ•´çš„å†…å­˜ç”Ÿå‘½å‘¨æœŸã€‚

### 6.3 å†…å­˜åˆ†é…åä½œæœºåˆ¶

**çœŸå®çš„å†…å­˜åˆ†é…æ–¹æ³•**ï¼šSGLangé€šè¿‡ScheduleBatchç±»çš„çœŸå®æ–¹æ³•æ¥ç®¡ç†å†…å­˜åˆ†é…å’Œé‡Šæ”¾ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹å±•ç¤ºçœŸå®çš„å†…å­˜åˆ†é…æ–¹æ³•ï¼Œçœç•¥äº†éƒ¨åˆ†é”™è¯¯å¤„ç†é€»è¾‘ã€‚å®Œæ•´å®ç°è¯·å‚è€ƒ `sglang/srt/managers/schedule_batch.py`ã€‚

```python
# æ¥è‡ªScheduleBatchçš„çœŸå®å†…å­˜åˆ†é…æ–¹æ³•
def alloc_req_slots(self, num_reqs: int):
    """åˆ†é…è¯·æ±‚æ§½ä½çš„çœŸå®æ–¹æ³•"""
    req_pool_indices = self.req_to_token_pool.alloc(num_reqs)  # åˆ†é…è¯·æ±‚æ± ç´¢å¼•
    if req_pool_indices is None:
        raise RuntimeError(
            "alloc_req_slots runs out of memory. "
            "Please set a smaller number for `--max-running-requests`. "
            f"{self.req_to_token_pool.available_size()=}, "
            f"{num_reqs=}, "
        )
    return req_pool_indices

def alloc_token_slots(self, num_tokens: int, backup_state: bool = False):
    """åˆ†é…tokenæ§½ä½çš„çœŸå®æ–¹æ³•"""
    # å¦‚æœéœ€è¦ï¼Œå…ˆæ¸…ç†æ ‘ç¼“å­˜
    self._evict_tree_cache_if_needed(num_tokens)
    
    # å¤‡ä»½çŠ¶æ€ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if backup_state:
        state = self.token_to_kv_pool_allocator.backup_state()
    
    # åˆ†é…KVç¼“å­˜ç©ºé—´
    out_cache_loc = self.token_to_kv_pool_allocator.alloc(num_tokens)
    if out_cache_loc is None:
        phase_str = "Prefill" if self.forward_mode.is_extend() else "Decode"
        error_msg = (
            f"{phase_str} out of memory. Try to lower your batch size.\n"
            f"Try to allocate {num_tokens} tokens.\n"
        )
        logger.error(error_msg)
        raise RuntimeError(error_msg)
    
    if backup_state:
        return out_cache_loc, state
    else:
        return out_cache_loc

# æ¥è‡ªPrefillAdderçš„çœŸå®è¯·æ±‚æ·»åŠ é€»è¾‘
def add_one_req(self, req: Req, has_chunked_req: bool):
    """PrefillAdderæ·»åŠ è¯·æ±‚çš„çœŸå®æ–¹æ³•ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    # åˆå§‹åŒ–è¯·æ±‚çš„ä¸‹ä¸€è½®è¾“å…¥
    req.init_next_round_input(self.tree_cache)
    
    # è®¡ç®—å‰ç¼€é•¿åº¦å’Œè¾“å…¥tokenæ•°
    prefix_len = len(req.prefix_indices)
    input_tokens = req.extend_input_len
    
    # æ£€æŸ¥æ˜¯å¦å¯ä»¥æ·»åŠ è¯·æ±‚
    if self.rem_chunk_tokens is None or input_tokens <= self.rem_chunk_tokens:
        # éåˆ†å—é¢„å¡«å……
        self.can_run_list.append(req)  # æ·»åŠ åˆ°å¯è¿è¡Œåˆ—è¡¨
        if self.is_hybrid:  # SWAæ··åˆç¼“å­˜
            swa_uuid_for_lock = self.tree_cache.inc_lock_ref(req.last_node)
            req.swa_uuid_for_lock = swa_uuid_for_lock
        else:
            self.tree_cache.inc_lock_ref(req.last_node)  # å¢åŠ é”å¼•ç”¨
        
        # æ›´æ–°é¢„å¡«å……é¢„ç®—
        self._update_prefill_budget(
            prefix_len, 
            input_tokens,
            min(req.sampling_params.max_new_tokens, CLIP_MAX_NEW_TOKENS)
        )
    else:
        # åˆ†å—é¢„å¡«å……å¤„ç†
        trunc_len = self.rem_chunk_tokens - self.page_size + 1
        req.extend_input_len = trunc_len
        req.fill_ids = req.fill_ids[:len(req.prefix_indices) + trunc_len]
        
        self.can_run_list.append(req)
        self.new_chunked_req = req  # è®¾ç½®ä¸ºæ–°çš„åˆ†å—è¯·æ±‚
    
    return self.budget_state()  # è¿”å›é¢„ç®—çŠ¶æ€
```

## 7. å¤šæ¨¡æ€æ•°æ®ç»“æ„

SGLangæ”¯æŒå¤šæ¨¡æ€è¾“å…¥ï¼Œé€šè¿‡ä¸“é—¨çš„æ•°æ®ç»“æ„æ¥ç»Ÿä¸€ç®¡ç†å›¾åƒã€è§†é¢‘ã€éŸ³é¢‘ç­‰ä¸åŒæ¨¡æ€çš„æ•°æ®ã€‚

### 7.1 MultimodalInputs

**å¤šæ¨¡æ€è¾“å…¥çš„ç»Ÿä¸€ç®¡ç†**ï¼šMultimodalInputsç±»æ˜¯å¤šæ¨¡æ€æ•°æ®çš„é¡¶å±‚å®¹å™¨ï¼Œç»Ÿä¸€ç®¡ç†å„ç§æ¨¡æ€çš„è¾“å…¥æ•°æ®å’Œç›¸å…³é…ç½®ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹å±•ç¤ºçœŸå®MultimodalInputsçš„æ ¸å¿ƒå­—æ®µï¼Œçœç•¥äº†éƒ¨åˆ†æ¨¡å‹ç‰¹å®šé…ç½®ã€‚å®Œæ•´å®ç°è¯·å‚è€ƒ `sglang/srt/managers/schedule_batch.py`ã€‚

```python
@dataclasses.dataclass
class MultimodalInputs:
    """çœŸå®çš„SGLangå¤šæ¨¡æ€è¾“å…¥æ•°æ®ç»“æ„"""
    
    # æ ¸å¿ƒæ•°æ®é¡¹
    mm_items: List[MultimodalDataItem]        # å¤šæ¨¡æ€æ•°æ®é¡¹åˆ—è¡¨
    image_pad_len: Optional[list] = None      # å›¾åƒå¡«å……é•¿åº¦
    num_image_tokens: Optional[int] = None    # å›¾åƒtokenæ•°é‡
    
    # å›¾åƒç›¸å…³token ID
    im_token_id: Optional[int] = None         # å›¾åƒtoken ID
    im_start_id: Optional[int] = None         # å›¾åƒå¼€å§‹token ID
    im_end_id: Optional[int] = None           # å›¾åƒç»“æŸtoken ID
    slice_start_id: Optional[int] = None      # åˆ‡ç‰‡å¼€å§‹token ID
    slice_end_id: Optional[int] = None        # åˆ‡ç‰‡ç»“æŸtoken ID
    
    # è§†é¢‘ç›¸å…³token ID
    video_token_id: Optional[int] = None      # è§†é¢‘token ID
    
    # éŸ³é¢‘ç›¸å…³token ID
    audio_token_id: Optional[int] = None      # éŸ³é¢‘token ID
    audio_start_id: Optional[int] = None      # éŸ³é¢‘å¼€å§‹token ID
    audio_end_id: Optional[int] = None        # éŸ³é¢‘ç»“æŸtoken ID
    
    # QWen2-VLç›¸å…³ä½ç½®ç¼–ç 
    mrope_positions: Optional[torch.Tensor] = None        # å¤šç»´ä½ç½®ç¼–ç 
    mrope_position_delta: Optional[torch.Tensor] = None   # ä½ç½®ç¼–ç å¢é‡
    
    def contains_image_inputs(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾åƒè¾“å…¥"""
        return any(item.is_image() for item in self.mm_items)
    
    def contains_video_inputs(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«è§†é¢‘è¾“å…¥"""
        return any(item.is_video() for item in self.mm_items)
    
    def contains_audio_inputs(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«éŸ³é¢‘è¾“å…¥"""
        return any(item.is_audio() for item in self.mm_items)
```

### 7.2 MultimodalDataItem

**å•ä¸ªå¤šæ¨¡æ€æ•°æ®é¡¹çš„ç»“æ„**ï¼šMultimodalDataItemè¡¨ç¤ºå•ä¸ªæ¨¡æ€çš„æ•°æ®ï¼ŒåŒ…å«åŸå§‹ç‰¹å¾ã€é¢„è®¡ç®—åµŒå…¥å’Œæ¨¡å‹ç‰¹å®šæ•°æ®ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹å±•ç¤ºçœŸå®MultimodalDataItemçš„ä¸»è¦å­—æ®µï¼Œçœç•¥äº†éƒ¨åˆ†è¾…åŠ©æ–¹æ³•ã€‚å®Œæ•´å®ç°è¯·å‚è€ƒ `sglang/srt/managers/schedule_batch.py`ã€‚

```python
@dataclasses.dataclass
class MultimodalDataItem:
    """çœŸå®çš„SGLangå¤šæ¨¡æ€æ•°æ®é¡¹ç»“æ„"""
    
    modality: Modality                        # æ¨¡æ€ç±»å‹ï¼ˆå›¾åƒ/è§†é¢‘/éŸ³é¢‘ï¼‰
    hash: int = None                          # æ•°æ®å“ˆå¸Œå€¼
    pad_value: int = None                     # å¡«å……å€¼
    offsets: Optional[list] = None            # åç§»é‡åˆ—è¡¨
    
    # åŸå§‹ç‰¹å¾æ•°æ®ï¼ˆäºŒé€‰ä¸€ï¼‰
    feature: Union[torch.Tensor, np.ndarray] = None              # åŸå§‹ç‰¹å¾ï¼ˆå¦‚pixel_valuesï¼‰
    precomputed_embeddings: Optional[Union[torch.Tensor, np.ndarray]] = None  # é¢„è®¡ç®—åµŒå…¥
    
    # æ¨¡å‹ç‰¹å®šæ•°æ®
    model_specific_data: dict[str, Any] = dataclasses.field(default_factory=dict)
    
    def __getattr__(self, name: str):
        """åŠ¨æ€è®¿é—®æ¨¡å‹ç‰¹å®šæ•°æ®"""
        if (
            "model_specific_data" in self.__dict__
            and name in self.__dict__["model_specific_data"]
        ):
            return self.__dict__["model_specific_data"][name]
        else:
            raise AttributeError(f"'{self.__class__.__name__}' object has no attribute '{name}'")
    
    def set(self, key: str, value: Any):
        """è®¾ç½®æ¨¡å‹ç‰¹å®šæ•°æ®"""
        if key in self.__dict__:
            self.__dict__[key] = value
        else:
            self.model_specific_data[key] = value
```

## 8. æ‰¹é‡è¯·æ±‚æ•°æ®ç»“æ„

**æ¶æ„å®šä½**ï¼šæ‰¹é‡è¯·æ±‚æ•°æ®ç»“æ„æ˜¯SGLangç½‘ç»œä¼ è¾“å±‚çš„ä¼˜åŒ–æŠ½è±¡ï¼Œé€šè¿‡å°†å¤šä¸ªå•ç‹¬è¯·æ±‚æ‰“åŒ…æˆæ‰¹é‡æ ¼å¼ï¼Œæ˜¾è‘—å‡å°‘ç½‘ç»œå¾€è¿”æ¬¡æ•°å’Œåºåˆ—åŒ–å¼€é”€ï¼Œæé«˜ç³»ç»Ÿæ•´ä½“ååé‡ã€‚

### 8.1 BatchTokenizedGenerateReqInputè¯¦è§£

**æ‰¹é‡ç”Ÿæˆè¯·æ±‚çš„å®Œæ•´å®ç°**ï¼šBatchTokenizedGenerateReqInputæ˜¯ç½‘ç»œä¼ è¾“ä¼˜åŒ–çš„æ ¸å¿ƒï¼Œæ”¯æŒæ‰¹é‡å¤„ç†ä»¥æé«˜æ•ˆç‡ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹å±•ç¤ºçœŸå®BatchTokenizedGenerateReqInputçš„å®Œæ•´å®šä¹‰å’Œä½¿ç”¨æ–¹æ³•ã€‚å®Œæ•´å®ç°è¯·å‚è€ƒ `sglang/srt/managers/io_struct.py`ã€‚

```python
@dataclass
class BatchTokenizedGenerateReqInput:
    """çœŸå®çš„SGLangæ‰¹é‡ç”Ÿæˆè¯·æ±‚è¾“å…¥"""
    # The batch of tokenized requests
    batch: List[TokenizedGenerateReqInput]  # æ‰¹é‡å·²tokenizeçš„è¯·æ±‚åˆ—è¡¨
    
    def __len__(self):
        """è¿”å›æ‰¹æ¬¡å¤§å°"""
        return len(self.batch)
    
    def __getitem__(self, i):
        """æ”¯æŒç´¢å¼•è®¿é—®ï¼Œä¾¿äºéå†å¤„ç†"""
        return self.batch[i]
    
    def __iter__(self):
        """æ”¯æŒè¿­ä»£è®¿é—®ï¼Œç”¨äºæ‰¹é‡å¤„ç†å¾ªç¯"""
        return iter(self.batch)
    

```

**æ‰¹é‡è¯·æ±‚çš„å¤„ç†ä¼˜åŠ¿**ï¼šé€šè¿‡å‡å°‘ç½‘ç»œå¾€è¿”æ¬¡æ•°å’Œè¯·æ±‚å¤„ç†å¼€é”€æ¥æé«˜ç³»ç»Ÿæ•´ä½“æ€§èƒ½ã€‚

### 8.2 BatchTokenizedEmbeddingReqInputè¯¦è§£

**æ‰¹é‡åµŒå…¥è¯·æ±‚çš„çœŸå®ç»“æ„**ï¼šSGLangæ”¯æŒæ‰¹é‡åµŒå…¥è¯·æ±‚ä»¥æé«˜å¤„ç†æ•ˆç‡ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹å±•ç¤ºçœŸå®BatchTokenizedEmbeddingReqInputçš„å®Œæ•´å®šä¹‰ã€‚å®Œæ•´å®ç°è¯·å‚è€ƒ `sglang/srt/managers/io_struct.py`ã€‚

```python
@dataclass
class BatchTokenizedEmbeddingReqInput:
    """çœŸå®çš„SGLangæ‰¹é‡åµŒå…¥è¯·æ±‚è¾“å…¥"""
    # The batch of tokenized embedding requests
    batch: List[TokenizedEmbeddingReqInput]  # æ‰¹é‡è¯·æ±‚åˆ—è¡¨

    def __len__(self):
        return len(self.batch)

    def __getitem__(self, i):
        return self.batch[i]

    def __iter__(self):
        return iter(self.batch)
```

### 8.3 æ‰¹é‡å¤„ç†çš„æ€§èƒ½ä¼˜åŒ–

**ç½‘ç»œä¼ è¾“ä¼˜åŒ–**ï¼šæ‰¹é‡è¯·æ±‚é€šè¿‡å‡å°‘ç½‘ç»œå¾€è¿”æ¬¡æ•°å’Œåºåˆ—åŒ–å¼€é”€æ¥æé«˜ç³»ç»Ÿæ•´ä½“æ€§èƒ½ã€‚ç›¸æ¯”å•ä¸ªè¯·æ±‚é€ä¸€å‘é€ï¼Œæ‰¹é‡è¯·æ±‚èƒ½å¤Ÿæ˜¾è‘—å‡å°‘ç½‘ç»œå»¶è¿Ÿå’Œå¸¦å®½æ¶ˆè€—ã€‚

## 9. åˆ†ç¦»å¼æ¶æ„æ•°æ®ç»“æ„

**æ¶æ„å®šä½**ï¼šåˆ†ç¦»å¼æ¶æ„æ•°æ®ç»“æ„æ”¯æŒSGLangçš„é¢„å¡«å……/è§£ç åˆ†ç¦»éƒ¨ç½²æ¨¡å¼ï¼Œé€šè¿‡ä¸“é—¨çš„å­—æ®µç®¡ç†è·¨èŠ‚ç‚¹çš„KVç¼“å­˜ä¼ è¾“å’Œè¯·æ±‚è·¯ç”±ï¼Œå®ç°å¤§è§„æ¨¡åˆ†å¸ƒå¼æ¨ç†ã€‚

### 9.1 åˆ†ç¦»å¼æ¶æ„æ ¸å¿ƒå­—æ®µ

**Reqä¸­çš„åˆ†ç¦»å¼æ‰©å±•**ï¼šä¸ºæ”¯æŒåˆ†ç¦»å¼éƒ¨ç½²ï¼ŒReqç±»åŒ…å«äº†ä¸“é—¨çš„åˆ†ç¦»å¼æ¶æ„å­—æ®µã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹å±•ç¤ºReqç±»ä¸­åˆ†ç¦»å¼æ¶æ„çš„çœŸå®å­—æ®µå®šä¹‰ã€‚å®Œæ•´å®ç°è¯·å‚è€ƒ `sglang/srt/managers/schedule_batch.py`ã€‚

```python
class Req:
    """Reqç±»ä¸­çš„åˆ†ç¦»å¼æ¶æ„ç›¸å…³å­—æ®µï¼ˆçœŸå®å®ç°ï¼‰"""
    
    # KVç¼“å­˜ä¼ è¾“ç»„ä»¶
    disagg_kv_sender: Optional[BaseKVSender] = None      # KVå‘é€å™¨ï¼Œç”¨äºå‘é€é¢„å¡«å……çš„KVç¼“å­˜
    disagg_kv_receiver: Optional[BaseKVReceiver] = None  # KVæ¥æ”¶å™¨ï¼Œç”¨äºæ¥æ”¶è¿œç¨‹KVç¼“å­˜
    
    # Bootstrapè¿æ¥é…ç½®
    bootstrap_host: Optional[str] = None                 # å¯åŠ¨ä¸»æœºåœ°å€ï¼Œç”¨äºèŠ‚ç‚¹å‘ç°
    bootstrap_port: Optional[int] = None                 # å¯åŠ¨ç«¯å£å·ï¼Œç”¨äºæœåŠ¡è¿æ¥
    bootstrap_room: Optional[str] = None                 # å¯åŠ¨æˆ¿é—´IDï¼Œç”¨äºè¯·æ±‚è·¯ç”±å’Œéš”ç¦»
    
    # æ•°æ®å¹¶è¡Œé…ç½®
    data_parallel_rank: Optional[int] = None             # æ•°æ®å¹¶è¡Œrankï¼Œç”¨äºè´Ÿè½½å‡è¡¡
    
    # SWAæ··åˆç¼“å­˜ç›¸å…³å­—æ®µ
    swa_uuid_for_lock: Optional[str] = None             # SWAé”å®šUUIDï¼Œé˜²æ­¢å¹¶å‘è®¿é—®å†²çª
    
```

**åˆ†ç¦»å¼æ¶æ„å­—æ®µè¯´æ˜**ï¼šè¿™äº›å­—æ®µæ”¯æŒSGLangçš„é¢„å¡«å……/è§£ç åˆ†ç¦»éƒ¨ç½²æ¨¡å¼ï¼Œå®ç°å¤§è§„æ¨¡åˆ†å¸ƒå¼æ¨ç†ã€‚

## 10. æ•°æ®ç»“æ„åä½œå…³ç³»è¯¦è§£

### 10.1 åä½œå…³ç³»å¯è§†åŒ–

```mermaid
graph TD
    subgraph "ğŸ”„ æ•°æ®ç»„ç»‡å±‚æ¬¡"
        A1["Req<br/>åŸå­å•ä½"]
        A2["List[Req]<br/>è¯·æ±‚é›†åˆ"]
        A3["ScheduleBatch<br/>æ‰¹æ¬¡æŠ½è±¡"]
    end

    subgraph "ğŸ’¾ èµ„æºç®¡ç†å±‚"
        B1["ReqToTokenPool<br/>è¯·æ±‚æ˜ å°„"]
        B2["BaseTokenToKVPoolAllocator<br/>KVåˆ†é…å™¨"]
        B3["BasePrefixCache<br/>å‰ç¼€ç¼“å­˜"]
    end

    subgraph "ğŸ”„ æ•°æ®è½¬æ¢å±‚"
        C1["ModelWorkerBatch<br/>æ¨¡å‹å±‚"]
        C2["ForwardBatch<br/>æ‰§è¡Œå±‚"]
        C3["GPU Tensors<br/>è®¡ç®—è½½ä½“"]
    end

    subgraph "ğŸŒ æ‰©å±•åŠŸèƒ½å±‚"
        D1["MultimodalInputs<br/>å¤šæ¨¡æ€"]
        D2["BatchRequests<br/>æ‰¹é‡ä¼˜åŒ–"]
        D3["Disaggregation<br/>åˆ†ç¦»å¼"]
    end

    A1 --> A2
    A2 --> A3
    A3 --> B1
    A3 --> B2
    A3 --> B3
    A3 --> C1
    C1 --> C2
    C2 --> C3
    A1 --> D1
    A2 --> D2
    A3 --> D3

    style A1 fill:#e3f2fd,color:#000000,stroke:#333
    style A2 fill:#e3f2fd,color:#000000,stroke:#333
    style A3 fill:#e3f2fd,color:#000000,stroke:#333
    style B1 fill:#f1f8e9,color:#000000,stroke:#333
    style B2 fill:#f1f8e9,color:#000000,stroke:#333
    style B3 fill:#f1f8e9,color:#000000,stroke:#333
    style C1 fill:#fff3e0,color:#000000,stroke:#333
    style C2 fill:#fff3e0,color:#000000,stroke:#333
    style C3 fill:#fff3e0,color:#000000,stroke:#333
    style D1 fill:#ffebee,color:#000000,stroke:#333
    style D2 fill:#ffebee,color:#000000,stroke:#333
    style D3 fill:#ffebee,color:#000000,stroke:#333
```

**å›¾ç¤ºè¯´æ˜**ï¼šè“è‰²è¡¨ç¤ºæ•°æ®ç»„ç»‡å±‚æ¬¡ï¼Œç»¿è‰²è¡¨ç¤ºèµ„æºç®¡ç†ï¼Œæ©™è‰²è¡¨ç¤ºæ•°æ®è½¬æ¢ï¼Œçº¢è‰²è¡¨ç¤ºæ‰©å±•åŠŸèƒ½ã€‚å±•ç¤ºäº†SGLangæ•°æ®ç»“æ„çš„å®Œæ•´åä½œç½‘ç»œã€‚

### 10.2 åä½œæœºåˆ¶è¯´æ˜

**æ•°æ®æµè½¬çš„åä½œè¿‡ç¨‹**ï¼šå„æ•°æ®ç»“æ„é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ¥å£è¿›è¡Œåä½œï¼Œç¡®ä¿é«˜æ•ˆçš„æ•°æ®æµè½¬å’Œèµ„æºç®¡ç†ã€‚

**æ ¸å¿ƒåä½œå…³ç³»**ï¼š
- **Reqå¯¹è±¡**ï¼šä½œä¸ºæœ€åŸºæœ¬çš„æ•°æ®å•å…ƒï¼Œæ‰¿è½½è¯·æ±‚çš„å®Œæ•´ä¿¡æ¯
- **ScheduleBatch**ï¼šå°†å¤šä¸ªReqç»„ç»‡æˆæ‰¹æ¬¡ï¼Œç®¡ç†èµ„æºæ± å¼•ç”¨å’Œè°ƒåº¦çŠ¶æ€
- **å†…å­˜ç®¡ç†ç»„ä»¶**ï¼šReqToTokenPoolã€BaseTokenToKVPoolAllocatorã€BasePrefixCacheååŒå·¥ä½œï¼Œç¡®ä¿å†…å­˜çš„é«˜æ•ˆåˆ†é…å’Œå›æ”¶
- **æ•°æ®è½¬æ¢**ï¼šScheduleBatchâ†’ModelWorkerBatchâ†’ForwardBatchçš„é€å±‚ç®€åŒ–ï¼Œæ¯å±‚ä¸“æ³¨äºç‰¹å®šçš„æŠ½è±¡çº§åˆ«

---

## 11. æ ¸å¿ƒè®¾è®¡åŸåˆ™

SGLangçš„æ•°æ®ç»“æ„è®¾è®¡ä½“ç°äº†å‡ ä¸ªé‡è¦çš„è®¾è®¡åŸåˆ™ï¼š

**åˆ†å±‚æŠ½è±¡**: é€šè¿‡Reqâ†’ScheduleBatchâ†’ModelWorkerBatchçš„åˆ†å±‚è®¾è®¡ï¼Œç³»ç»Ÿèƒ½å¤Ÿåœ¨ä¸åŒæŠ½è±¡å±‚é¢è¿›è¡Œä¼˜åŒ–ï¼Œè°ƒåº¦å™¨å…³æ³¨é«˜å±‚å†³ç­–ï¼Œæ¨¡å‹æ‰§è¡Œå™¨å…³æ³¨åº•å±‚è®¡ç®—ã€‚

**æ¨¡å—åŒ–è®¾è®¡**: å„ä¸ªæ•°æ®ç»“æ„èŒè´£æ¸…æ™°ï¼Œç›¸äº’ä¹‹é—´é€šè¿‡æ˜ç¡®çš„æ¥å£è¿›è¡Œäº¤äº’ï¼Œæé«˜äº†ç³»ç»Ÿçš„å¯ç»´æŠ¤æ€§å’Œå¯æµ‹è¯•æ€§ã€‚

**æ€§èƒ½ä¼˜åŒ–**: æ•°æ®ç»“æ„å……åˆ†è€ƒè™‘äº†æ€§èƒ½å› ç´ ï¼š
- æ‰¹é‡åŒ–å¤„ç†å‡å°‘å‡½æ•°è°ƒç”¨å¼€é”€
- å¼ é‡åŒ–æ•°æ®æ”¯æŒGPUå¹¶è¡Œè®¡ç®—
- å†…å­˜æ± è®¾è®¡æé«˜å†…å­˜å±€éƒ¨æ€§
- ç¼“å­˜å‹å¥½çš„æ•°æ®å¸ƒå±€

**æ‰©å±•æ€§**: é€šè¿‡æŠ½è±¡åŸºç±»å’Œmixinæ¨¡å¼ï¼Œæ•°æ®ç»“æ„å…·å¤‡è‰¯å¥½çš„æ‰©å±•æ€§ï¼Œèƒ½å¤Ÿæ”¯æŒæ–°åŠŸèƒ½çš„æ·»åŠ å’Œç°æœ‰åŠŸèƒ½çš„ä¼˜åŒ–ã€‚

### 11.1 å®ç°ç‰¹è‰²

**æºç å‡†ç¡®æ€§**: æœ¬æ–‡æ¡£åŸºäºçœŸå®SGLangæºç ç¼–å†™ï¼Œæ‰€æœ‰æ•°æ®ç»“æ„å®šä¹‰éƒ½æ¥è‡ªå®é™…å®ç°ï¼Œç¡®ä¿æŠ€æœ¯å‡†ç¡®æ€§ã€‚

**æ•™å­¦ä¸å®è·µå¹¶é‡**: é‡‡ç”¨"æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ + æºç å®ç°ç»†èŠ‚"çš„åŒé‡ç»“æ„ï¼Œæ—¢ä¾¿äºç†è§£è®¾è®¡æ€æƒ³ï¼Œåˆæä¾›å…·ä½“å®ç°å‚è€ƒã€‚

**å¤æ‚æ€§é€æ˜**: æ˜ç¡®å±•ç¤ºäº†æ•™å­¦ç®€åŒ–ç‰ˆæœ¬ä¸çœŸå®æºç çš„å·®å¼‚ï¼Œè®©å¼€å‘è€…äº†è§£å®é™…ç³»ç»Ÿçš„å¤æ‚æ€§ã€‚

### 11.2 æ¼”è¿›è¶‹åŠ¿

SGLangçš„æ•°æ®ç»“æ„å±•ç°äº†ç°ä»£æ¨ç†ç³»ç»Ÿçš„æ¼”è¿›æ–¹å‘ï¼š
- **å¤šæ¨¡æ€æ”¯æŒ**: ä»çº¯æ–‡æœ¬æ‰©å±•åˆ°å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘
- **åˆ†ç¦»å¼æ¶æ„**: æ”¯æŒé¢„å¡«å……/è§£ç åˆ†ç¦»çš„å¤§è§„æ¨¡éƒ¨ç½²
- **é«˜çº§ä¼˜åŒ–**: æŠ•æœºè§£ç ã€æ··åˆç¼“å­˜ã€DPæ³¨æ„åŠ›ç­‰å‰æ²¿æŠ€æœ¯
- **äº§ä¸šåŒ–éœ€æ±‚**: ä¼šè¯ç®¡ç†ã€LoRAé€‚é…å™¨ã€ç›‘æ§è°ƒè¯•ç­‰å·¥ç¨‹ç‰¹æ€§

ç†è§£è¿™äº›æ ¸å¿ƒæ•°æ®ç»“æ„åŠå…¶ç›¸äº’å…³ç³»ï¼Œæ˜¯æ·±å…¥æŒæ¡SGLangè°ƒåº¦å™¨å·¥ä½œæœºåˆ¶çš„å…³é”®ã€‚è¿™äº›æ•°æ®ç»“æ„ä¸ä»…æ‰¿è½½ç€ç³»ç»Ÿçš„æ ¸å¿ƒä¿¡æ¯ï¼Œè¿˜ä½“ç°äº†SGLangåœ¨æ€§èƒ½ã€å¯ç»´æŠ¤æ€§å’Œæ‰©å±•æ€§æ–¹é¢çš„è®¾è®¡è€ƒé‡ã€‚

**æ‰¿ä¸Šå¯ä¸‹**ï¼šåœ¨ç¬¬ä¸€ç« æˆ‘ä»¬äº†è§£äº†è°ƒåº¦å™¨çš„æ•´ä½“æ¶æ„å’Œè®¾è®¡ç†å¿µï¼Œæœ¬ç« æ·±å…¥å‰–æäº†æ”¯æ’‘è¿™äº›æ¶æ„çš„æ ¸å¿ƒæ•°æ®æŠ½è±¡ã€‚æœ‰äº†è¿™äº›åŸºç¡€ï¼Œæˆ‘ä»¬å°±ä¸ºæ·±å…¥æ¢è®¨è°ƒåº¦å™¨åœ¨å®é™…è¿è¡Œä¸­çš„è¯·æ±‚å¤„ç†æœºåˆ¶ã€æ‰¹æ¬¡è°ƒåº¦ç­–ç•¥å’Œå†…å­˜ç®¡ç†ç®—æ³•å¥ å®šäº†åšå®åŸºç¡€ã€‚æ¥ä¸‹æ¥çš„ç« èŠ‚å°†å±•ç¤ºè¿™äº›æ•°æ®ç»“æ„æ˜¯å¦‚ä½•åœ¨å…·ä½“çš„è°ƒåº¦æµç¨‹ä¸­å‘æŒ¥ä½œç”¨çš„ã€‚
