# æ ¸å¿ƒæ•°æ®ç»“æ„

---

SGLangè°ƒåº¦å™¨çš„é«˜æ•ˆè¿è¡Œä¾èµ–äºä¸€ç³»åˆ—ç²¾å¿ƒè®¾è®¡çš„æ•°æ®ç»“æ„ã€‚è¿™äº›æ•°æ®ç»“æ„ä¸ä»…æ‰¿è½½ç€è¯·æ±‚çš„å„ç§ä¿¡æ¯ï¼Œè¿˜è´Ÿè´£æ‰¹æ¬¡ç®¡ç†ã€å†…å­˜åˆ†é…å’Œæ¨¡å‹æ¨ç†çš„åè°ƒã€‚ç†è§£è¿™äº›æ ¸å¿ƒæ•°æ®ç»“æ„æ˜¯æ·±å…¥æŒæ¡SGLangè°ƒåº¦å™¨å·¥ä½œåŸç†çš„åŸºç¡€ã€‚

---

## 1. æ•°æ®æµè½¬æ¶æ„

SGLangé‡‡ç”¨åˆ†å±‚çš„æ•°æ®å¤„ç†æ¶æ„ï¼Œè¯·æ±‚ä»æ¥æ”¶åˆ°æ‰§è¡Œç»å†äº†å››ä¸ªä¸»è¦çš„æ•°æ®ç»“æ„å±‚æ¬¡ï¼Œæ¯ä¸€å±‚éƒ½æœ‰æ˜ç¡®çš„èŒè´£åˆ†å·¥ï¼š

**è°ƒåº¦å™¨å±‚é¢çš„ScheduleBatch**è´Ÿè´£å­˜å‚¨è°ƒåº¦å™¨éœ€è¦çš„æ‰€æœ‰ä¿¡æ¯ï¼ŒåŒ…æ‹¬è¯·æ±‚åˆ—è¡¨ã€å†…å­˜æ± å¼•ç”¨ã€ç¼“å­˜ç®¡ç†ç­‰é«˜å±‚è°ƒåº¦å†³ç­–æ‰€éœ€çš„æ•°æ®ã€‚

**æ¨¡å‹å·¥ä½œå™¨å±‚é¢çš„ModelWorkerBatch**æ˜¯ScheduleBatchçš„ç®€åŒ–ç‰ˆæœ¬ï¼ŒåªåŒ…å«æ¨¡å‹å‰å‘æ¨ç†æ‰€éœ€çš„æ ¸å¿ƒæ•°æ®ï¼Œå»é™¤äº†è°ƒåº¦å™¨ç‰¹æœ‰çš„ç®¡ç†ä¿¡æ¯ã€‚

**æ¨¡å‹æ‰§è¡Œå™¨å±‚é¢çš„ForwardBatch**åŒ…å«æœ€åº•å±‚çš„GPUå¼ é‡æ•°æ®ï¼Œæ˜¯å®é™…åœ¨GPUä¸Šæ‰§è¡Œè®¡ç®—æ—¶ä½¿ç”¨çš„æ•°æ®æ ¼å¼ã€‚

### 1.1 æ•°æ®æµè½¬å¯è§†åŒ–

```mermaid
graph TD
    A["ğŸ”„ æ•°æ®æµè½¬æ¶æ„"] --> B["Reqç±»<br/>è¯·æ±‚å¯¹è±¡"]
    A --> C["ScheduleBatchç±»<br/>è°ƒåº¦å™¨æ‰¹æ¬¡"]
    A --> D["ModelWorkerBatchç±»<br/>æ¨¡å‹å·¥ä½œå™¨æ‰¹æ¬¡"]
    A --> E["ForwardBatchç±»<br/>GPUå‰å‘æ‰¹æ¬¡"]
    
    B --> |"ç»„ç»‡æˆæ‰¹æ¬¡"| C
    C --> |"ç®€åŒ–ä¼ é€’"| D
    D --> |"å¼ é‡è½¬æ¢"| E
    
    F["ğŸ“‹ è°ƒåº¦å™¨å±‚é¢"] --> C
    G["âš™ï¸ æ¨¡å‹å·¥ä½œå™¨å±‚é¢"] --> D
    H["ğŸ”¥ GPUæ‰§è¡Œå±‚é¢"] --> E
    
    C --> |"åŒ…å«"| I["reqs: List[Req]<br/>req_to_token_pool<br/>tree_cache"]
    D --> |"åŒ…å«"| J["input_ids: Tensor<br/>seq_lens: Tensor<br/>sampling_info"]
    E --> |"åŒ…å«"| K["positions: Tensor<br/>attn_backend<br/>token_to_kv_pool"]
    
    style A fill:#e1f5fe,color:#000000
    style B fill:#f3e5f5,color:#000000
    style C fill:#e8f5e8,color:#000000
    style D fill:#fff3e0,color:#000000
    style E fill:#ffebee,color:#000000
    style F fill:#e3f2fd,color:#000000
    style G fill:#f1f8e9,color:#000000
    style H fill:#fff8e1,color:#000000
    style I fill:#f0f4c3,color:#000000
    style J fill:#e8eaf6,color:#000000
    style K fill:#fce4ec,color:#000000
```

**å›¾ç¤ºè¯´æ˜**ï¼šè“è‰²èŠ‚ç‚¹è¡¨ç¤ºæ•°æ®æµè½¬æ¶æ„æ€»è§ˆï¼Œç´«è‰²è¡¨ç¤ºåŸºç¡€è¯·æ±‚å¯¹è±¡ï¼Œç»¿è‰²è¡¨ç¤ºè°ƒåº¦å™¨æ‰¹æ¬¡ï¼Œæ©™è‰²è¡¨ç¤ºæ¨¡å‹å·¥ä½œå™¨æ‰¹æ¬¡ï¼Œçº¢è‰²è¡¨ç¤ºGPUå‰å‘æ‰¹æ¬¡ã€‚ç®­å¤´å±•ç¤ºäº†æ•°æ®çš„è½¬æ¢å’Œä¼ é€’å…³ç³»ã€‚

è¿™ç§åˆ†å±‚è®¾è®¡ç¡®ä¿äº†æ¯ä¸ªç»„ä»¶åªå¤„ç†ä¸å…¶èŒè´£ç›¸å…³çš„æ•°æ®ï¼Œæé«˜äº†ç³»ç»Ÿçš„æ¨¡å—åŒ–ç¨‹åº¦å’Œæ‰§è¡Œæ•ˆç‡ã€‚

---

## 2. Reqæ•°æ®ç»“æ„

Reqç±»æ˜¯SGLangä¸­è¡¨ç¤ºå•ä¸ªè¯·æ±‚çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼ŒåŒ…å«äº†è¯·æ±‚ä»åˆ›å»ºåˆ°å®Œæˆçš„å…¨éƒ¨ä¿¡æ¯ã€‚

### 2.1 æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ

**Reqç±»çš„è®¾è®¡ç†å¿µ**ï¼šReqç±»æ˜¯SGLangä¸­è¡¨ç¤ºå•ä¸ªè¯·æ±‚çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼ŒåŒ…å«äº†ä»è¾“å…¥åˆ°è¾“å‡ºçš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸä¿¡æ¯ã€‚è®¾è®¡ä¸Šé‡‡ç”¨äº†ä¸°å¯Œçš„å‚æ•°æ”¯æŒï¼Œèƒ½å¤Ÿå¤„ç†æ–‡æœ¬ç”Ÿæˆã€åµŒå…¥è®¡ç®—ã€å¤šæ¨¡æ€è¾“å…¥ç­‰å¤šç§åœºæ™¯ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹ä¸ºReqç±»çš„æ ¸å¿ƒå±æ€§ç®€åŒ–ç‰ˆæœ¬ï¼Œçªå‡ºä¸»è¦æ¦‚å¿µã€‚çœŸå®å®ç°åŒ…å«40+ä¸ªå±æ€§ï¼Œæ”¯æŒæ›´å¤šé«˜çº§åŠŸèƒ½ã€‚

```python
class Req:
    """è¯·æ±‚å¯¹è±¡ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    def __init__(self, rid: str, origin_input_text: str, origin_input_ids: List[int],
                 sampling_params: SamplingParams, return_logprob: bool = False,
                 stream: bool = False, lora_id: Optional[str] = None):
        # åŸºæœ¬è¯·æ±‚ä¿¡æ¯
        self.rid = rid                          # è¯·æ±‚å”¯ä¸€æ ‡è¯†ç¬¦
        self.origin_input_text = origin_input_text    # åŸå§‹è¾“å…¥æ–‡æœ¬
        self.origin_input_ids = origin_input_ids      # åŸå§‹è¾“å…¥tokenåºåˆ—
        self.output_ids = []                         # è¾“å‡ºtokenåºåˆ—
        
        # å¤„ç†é…ç½®
        self.sampling_params = sampling_params        # é‡‡æ ·å‚æ•°é…ç½®
        self.return_logprob = return_logprob         # æ˜¯å¦è¿”å›å¯¹æ•°æ¦‚ç‡
        self.stream = stream                        # æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
        self.lora_id = lora_id                      # LoRAé€‚é…å™¨ID
        
        # çŠ¶æ€ç®¡ç†
        self.finished_reason = None                  # å®ŒæˆåŸå› 
        self.req_pool_idx = None                    # å†…å­˜æ± ç´¢å¼•
```

### 2.2 æºç å®ç°ç»†èŠ‚

**çœŸå®Reqç±»çš„å®Œæ•´å‚æ•°**ï¼šç”Ÿäº§ç¯å¢ƒä¸­çš„Reqç±»æ”¯æŒä¸°å¯Œçš„å‚æ•°é…ç½®ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€è¾“å…¥ã€LoRAé€‚é…å™¨ã€ä¼šè¯ç®¡ç†ã€åˆ†ç¦»å¼æ¶æ„ç­‰é«˜çº§åŠŸèƒ½ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹å±•ç¤ºçœŸå®Reqç±»çš„ä¸»è¦å‚æ•°ï¼Œçœç•¥äº†éƒ¨åˆ†å†…éƒ¨å®ç°ç»†èŠ‚ã€‚å®Œæ•´å®ç°è¯·å‚è€ƒ `sglang/srt/managers/schedule_batch.py`ã€‚

```python
class Req:
    """çœŸå®çš„SGLang Reqç±»å®ç°"""
    
    def __init__(
        self,
        rid: str,                              # è¯·æ±‚IDï¼ˆrequest idï¼‰
        origin_input_text: str,               # åŸå§‹è¾“å…¥æ–‡æœ¬
        origin_input_ids: List[int],          # åŸå§‹è¾“å…¥tokenåºåˆ—
        sampling_params: SamplingParams,      # é‡‡æ ·å‚æ•°é…ç½®
        return_logprob: bool = False,         # æ˜¯å¦è¿”å›å¯¹æ•°æ¦‚ç‡
        stream: bool = False,                 # æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
        lora_id: Optional[str] = None,        # LoRAé€‚é…å™¨ID
        session_id: Optional[str] = None,     # ä¼šè¯ID
        # ... è¿˜æœ‰20+ä¸ªå‚æ•°æ”¯æŒå¤šæ¨¡æ€ã€åˆ†ç¦»å¼æ¶æ„ç­‰é«˜çº§åŠŸèƒ½
    ):
        # åŸºç¡€è¯·æ±‚ä¿¡æ¯
        self.rid = rid                        # è¯·æ±‚å”¯ä¸€æ ‡è¯†ç¬¦
        self.origin_input_text = origin_input_text  # åŸå§‹è¾“å…¥æ–‡æœ¬
        self.origin_input_ids = origin_input_ids    # åŸå§‹tokenåºåˆ—
        self.output_ids = []                  # è¾“å‡ºtokenåºåˆ—ï¼ˆoutput token idsï¼‰
        self.fill_ids = []                    # å®Œæ•´tokenåºåˆ—ï¼ˆinput + outputï¼‰
        
        # å¤„ç†é…ç½®
        self.sampling_params = sampling_params      # é‡‡æ ·å‚æ•°
        self.stream = stream                        # æµå¼è¾“å‡ºæ ‡å¿—
        self.lora_id = lora_id                     # LoRAé€‚é…å™¨ID
        self.session_id = session_id               # ä¼šè¯ID
        
        # çŠ¶æ€ç®¡ç†
        self.finished_reason = None           # å®ŒæˆåŸå› ï¼ˆfinish reasonï¼‰
        self.req_pool_idx: Optional[int] = None  # è¯·æ±‚æ± ç´¢å¼•
        self.to_abort = False                 # æ˜¯å¦éœ€è¦ä¸­æ­¢
        
        # å¤šæ¨¡æ€æ”¯æŒ
        self.multimodal_inputs: Optional[MultimodalInputs] = None  # å¤šæ¨¡æ€è¾“å…¥
        self.input_embeds = input_embeds      # è¾“å…¥åµŒå…¥å‘é‡
        
        # å‰ç¼€ç¼“å­˜ä¼˜åŒ–
        self.prefix_indices: torch.Tensor = []      # å‰ç¼€ç¼“å­˜ç´¢å¼•
        self.extend_input_len = 0                   # éœ€è¦é¢„å¡«å……çš„tokenæ•°é‡
        
        # è¿˜æœ‰30+ä¸ªå­—æ®µæ”¯æŒå¢é‡è§£ç ã€åˆ†ç¦»å¼æ¶æ„ã€æ€§èƒ½ä¼˜åŒ–ç­‰åŠŸèƒ½...

ğŸ’¡ **å®ç°è¯´æ˜**: çœŸå®çš„Reqç±»æœ‰50+ä¸ªå­—æ®µï¼Œæ”¯æŒå¤šæ¨¡æ€è¾“å…¥ã€ä¼šè¯ç®¡ç†ã€LoRAé€‚é…å™¨ã€åˆ†ç¦»å¼æ¶æ„ã€å¢é‡è§£ç ã€å‰ç¼€ç¼“å­˜ç­‰é«˜çº§åŠŸèƒ½ã€‚æ•™å­¦ç‰ˆæœ¬çªå‡ºæ ¸å¿ƒçš„"è¾“å…¥â†’å¤„ç†â†’è¾“å‡º"æµç¨‹ã€‚
```

### 2.3 å…³é”®å­—æ®µåˆ†ç±»

**åŸºç¡€ä¿¡æ¯**: `rid`, `origin_input_text`, `origin_input_ids`, `output_ids`
**å¤„ç†é…ç½®**: `sampling_params`, `stream`, `return_logprob`, `lora_id`  
**çŠ¶æ€ç®¡ç†**: `finished_reason`, `to_abort`, `req_pool_idx`
**å¤šæ¨¡æ€**: `multimodal_inputs`, `input_embeds`, `token_type_ids`
**ä¼šè¯æ”¯æŒ**: `session_id`, `bootstrap_host`, `bootstrap_port`
**æ€§èƒ½ä¼˜åŒ–**: `prefix_indices`, `extend_input_len`, `surr_offset`

### 2.4 çŠ¶æ€ç®¡ç†

Reqç±»ç»´æŠ¤ç€è¯·æ±‚åœ¨å¤„ç†è¿‡ç¨‹ä¸­çš„å„ç§çŠ¶æ€ä¿¡æ¯ï¼š

**è¾“å‡ºç®¡ç†**  
output_idsåˆ—è¡¨è®°å½•äº†æ¨¡å‹ç”Ÿæˆçš„æ‰€æœ‰tokenï¼Œfill_idsæ˜¯origin_input_idså’Œoutput_idsçš„ç»„åˆï¼Œè¡¨ç¤ºå½“å‰çš„å®Œæ•´tokenåºåˆ—ã€‚

**ç”Ÿæˆæ§åˆ¶**  
finished_reasonè®°å½•è¯·æ±‚å®Œæˆçš„åŸå› ï¼Œå¯èƒ½æ˜¯è¾¾åˆ°æœ€å¤§é•¿åº¦ã€é‡åˆ°åœæ­¢tokenæˆ–å…¶ä»–æ¡ä»¶ã€‚å„ç§é•¿åº¦é™åˆ¶å’Œæ§åˆ¶å‚æ•°ç¡®ä¿ç”Ÿæˆè¿‡ç¨‹æŒ‰é¢„æœŸè¿›è¡Œã€‚

**å†…å­˜æ˜ å°„**  
req_pool_indiceså’Œå…¶ä»–ç´¢å¼•ä¿¡æ¯ç»´æŠ¤ç€è¯·æ±‚åœ¨å„ç§å†…å­˜æ± ä¸­çš„ä½ç½®ï¼Œè¿™å¯¹äºå†…å­˜ç®¡ç†å’Œç¼“å­˜æœºåˆ¶è‡³å…³é‡è¦ã€‚

### å¤šæ¨¡æ€æ”¯æŒ

Reqç±»è¿˜æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ï¼ŒåŒ…æ‹¬å›¾åƒã€éŸ³é¢‘ç­‰éæ–‡æœ¬æ•°æ®ï¼š

```python
# æ¥è‡ªReqç±»çš„çœŸå®å¤šæ¨¡æ€å­—æ®µ
self.input_embeds = input_embeds                              # è¾“å…¥åµŒå…¥å‘é‡
self.multimodal_inputs: Optional[MultimodalInputs] = None    # å¤šæ¨¡æ€è¾“å…¥ç»Ÿä¸€æ¥å£
```

è¿™ç§è®¾è®¡ä½¿å¾—SGLangèƒ½å¤Ÿå¤„ç†ä¸ä»…ä»…æ˜¯æ–‡æœ¬çš„å¤šç§æ¨¡æ€è¾“å…¥ï¼Œä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æä¾›äº†åŸºç¡€æ”¯æŒã€‚

---

## 3. ScheduleBatchæ•°æ®ç»“æ„

ScheduleBatchæ˜¯è°ƒåº¦å™¨å±‚é¢çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼Œè´Ÿè´£ç®¡ç†ä¸€ä¸ªæ‰¹æ¬¡ä¸­æ‰€æœ‰è¯·æ±‚çš„ä¿¡æ¯å’Œèµ„æºã€‚

### 3.1 æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ

```python
@dataclasses.dataclass
class ScheduleBatch:
    """æ‰¹æ¬¡æ•°æ®ç»“æ„çš„æ ¸å¿ƒæ¦‚å¿µ"""
    # è¯·æ±‚å’Œèµ„æºç®¡ç†
    reqs: List[Req]                          # æ‰¹æ¬¡ä¸­çš„è¯·æ±‚åˆ—è¡¨
    req_to_token_pool: ReqToTokenPool        # è¯·æ±‚åˆ°tokenæ± çš„æ˜ å°„
    token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator  # KVç¼“å­˜åˆ†é…å™¨
    tree_cache: BasePrefixCache              # å‰ç¼€ç¼“å­˜æ ‘
    
    # æ‰¹æ¬¡é…ç½®
    forward_mode: ForwardMode                # å‰å‘æ¨¡å¼ï¼ˆé¢„å¡«å……/è§£ç ï¼‰
    enable_overlap: bool = False             # æ˜¯å¦å¯ç”¨é‡å å¤„ç†
    batch_is_full: bool = False             # æ‰¹æ¬¡æ˜¯å¦å·²æ»¡
    
    # GPUå¼ é‡æ•°æ®
    input_ids: torch.Tensor = None          # è¾“å…¥token IDå¼ é‡
    seq_lens: torch.Tensor = None           # åºåˆ—é•¿åº¦å¼ é‡
    req_pool_indices: torch.Tensor = None   # è¯·æ±‚æ± ç´¢å¼•å¼ é‡
```

### 3.2 æºç å®ç°ç»†èŠ‚

**çœŸå®ScheduleBatchçš„å®Œæ•´ç»“æ„**ï¼šç”Ÿäº§ç¯å¢ƒä¸­çš„ScheduleBatchåŒ…å«äº†æ‰¹æ¬¡ç®¡ç†æ‰€éœ€çš„å…¨éƒ¨ä¿¡æ¯ï¼Œä»åŸºç¡€çš„è¯·æ±‚åˆ—è¡¨åˆ°å¤æ‚çš„GPUå¼ é‡æ•°æ®ï¼Œæ”¯æŒå¤šç§å‰å‘æ¨¡å¼å’Œä¼˜åŒ–ç­–ç•¥ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹å±•ç¤ºçœŸå®ScheduleBatchçš„ä¸»è¦å±æ€§ï¼Œçœç•¥äº†éƒ¨åˆ†å†…éƒ¨æ–¹æ³•å®ç°ã€‚å®Œæ•´å®ç°è¯·å‚è€ƒ `sglang/srt/managers/schedule_batch.py`ã€‚

```python
@dataclasses.dataclass
class ScheduleBatch(ScheduleBatchDisaggregationDecodeMixin):
    """çœŸå®çš„SGLang ScheduleBatchå®ç°"""
    
    # æ ¸å¿ƒç»„ç»‡
    reqs: List[Req]                                    # æ‰¹æ¬¡ä¸­çš„è¯·æ±‚åˆ—è¡¨
    req_to_token_pool: ReqToTokenPool = None          # è¯·æ±‚åˆ°tokenæ± æ˜ å°„
    token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator = None  # KVç¼“å­˜åˆ†é…å™¨
    tree_cache: BasePrefixCache = None                # å‰ç¼€ç¼“å­˜
    
    # æ‰¹æ¬¡æ§åˆ¶
    forward_mode: ForwardMode = None                  # å‰å‘æ¨¡å¼ï¼ˆé¢„å¡«å……/è§£ç ï¼‰
    batch_is_full: bool = False                       # æ‰¹æ¬¡æ»¡æ ‡å¿—
    enable_overlap: bool = False                      # æ˜¯å¦å¯ç”¨é‡å å¤„ç†
    launch_done: Optional[threading.Event] = None    # é‡å äº‹ä»¶å¾ªç¯åŒæ­¥
    
    # GPUå¼ é‡æ•°æ®
    input_ids: torch.Tensor = None                    # è¾“å…¥token IDå¼ é‡ [b]
    seq_lens: torch.Tensor = None                     # åºåˆ—é•¿åº¦å¼ é‡ [b]
    req_pool_indices: torch.Tensor = None             # è¯·æ±‚æ± ç´¢å¼•å¼ é‡ [b]
    out_cache_loc: torch.Tensor = None                # KVç¼“å­˜è¾“å‡ºä½ç½® [b]
    
    # é‡‡æ ·ä¿¡æ¯
    sampling_info: SamplingBatchInfo = None           # å½“å‰æ‰¹æ¬¡é‡‡æ ·ä¿¡æ¯
    next_batch_sampling_info: SamplingBatchInfo = None  # ä¸‹ä¸€æ‰¹æ¬¡é‡‡æ ·ä¿¡æ¯
    
    # å¤šæ¨¡æ€ä¸é«˜çº§åŠŸèƒ½
    multimodal_inputs: Optional[List] = None          # å¤šæ¨¡æ€è¾“å…¥æ•°æ®
    spec_algorithm: SpeculativeAlgorithm = None       # æŠ•æœºè§£ç ç®—æ³•
    has_stream: bool = False                          # æ˜¯å¦æœ‰æµå¼è¯·æ±‚
    has_grammar: bool = False                         # æ˜¯å¦æœ‰è¯­æ³•çº¦æŸ
    
    # ... è¿˜æœ‰40+ä¸ªå­—æ®µæ”¯æŒDPæ³¨æ„åŠ›ã€åˆ†å±‚ç¼“å­˜ã€åˆ†ç¦»å¼æ¶æ„ç­‰é«˜çº§åŠŸèƒ½

ğŸ’¡ **å®ç°è¯´æ˜**: çœŸå®çš„ScheduleBatchæœ‰60+ä¸ªå­—æ®µï¼Œæ”¯æŒå¤šæ¨¡æ€ã€DPæ³¨æ„åŠ›ã€æŠ•æœºè§£ç ã€åˆ†å±‚ç¼“å­˜ã€åˆ†ç¦»å¼æ¶æ„ç­‰å¤æ‚åŠŸèƒ½ã€‚ä¸Šè¿°ä»£ç çªå‡º"è¯·æ±‚â†’èµ„æºâ†’å¼ é‡"çš„æ ¸å¿ƒç»„ç»‡ç»“æ„ã€‚
```

### 3.3 å­—æ®µåŠŸèƒ½åˆ†ç±»

**æ ¸å¿ƒç»„ç»‡**: `reqs`, `req_to_token_pool`, `token_to_kv_pool_allocator`, `tree_cache`
**æ‰¹æ¬¡æ§åˆ¶**: `forward_mode`, `batch_is_full`, `enable_overlap`, `launch_done`
**GPUæ•°æ®**: `input_ids`, `seq_lens`, `req_pool_indices`, `out_cache_loc`
**å¤šæ¨¡æ€**: `multimodal_inputs`, `input_embeds`, `token_type_ids`
**æ€§èƒ½ä¼˜åŒ–**: `spec_algorithm`, `can_run_dp_cuda_graph`, `hicache_consumer_index`
**ç‰¹æ®ŠåŠŸèƒ½**: `has_stream`, `has_grammar`, `is_prefill_only`, `chunked_req`

### 3.4 GPUå¼ é‡æ•°æ®

ScheduleBatchåŒ…å«äº†ä¼ é€’ç»™æ¨¡å‹æ‰§è¡Œå™¨çš„æ‰¹é‡åŒ–å¼ é‡æ•°æ®ï¼š

```python
# æ¥è‡ªScheduleBatchçš„çœŸå®GPUå¼ é‡å­—æ®µ
class ScheduleBatch:
    # Batched arguments to model runner
    input_ids: torch.Tensor = None            # shape: [b], int64 - è¾“å…¥token IDå¼ é‡
    seq_lens: torch.Tensor = None             # shape: [b], int64 - åºåˆ—é•¿åº¦å¼ é‡
    req_pool_indices: torch.Tensor = None     # shape: [b], int64 - è¯·æ±‚æ± ç´¢å¼•å¼ é‡
    out_cache_loc: torch.Tensor = None        # shape: [b], int64 - KVç¼“å­˜è¾“å‡ºä½ç½®
    output_ids: torch.Tensor = None           # shape: [b], int64 - è¾“å‡ºtokenå¼ é‡
    
    # é‡‡æ ·ä¿¡æ¯
    sampling_info: SamplingBatchInfo = None           # å½“å‰æ‰¹æ¬¡é‡‡æ ·ä¿¡æ¯
    next_batch_sampling_info: SamplingBatchInfo = None # ä¸‹ä¸€æ‰¹æ¬¡é‡‡æ ·ä¿¡æ¯
```

è¿™äº›å¼ é‡åŒ–çš„æ•°æ®æ˜¯GPUè®¡ç®—çš„ç›´æ¥è¾“å…¥ï¼Œå°†æ‰¹æ¬¡ä¸­æ‰€æœ‰è¯·æ±‚çš„ç›¸å…³ä¿¡æ¯ç»„ç»‡æˆäº†é€‚åˆå¹¶è¡Œå¤„ç†çš„æ ¼å¼ã€‚

---

## 4. ModelWorkerBatchæ•°æ®ç»“æ„

ModelWorkerBatchæ˜¯ScheduleBatchå‘æ¨¡å‹å·¥ä½œå™¨ä¼ é€’çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œå»é™¤äº†è°ƒåº¦å™¨ç‰¹æœ‰çš„ç®¡ç†ä¿¡æ¯ï¼Œä¸“æ³¨äºæ¨¡å‹æ¨ç†æ‰€éœ€çš„æ ¸å¿ƒæ•°æ®ã€‚

### 4.1 æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ

```python
@dataclasses.dataclass
class ModelWorkerBatch:
    """æ¨¡å‹å·¥ä½œå™¨æ‰¹æ¬¡çš„æ ¸å¿ƒæ¦‚å¿µ"""
    bid: int                               # æ‰¹æ¬¡ID
    forward_mode: ForwardMode              # å‰å‘æ¨¡å¼ï¼ˆé¢„å¡«å……/è§£ç ï¼‰
    input_ids: torch.Tensor               # è¾“å…¥tokenå¼ é‡
    req_pool_indices: torch.Tensor        # è¯·æ±‚æ± ç´¢å¼•
    seq_lens: torch.Tensor                # åºåˆ—é•¿åº¦
    out_cache_loc: torch.Tensor           # è¾“å‡ºç¼“å­˜ä½ç½®
    sampling_info: SamplingBatchInfo      # é‡‡æ ·ä¿¡æ¯
```

### 4.2 æºç å®ç°ç»†èŠ‚

```python
@dataclasses.dataclass
class ModelWorkerBatch:
    """çœŸå®çš„SGLang ModelWorkerBatchå®ç°"""
    
    # Basic batch info
    bid: int                               # æ‰¹æ¬¡ID
    forward_mode: ForwardMode              # å‰å‘æ¨¡å¼
    
    # Core tensors
    input_ids: torch.Tensor               # è¾“å…¥tokenå¼ é‡
    req_pool_indices: torch.Tensor        # è¯·æ±‚æ± ç´¢å¼•
    seq_lens: torch.Tensor                # åºåˆ—é•¿åº¦
    out_cache_loc: torch.Tensor           # KVç¼“å­˜è¾“å‡ºä½ç½®
    seq_lens_cpu: Optional[torch.Tensor]  # CPUä¸Šçš„åºåˆ—é•¿åº¦å¼ é‡
    seq_lens_sum: int                     # åºåˆ—é•¿åº¦æ€»å’Œ

    # For logprob
    return_logprob: bool
    top_logprobs_nums: Optional[List[int]]
    token_ids_logprobs: Optional[List[List[int]]]

    # For DP attention
    global_num_tokens: Optional[List[int]]
    global_num_tokens_for_logprob: Optional[List[int]]
    is_extend_in_batch: bool
    can_run_dp_cuda_graph: bool
    tbo_split_seq_index: Optional[int]
    global_forward_mode: Optional[ForwardMode]

    # For extend mode
    extend_num_tokens: Optional[int]
    extend_seq_lens: Optional[List[int]]
    extend_prefix_lens: Optional[List[int]]
    extend_logprob_start_lens: Optional[List[int]]
    extend_input_logprob_token_ids: Optional[torch.Tensor]

    # For multimodal
    multimodal_inputs: Optional[List[MultimodalInputs]]

    # For encoder-decoder
    encoder_cached: Optional[List[bool]]
    encoder_lens: Optional[torch.Tensor]
    encoder_lens_cpu: Optional[List[int]]
    encoder_out_cache_loc: Optional[torch.Tensor]

    # For LoRA
    lora_ids: Optional[List[str]]

    # Sampling info
    sampling_info: SamplingBatchInfo

    # Additional data
    orig_seq_lens: Optional[torch.Tensor] = None  # Qwen-1Mç›¸å…³
    input_embeds: Optional[torch.Tensor] = None   # è¾“å…¥åµŒå…¥
    token_type_ids: Optional[torch.Tensor] = None # è·¨ç¼–ç å™¨æ¨¡å‹

    # Speculative decoding
    spec_algorithm: SpeculativeAlgorithm = None
    spec_info: Optional[Union[EagleVerifyInput, EagleDraftInput]] = None

ğŸ’¡ **å®ç°è¯´æ˜**: çœŸå®çš„ModelWorkerBatchæœ‰30+ä¸ªå­—æ®µï¼ŒåŒ…å«DPæ³¨æ„åŠ›ã€ç¼–ç å™¨-è§£ç å™¨ã€LoRAã€æŠ•æœºè§£ç ç­‰å¤æ‚åŠŸèƒ½çš„æ”¯æŒã€‚æ•™å­¦ç‰ˆæœ¬çªå‡ºæ ¸å¿ƒçš„"è¾“å…¥â†’æ¨ç†â†’è¾“å‡º"æ•°æ®æµã€‚
```

è¿™ç§ç®€åŒ–ç¡®ä¿äº†æ¨¡å‹å·¥ä½œå™¨åªéœ€è¦å…³æ³¨æ¨ç†ç›¸å…³çš„ä¿¡æ¯ï¼Œæé«˜äº†æ•°æ®ä¼ é€’çš„æ•ˆç‡ã€‚

---

## 5. ForwardBatchæ•°æ®ç»“æ„

ForwardBatchæ˜¯æ•°æ®æµè½¬çš„æœ€åº•å±‚ï¼ŒåŒ…å«GPUæ¨¡å‹æ‰§è¡Œæ—¶çš„æ‰€æœ‰å¼ é‡æ•°æ®ï¼Œæ˜¯å®é™…åœ¨GPUä¸Šæ‰§è¡Œè®¡ç®—çš„æ•°æ®æ ¼å¼ã€‚

### 5.1 æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ

**GPUè®¡ç®—çš„æ•°æ®è½½ä½“**ï¼šForwardBatchå°†ModelWorkerBatchè¿›ä¸€æ­¥è½¬æ¢ä¸ºGPUå‹å¥½çš„å¼ é‡æ ¼å¼ï¼Œä¸“æ³¨äºæ¨¡å‹æ¨ç†çš„æ ¸å¿ƒè®¡ç®—éœ€æ±‚ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹ä¸ºForwardBatchçš„æ ¸å¿ƒå¼ é‡ç®€åŒ–ç‰ˆæœ¬ï¼Œçªå‡ºGPUè®¡ç®—è¦ç´ ã€‚çœŸå®å®ç°åŒ…å«50+ä¸ªå­—æ®µï¼Œæ”¯æŒå„ç§é«˜çº§ä¼˜åŒ–ã€‚

```python
@dataclass
class ForwardBatch:
    """GPUå‰å‘è®¡ç®—æ‰¹æ¬¡ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    # åŸºç¡€ä¿¡æ¯
    forward_mode: ForwardMode           # å‰å‘æ¨¡å¼ï¼ˆé¢„å¡«å……/è§£ç ï¼‰
    batch_size: int                     # æ‰¹æ¬¡å¤§å°
    
    # æ ¸å¿ƒå¼ é‡æ•°æ®
    input_ids: torch.Tensor            # è¾“å…¥token IDå¼ é‡ [batch_size]
    req_pool_indices: torch.Tensor     # è¯·æ±‚æ± ç´¢å¼• [batch_size]
    seq_lens: torch.Tensor             # åºåˆ—é•¿åº¦ [batch_size]
    out_cache_loc: torch.Tensor        # KVç¼“å­˜è¾“å‡ºä½ç½® [batch_size]
    
    # ä½ç½®å’Œé•¿åº¦ä¿¡æ¯
    positions: torch.Tensor            # ä½ç½®ç¼–ç  [total_tokens]
    seq_lens_sum: int                  # æ‰€æœ‰åºåˆ—é•¿åº¦æ€»å’Œ
```

### 5.2 æºç å®ç°ç»†èŠ‚

**çœŸå®ForwardBatchçš„å®Œæ•´ç»“æ„**ï¼šç”Ÿäº§ç¯å¢ƒä¸­çš„ForwardBatchåŒ…å«äº†GPUæ¨¡å‹æ‰§è¡Œæ‰€éœ€çš„å…¨éƒ¨å¼ é‡æ•°æ®ï¼Œæ”¯æŒå¤šæ¨¡æ€ã€æŠ•æœºè§£ç ã€DPæ³¨æ„åŠ›ç­‰é«˜çº§åŠŸèƒ½ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹å±•ç¤ºçœŸå®ForwardBatchçš„ä¸»è¦å¼ é‡ï¼Œçœç•¥äº†éƒ¨åˆ†ä¼˜åŒ–ç›¸å…³å­—æ®µã€‚å®Œæ•´å®ç°è¯·å‚è€ƒ `sglang/srt/model_executor/forward_batch_info.py`ã€‚

```python
@dataclass
class ForwardBatch:
    """çœŸå®çš„SGLang ForwardBatchå®ç°"""
    
    # åŸºç¡€å‰å‘ä¿¡æ¯
    forward_mode: ForwardMode           # å‰å‘æ¨¡å¼
    batch_size: int                     # æ‰¹æ¬¡å¤§å°
    
    # æ ¸å¿ƒè¾“å…¥å¼ é‡
    input_ids: torch.Tensor            # è¾“å…¥token ID [batch_size]
    req_pool_indices: torch.Tensor     # è¯·æ±‚æ± ç´¢å¼• [batch_size]
    seq_lens: torch.Tensor             # åºåˆ—é•¿åº¦ [batch_size]
    out_cache_loc: torch.Tensor        # KVç¼“å­˜è¾“å‡ºä½ç½® [batch_size]
    seq_lens_sum: int                  # åºåˆ—é•¿åº¦æ€»å’Œ
    
    # ä½ç½®å’Œæ³¨æ„åŠ›ä¿¡æ¯
    positions: torch.Tensor = None     # ä½ç½®ç¼–ç å¼ é‡
    seq_lens_cpu: Optional[torch.Tensor] = None  # CPUä¸Šçš„åºåˆ—é•¿åº¦
    
    # é¢„å¡«å……æ¨¡å¼ä¸“ç”¨
    extend_num_tokens: Optional[int] = None      # æ‰©å±•tokenæ•°é‡
    extend_seq_lens: Optional[torch.Tensor] = None  # æ‰©å±•åºåˆ—é•¿åº¦
    extend_start_loc: Optional[torch.Tensor] = None  # æ‰©å±•èµ·å§‹ä½ç½®
    
    # å¤šæ¨¡æ€æ”¯æŒ
    multimodal_inputs: Optional[List] = None     # å¤šæ¨¡æ€è¾“å…¥æ•°æ®
    input_embeds: Optional[torch.Tensor] = None  # è¾“å…¥åµŒå…¥å¼ é‡
    
    # KVç¼“å­˜å’Œæ³¨æ„åŠ›åç«¯
    token_to_kv_pool: KVCache = None            # KVç¼“å­˜æ± å¼•ç”¨
    attn_backend: AttentionBackend = None       # æ³¨æ„åŠ›åç«¯
    
    # DPæ³¨æ„åŠ›ä¼˜åŒ–
    global_num_tokens_gpu: Optional[torch.Tensor] = None  # å…¨å±€tokenæ•°GPUå¼ é‡
    dp_padding_mode: Optional[DpPaddingMode] = None       # DPå¡«å……æ¨¡å¼
    
    # æŠ•æœºè§£ç æ”¯æŒ
    spec_algorithm: SpeculativeAlgorithm = None  # æŠ•æœºè§£ç ç®—æ³•
    spec_info: Optional[Union[EagleVerifyInput, EagleDraftInput]] = None
```

### 5.3 æ•°æ®è½¬æ¢æµç¨‹

**ä»ModelWorkerBatchåˆ°ForwardBatchçš„è½¬æ¢**ï¼š

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹å±•ç¤ºçœŸå®çš„ForwardBatchåˆ›å»ºè¿‡ç¨‹ï¼ŒåŸºäºå®é™…çš„`init_new`ç±»æ–¹æ³•ã€‚å®Œæ•´å®ç°è¯·å‚è€ƒ `sglang/srt/model_executor/forward_batch_info.py`ã€‚

```python
@classmethod
def init_new(cls, batch: ModelWorkerBatch, model_runner: ModelRunner):
    """ä»ModelWorkerBatchåˆ›å»ºForwardBatchçš„çœŸå®æ–¹æ³•"""
    return cls(
        # åŸºç¡€ä¿¡æ¯ä»ModelWorkerBatchç›´æ¥å¤åˆ¶
        forward_mode=batch.forward_mode,           # å‰å‘æ¨¡å¼
        batch_size=len(batch.seq_lens),           # æ‰¹æ¬¡å¤§å°
        input_ids=batch.input_ids,                # è¾“å…¥tokenå¼ é‡
        req_pool_indices=batch.req_pool_indices,  # è¯·æ±‚æ± ç´¢å¼•
        seq_lens=batch.seq_lens,                  # åºåˆ—é•¿åº¦
        out_cache_loc=batch.out_cache_loc,        # è¾“å‡ºç¼“å­˜ä½ç½®
        seq_lens_sum=batch.seq_lens_sum,          # åºåˆ—é•¿åº¦æ€»å’Œ
        
        # å¤šæ¨¡æ€å’Œç¼–ç å™¨æ”¯æŒ
        multimodal_inputs=batch.multimodal_inputs,      # å¤šæ¨¡æ€è¾“å…¥
        encoder_cached=batch.encoder_cached,            # ç¼–ç å™¨ç¼“å­˜çŠ¶æ€
        encoder_lens=batch.encoder_lens,                # ç¼–ç å™¨é•¿åº¦
        
        # é‡‡æ ·å’Œå¯¹æ•°æ¦‚ç‡
        return_logprob=batch.return_logprob,            # æ˜¯å¦è¿”å›å¯¹æ•°æ¦‚ç‡
        top_logprobs_nums=batch.top_logprobs_nums,      # top-kå¯¹æ•°æ¦‚ç‡æ•°é‡
        token_ids_logprobs=batch.token_ids_logprobs,    # tokenå¯¹æ•°æ¦‚ç‡
        
        # LoRAå’ŒæŠ•æœºè§£ç 
        lora_ids=batch.lora_ids,                        # LoRAé€‚é…å™¨IDåˆ—è¡¨
        sampling_info=batch.sampling_info,              # é‡‡æ ·ä¿¡æ¯
        spec_algorithm=batch.spec_algorithm,            # æŠ•æœºè§£ç ç®—æ³•
        
        # ä»model_runnerè·å–èµ„æºå¼•ç”¨
        req_to_token_pool=model_runner.req_to_token_pool,    # è¯·æ±‚åˆ°tokenæ± 
        token_to_kv_pool=model_runner.token_to_kv_pool,      # KVç¼“å­˜æ± 
        attn_backend=model_runner.attn_backend,              # æ³¨æ„åŠ›åç«¯
    )
```

---

## 6. å†…å­˜ç®¡ç†æ•°æ®ç»“æ„

### 6.1 å†…å­˜ç®¡ç†ç»„ä»¶

**ReqToTokenPool**ï¼šç®¡ç†è¯·æ±‚åˆ°tokenä½ç½®çš„æ˜ å°„å…³ç³»ï¼Œç»´æŠ¤æ¯ä¸ªè¯·æ±‚åœ¨å†…å­˜ä¸­çš„tokenä½ç½®ä¿¡æ¯ï¼Œæ”¯æŒåŠ¨æ€çš„å†…å­˜åˆ†é…å’Œå›æ”¶ã€‚

**BaseTokenToKVPoolAllocator**ï¼šKVç¼“å­˜åˆ†é…å™¨çš„æŠ½è±¡åŸºç±»ï¼Œè´Ÿè´£ç®¡ç†KVç¼“å­˜çš„åˆ†é…ã€å›æ”¶å’Œä¼˜åŒ–ã€‚ä¸åŒå®ç°å¯ä»¥é‡‡ç”¨ä¸åŒçš„åˆ†é…ç­–ç•¥ã€‚

**BasePrefixCache**ï¼šå‰ç¼€ç¼“å­˜çš„é€šç”¨æ¥å£ï¼Œå…·ä½“å®ç°å¦‚RadixCacheèƒ½å¤Ÿè¯†åˆ«å’Œå¤ç”¨è¯·æ±‚é—´çš„å…¬å…±å‰ç¼€ï¼Œæ˜¾è‘—å‡å°‘è®¡ç®—å¼€é”€ã€‚

### 6.2 å†…å­˜åˆ†é…åä½œæœºåˆ¶

**çœŸå®çš„å†…å­˜åˆ†é…æ–¹æ³•**ï¼šSGLangé€šè¿‡ScheduleBatchç±»çš„çœŸå®æ–¹æ³•æ¥ç®¡ç†å†…å­˜åˆ†é…å’Œé‡Šæ”¾ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹å±•ç¤ºçœŸå®çš„å†…å­˜åˆ†é…æ–¹æ³•ï¼Œçœç•¥äº†éƒ¨åˆ†é”™è¯¯å¤„ç†é€»è¾‘ã€‚å®Œæ•´å®ç°è¯·å‚è€ƒ `sglang/srt/managers/schedule_batch.py`ã€‚

```python
# æ¥è‡ªScheduleBatchçš„çœŸå®å†…å­˜åˆ†é…æ–¹æ³•
def alloc_req_slots(self, num_reqs: int):
    """åˆ†é…è¯·æ±‚æ§½ä½çš„çœŸå®æ–¹æ³•"""
    req_pool_indices = self.req_to_token_pool.alloc(num_reqs)  # åˆ†é…è¯·æ±‚æ± ç´¢å¼•
    if req_pool_indices is None:
        raise RuntimeError(
            "alloc_req_slots runs out of memory. "
            "Please set a smaller number for `--max-running-requests`. "
            f"{self.req_to_token_pool.available_size()=}, "
            f"{num_reqs=}, "
        )
    return req_pool_indices

def alloc_token_slots(self, num_tokens: int, backup_state: bool = False):
    """åˆ†é…tokenæ§½ä½çš„çœŸå®æ–¹æ³•"""
    # å¦‚æœéœ€è¦ï¼Œå…ˆæ¸…ç†æ ‘ç¼“å­˜
    self._evict_tree_cache_if_needed(num_tokens)
    
    # å¤‡ä»½çŠ¶æ€ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if backup_state:
        state = self.token_to_kv_pool_allocator.backup_state()
    
    # åˆ†é…KVç¼“å­˜ç©ºé—´
    out_cache_loc = self.token_to_kv_pool_allocator.alloc(num_tokens)
    if out_cache_loc is None:
        phase_str = "Prefill" if self.forward_mode.is_extend() else "Decode"
        error_msg = (
            f"{phase_str} out of memory. Try to lower your batch size.\n"
            f"Try to allocate {num_tokens} tokens.\n"
        )
        logger.error(error_msg)
        raise RuntimeError(error_msg)
    
    if backup_state:
        return out_cache_loc, state
    else:
        return out_cache_loc

# æ¥è‡ªPrefillAdderçš„çœŸå®è¯·æ±‚æ·»åŠ é€»è¾‘
def add_one_req(self, req: Req, has_chunked_req: bool):
    """PrefillAdderæ·»åŠ è¯·æ±‚çš„çœŸå®æ–¹æ³•ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    # åˆå§‹åŒ–è¯·æ±‚çš„ä¸‹ä¸€è½®è¾“å…¥
    req.init_next_round_input(self.tree_cache)
    
    # è®¡ç®—å‰ç¼€é•¿åº¦å’Œè¾“å…¥tokenæ•°
    prefix_len = len(req.prefix_indices)
    input_tokens = req.extend_input_len
    
    # æ£€æŸ¥æ˜¯å¦å¯ä»¥æ·»åŠ è¯·æ±‚
    if self.rem_chunk_tokens is None or input_tokens <= self.rem_chunk_tokens:
        # éåˆ†å—é¢„å¡«å……
        self.can_run_list.append(req)  # æ·»åŠ åˆ°å¯è¿è¡Œåˆ—è¡¨
        if self.is_hybrid:  # SWAæ··åˆç¼“å­˜
            swa_uuid_for_lock = self.tree_cache.inc_lock_ref(req.last_node)
            req.swa_uuid_for_lock = swa_uuid_for_lock
        else:
            self.tree_cache.inc_lock_ref(req.last_node)  # å¢åŠ é”å¼•ç”¨
        
        # æ›´æ–°é¢„å¡«å……é¢„ç®—
        self._update_prefill_budget(
            prefix_len, 
            input_tokens,
            min(req.sampling_params.max_new_tokens, CLIP_MAX_NEW_TOKENS)
        )
    else:
        # åˆ†å—é¢„å¡«å……å¤„ç†
        trunc_len = self.rem_chunk_tokens - self.page_size + 1
        req.extend_input_len = trunc_len
        req.fill_ids = req.fill_ids[:len(req.prefix_indices) + trunc_len]
        
        self.can_run_list.append(req)
        self.new_chunked_req = req  # è®¾ç½®ä¸ºæ–°çš„åˆ†å—è¯·æ±‚
    
    return self.budget_state()  # è¿”å›é¢„ç®—çŠ¶æ€
```

## 7. å¤šæ¨¡æ€æ•°æ®ç»“æ„

SGLangæ”¯æŒå¤šæ¨¡æ€è¾“å…¥ï¼Œé€šè¿‡ä¸“é—¨çš„æ•°æ®ç»“æ„æ¥ç»Ÿä¸€ç®¡ç†å›¾åƒã€è§†é¢‘ã€éŸ³é¢‘ç­‰ä¸åŒæ¨¡æ€çš„æ•°æ®ã€‚

### 7.1 MultimodalInputs

**å¤šæ¨¡æ€è¾“å…¥çš„ç»Ÿä¸€ç®¡ç†**ï¼šMultimodalInputsç±»æ˜¯å¤šæ¨¡æ€æ•°æ®çš„é¡¶å±‚å®¹å™¨ï¼Œç»Ÿä¸€ç®¡ç†å„ç§æ¨¡æ€çš„è¾“å…¥æ•°æ®å’Œç›¸å…³é…ç½®ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹å±•ç¤ºçœŸå®MultimodalInputsçš„æ ¸å¿ƒå­—æ®µï¼Œçœç•¥äº†éƒ¨åˆ†æ¨¡å‹ç‰¹å®šé…ç½®ã€‚å®Œæ•´å®ç°è¯·å‚è€ƒ `sglang/srt/managers/schedule_batch.py`ã€‚

```python
@dataclasses.dataclass
class MultimodalInputs:
    """çœŸå®çš„SGLangå¤šæ¨¡æ€è¾“å…¥æ•°æ®ç»“æ„"""
    
    # æ ¸å¿ƒæ•°æ®é¡¹
    mm_items: List[MultimodalDataItem]        # å¤šæ¨¡æ€æ•°æ®é¡¹åˆ—è¡¨
    image_pad_len: Optional[list] = None      # å›¾åƒå¡«å……é•¿åº¦
    num_image_tokens: Optional[int] = None    # å›¾åƒtokenæ•°é‡
    
    # å›¾åƒç›¸å…³token ID
    im_token_id: Optional[int] = None         # å›¾åƒtoken ID
    im_start_id: Optional[int] = None         # å›¾åƒå¼€å§‹token ID
    im_end_id: Optional[int] = None           # å›¾åƒç»“æŸtoken ID
    slice_start_id: Optional[int] = None      # åˆ‡ç‰‡å¼€å§‹token ID
    slice_end_id: Optional[int] = None        # åˆ‡ç‰‡ç»“æŸtoken ID
    
    # è§†é¢‘ç›¸å…³token ID
    video_token_id: Optional[int] = None      # è§†é¢‘token ID
    
    # éŸ³é¢‘ç›¸å…³token ID
    audio_token_id: Optional[int] = None      # éŸ³é¢‘token ID
    audio_start_id: Optional[int] = None      # éŸ³é¢‘å¼€å§‹token ID
    audio_end_id: Optional[int] = None        # éŸ³é¢‘ç»“æŸtoken ID
    
    # QWen2-VLç›¸å…³ä½ç½®ç¼–ç 
    mrope_positions: Optional[torch.Tensor] = None        # å¤šç»´ä½ç½®ç¼–ç 
    mrope_position_delta: Optional[torch.Tensor] = None   # ä½ç½®ç¼–ç å¢é‡
    
    def contains_image_inputs(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾åƒè¾“å…¥"""
        return any(item.is_image() for item in self.mm_items)
    
    def contains_video_inputs(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«è§†é¢‘è¾“å…¥"""
        return any(item.is_video() for item in self.mm_items)
    
    def contains_audio_inputs(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«éŸ³é¢‘è¾“å…¥"""
        return any(item.is_audio() for item in self.mm_items)
```

### 7.2 MultimodalDataItem

**å•ä¸ªå¤šæ¨¡æ€æ•°æ®é¡¹çš„ç»“æ„**ï¼šMultimodalDataItemè¡¨ç¤ºå•ä¸ªæ¨¡æ€çš„æ•°æ®ï¼ŒåŒ…å«åŸå§‹ç‰¹å¾ã€é¢„è®¡ç®—åµŒå…¥å’Œæ¨¡å‹ç‰¹å®šæ•°æ®ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹å±•ç¤ºçœŸå®MultimodalDataItemçš„ä¸»è¦å­—æ®µï¼Œçœç•¥äº†éƒ¨åˆ†è¾…åŠ©æ–¹æ³•ã€‚å®Œæ•´å®ç°è¯·å‚è€ƒ `sglang/srt/managers/schedule_batch.py`ã€‚

```python
@dataclasses.dataclass
class MultimodalDataItem:
    """çœŸå®çš„SGLangå¤šæ¨¡æ€æ•°æ®é¡¹ç»“æ„"""
    
    modality: Modality                        # æ¨¡æ€ç±»å‹ï¼ˆå›¾åƒ/è§†é¢‘/éŸ³é¢‘ï¼‰
    hash: int = None                          # æ•°æ®å“ˆå¸Œå€¼
    pad_value: int = None                     # å¡«å……å€¼
    offsets: Optional[list] = None            # åç§»é‡åˆ—è¡¨
    
    # åŸå§‹ç‰¹å¾æ•°æ®ï¼ˆäºŒé€‰ä¸€ï¼‰
    feature: Union[torch.Tensor, np.ndarray] = None              # åŸå§‹ç‰¹å¾ï¼ˆå¦‚pixel_valuesï¼‰
    precomputed_embeddings: Optional[Union[torch.Tensor, np.ndarray]] = None  # é¢„è®¡ç®—åµŒå…¥
    
    # æ¨¡å‹ç‰¹å®šæ•°æ®
    model_specific_data: dict[str, Any] = dataclasses.field(default_factory=dict)
    
    def __getattr__(self, name: str):
        """åŠ¨æ€è®¿é—®æ¨¡å‹ç‰¹å®šæ•°æ®"""
        if (
            "model_specific_data" in self.__dict__
            and name in self.__dict__["model_specific_data"]
        ):
            return self.__dict__["model_specific_data"][name]
        else:
            raise AttributeError(f"'{self.__class__.__name__}' object has no attribute '{name}'")
    
    def set(self, key: str, value: Any):
        """è®¾ç½®æ¨¡å‹ç‰¹å®šæ•°æ®"""
        if key in self.__dict__:
            self.__dict__[key] = value
        else:
            self.model_specific_data[key] = value
```

## 8. æ‰¹é‡è¯·æ±‚æ•°æ®ç»“æ„

SGLangæ”¯æŒæ‰¹é‡è¯·æ±‚ä»¥æé«˜ç½‘ç»œä¼ è¾“æ•ˆç‡å’Œå¤„ç†æ€§èƒ½ï¼š

### 8.1 BatchTokenizedGenerateReqInput

**æ‰¹é‡ç”Ÿæˆè¯·æ±‚çš„çœŸå®ç»“æ„**ï¼šBatchTokenizedGenerateReqInputåŒ…å«å¤šä¸ªå·²tokenizeçš„ç”Ÿæˆè¯·æ±‚ï¼Œæ”¯æŒæ‰¹é‡å¤„ç†ä»¥æé«˜æ•ˆç‡ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹å±•ç¤ºçœŸå®BatchTokenizedGenerateReqInputçš„å®Œæ•´å®šä¹‰ã€‚å®Œæ•´å®ç°è¯·å‚è€ƒ `sglang/srt/managers/io_struct.py`ã€‚

```python
@dataclass
class BatchTokenizedGenerateReqInput:
    """çœŸå®çš„SGLangæ‰¹é‡ç”Ÿæˆè¯·æ±‚è¾“å…¥"""
    # The batch of tokenized requests
    batch: List[TokenizedGenerateReqInput]  # æ‰¹é‡å·²tokenizeçš„è¯·æ±‚åˆ—è¡¨
    
    def __len__(self):
        """è¿”å›æ‰¹æ¬¡å¤§å°"""
        return len(self.batch)
    
    def __getitem__(self, i):
        """æ”¯æŒç´¢å¼•è®¿é—®"""
        return self.batch[i]
    
    def __iter__(self):
        """æ”¯æŒè¿­ä»£è®¿é—®"""
        return iter(self.batch)
```

### 8.2 BatchTokenizedEmbeddingReqInput

æ‰¹é‡åµŒå…¥è¯·æ±‚çš„è¾“å…¥ç»“æ„ï¼ŒåŒ…å«å¤šä¸ªTokenizedEmbeddingReqInputï¼š

```python
@dataclass  
class BatchTokenizedEmbeddingReqInput:
    """æ‰¹é‡åµŒå…¥è¯·æ±‚è¾“å…¥"""
    requests: List[TokenizedEmbeddingReqInput]  # æ‰¹é‡è¯·æ±‚åˆ—è¡¨
    
    def __len__(self):
        return len(self.requests)
    
    def __iter__(self):
        return iter(self.requests)
```

è¿™äº›æ‰¹é‡ç»“æ„é€šè¿‡å‡å°‘ç½‘ç»œå¾€è¿”æ¬¡æ•°å’Œè¯·æ±‚å¤„ç†å¼€é”€æ¥æé«˜ç³»ç»Ÿæ•´ä½“æ€§èƒ½ã€‚

## 9. åˆ†ç¦»å¼æ¶æ„æ•°æ®ç»“æ„

åœ¨åˆ†ç¦»å¼æ¶æ„ä¸­ï¼Œè¯·æ±‚å¯¹è±¡åŒ…å«é¢å¤–çš„åˆ†ç¦»å¼ç›¸å…³å­—æ®µï¼š

```python
class Req:
    # åˆ†ç¦»å¼æ¶æ„ç›¸å…³å­—æ®µ
    disagg_kv_sender: Optional[BaseKVSender] = None      # KVå‘é€å™¨
    disagg_kv_receiver: Optional[BaseKVReceiver] = None  # KVæ¥æ”¶å™¨
    bootstrap_host: Optional[str] = None                 # å¯åŠ¨ä¸»æœº
    bootstrap_port: Optional[int] = None                 # å¯åŠ¨ç«¯å£
    bootstrap_room: Optional[str] = None                 # å¯åŠ¨æˆ¿é—´ID
    
    # SWAæ··åˆç¼“å­˜ç›¸å…³å­—æ®µ
    swa_uuid_for_lock: Optional[str] = None             # SWAé”å®šUUID
```

## 10. æ•°æ®ç»“æ„åä½œå…³ç³»

è¿™äº›æ•°æ®ç»“æ„ä¹‹é—´å­˜åœ¨ç€å¤æ‚çš„åä½œå…³ç³»ã€‚Reqå¯¹è±¡æ˜¯æœ€åŸºæœ¬çš„æ•°æ®å•å…ƒï¼ŒScheduleBatchå°†å¤šä¸ªReqç»„ç»‡æˆæ‰¹æ¬¡ï¼ŒåŒæ—¶ç®¡ç†ç€å„ç§èµ„æºæ± çš„å¼•ç”¨ã€‚åœ¨å¤„ç†è¿‡ç¨‹ä¸­ï¼ŒScheduleBatchä¼šè½¬æ¢æˆModelWorkerBatchä¼ é€’ç»™ä¸‹æ¸¸ç»„ä»¶ï¼Œæœ€ç»ˆè½¬æ¢æˆForwardBatchåœ¨GPUä¸Šæ‰§è¡Œã€‚

å†…å­˜ç®¡ç†ç›¸å…³çš„æ•°æ®ç»“æ„ä¸ºè¿™ä¸ªè¿‡ç¨‹æä¾›æ”¯æ’‘ï¼Œç¡®ä¿æ¯ä¸ªè¯·æ±‚éƒ½èƒ½è·å¾—å¿…è¦çš„å†…å­˜èµ„æºï¼ŒåŒæ—¶é€šè¿‡ç¼“å­˜æœºåˆ¶ä¼˜åŒ–æ€§èƒ½ã€‚å¤šæ¨¡æ€æ•°æ®ç»“æ„åˆ™æ‰©å±•äº†ç³»ç»Ÿçš„è¾“å…¥èƒ½åŠ›ï¼Œä½¿å¾—SGLangèƒ½å¤Ÿå¤„ç†ä¸ä»…ä»…æ˜¯æ–‡æœ¬çš„å¤šç§ç±»å‹è¾“å…¥ã€‚

---

## 11. æ ¸å¿ƒè®¾è®¡åŸåˆ™

SGLangçš„æ•°æ®ç»“æ„è®¾è®¡ä½“ç°äº†å‡ ä¸ªé‡è¦çš„è®¾è®¡åŸåˆ™ï¼š

**åˆ†å±‚æŠ½è±¡**: é€šè¿‡Reqâ†’ScheduleBatchâ†’ModelWorkerBatchçš„åˆ†å±‚è®¾è®¡ï¼Œç³»ç»Ÿèƒ½å¤Ÿåœ¨ä¸åŒæŠ½è±¡å±‚é¢è¿›è¡Œä¼˜åŒ–ï¼Œè°ƒåº¦å™¨å…³æ³¨é«˜å±‚å†³ç­–ï¼Œæ¨¡å‹æ‰§è¡Œå™¨å…³æ³¨åº•å±‚è®¡ç®—ã€‚

**æ¨¡å—åŒ–è®¾è®¡**: å„ä¸ªæ•°æ®ç»“æ„èŒè´£æ¸…æ™°ï¼Œç›¸äº’ä¹‹é—´é€šè¿‡æ˜ç¡®çš„æ¥å£è¿›è¡Œäº¤äº’ï¼Œæé«˜äº†ç³»ç»Ÿçš„å¯ç»´æŠ¤æ€§å’Œå¯æµ‹è¯•æ€§ã€‚

**æ€§èƒ½ä¼˜åŒ–**: æ•°æ®ç»“æ„å……åˆ†è€ƒè™‘äº†æ€§èƒ½å› ç´ ï¼š
- æ‰¹é‡åŒ–å¤„ç†å‡å°‘å‡½æ•°è°ƒç”¨å¼€é”€
- å¼ é‡åŒ–æ•°æ®æ”¯æŒGPUå¹¶è¡Œè®¡ç®—
- å†…å­˜æ± è®¾è®¡æé«˜å†…å­˜å±€éƒ¨æ€§
- ç¼“å­˜å‹å¥½çš„æ•°æ®å¸ƒå±€

**æ‰©å±•æ€§**: é€šè¿‡æŠ½è±¡åŸºç±»å’Œmixinæ¨¡å¼ï¼Œæ•°æ®ç»“æ„å…·å¤‡è‰¯å¥½çš„æ‰©å±•æ€§ï¼Œèƒ½å¤Ÿæ”¯æŒæ–°åŠŸèƒ½çš„æ·»åŠ å’Œç°æœ‰åŠŸèƒ½çš„ä¼˜åŒ–ã€‚

### 11.1 å®ç°ç‰¹è‰²

**æºç å‡†ç¡®æ€§**: æœ¬æ–‡æ¡£åŸºäºçœŸå®SGLangæºç ç¼–å†™ï¼Œæ‰€æœ‰æ•°æ®ç»“æ„å®šä¹‰éƒ½æ¥è‡ªå®é™…å®ç°ï¼Œç¡®ä¿æŠ€æœ¯å‡†ç¡®æ€§ã€‚

**æ•™å­¦ä¸å®è·µå¹¶é‡**: é‡‡ç”¨"æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ + æºç å®ç°ç»†èŠ‚"çš„åŒé‡ç»“æ„ï¼Œæ—¢ä¾¿äºç†è§£è®¾è®¡æ€æƒ³ï¼Œåˆæä¾›å…·ä½“å®ç°å‚è€ƒã€‚

**å¤æ‚æ€§é€æ˜**: æ˜ç¡®å±•ç¤ºäº†æ•™å­¦ç®€åŒ–ç‰ˆæœ¬ä¸çœŸå®æºç çš„å·®å¼‚ï¼Œè®©å¼€å‘è€…äº†è§£å®é™…ç³»ç»Ÿçš„å¤æ‚æ€§ã€‚

### 11.2 æ¼”è¿›è¶‹åŠ¿

SGLangçš„æ•°æ®ç»“æ„å±•ç°äº†ç°ä»£æ¨ç†ç³»ç»Ÿçš„æ¼”è¿›æ–¹å‘ï¼š
- **å¤šæ¨¡æ€æ”¯æŒ**: ä»çº¯æ–‡æœ¬æ‰©å±•åˆ°å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘
- **åˆ†ç¦»å¼æ¶æ„**: æ”¯æŒé¢„å¡«å……/è§£ç åˆ†ç¦»çš„å¤§è§„æ¨¡éƒ¨ç½²
- **é«˜çº§ä¼˜åŒ–**: æŠ•æœºè§£ç ã€æ··åˆç¼“å­˜ã€DPæ³¨æ„åŠ›ç­‰å‰æ²¿æŠ€æœ¯
- **äº§ä¸šåŒ–éœ€æ±‚**: ä¼šè¯ç®¡ç†ã€LoRAé€‚é…å™¨ã€ç›‘æ§è°ƒè¯•ç­‰å·¥ç¨‹ç‰¹æ€§

ç†è§£è¿™äº›æ ¸å¿ƒæ•°æ®ç»“æ„åŠå…¶ç›¸äº’å…³ç³»ï¼Œæ˜¯æ·±å…¥æŒæ¡SGLangè°ƒåº¦å™¨å·¥ä½œæœºåˆ¶çš„å…³é”®ã€‚è¿™äº›æ•°æ®ç»“æ„ä¸ä»…æ‰¿è½½ç€ç³»ç»Ÿçš„æ ¸å¿ƒä¿¡æ¯ï¼Œè¿˜ä½“ç°äº†SGLangåœ¨æ€§èƒ½ã€å¯ç»´æŠ¤æ€§å’Œæ‰©å±•æ€§æ–¹é¢çš„è®¾è®¡è€ƒé‡ã€‚

**æ‰¿ä¸Šå¯ä¸‹**ï¼šåœ¨ç¬¬ä¸€ç« æˆ‘ä»¬äº†è§£äº†è°ƒåº¦å™¨çš„æ•´ä½“æ¶æ„å’Œè®¾è®¡ç†å¿µï¼Œæœ¬ç« æ·±å…¥å‰–æäº†æ”¯æ’‘è¿™äº›æ¶æ„çš„æ ¸å¿ƒæ•°æ®æŠ½è±¡ã€‚æœ‰äº†è¿™äº›åŸºç¡€ï¼Œæˆ‘ä»¬å°±ä¸ºæ·±å…¥æ¢è®¨è°ƒåº¦å™¨åœ¨å®é™…è¿è¡Œä¸­çš„è¯·æ±‚å¤„ç†æœºåˆ¶ã€æ‰¹æ¬¡è°ƒåº¦ç­–ç•¥å’Œå†…å­˜ç®¡ç†ç®—æ³•å¥ å®šäº†åšå®åŸºç¡€ã€‚æ¥ä¸‹æ¥çš„ç« èŠ‚å°†å±•ç¤ºè¿™äº›æ•°æ®ç»“æ„æ˜¯å¦‚ä½•åœ¨å…·ä½“çš„è°ƒåº¦æµç¨‹ä¸­å‘æŒ¥ä½œç”¨çš„ã€‚
