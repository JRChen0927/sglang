# æ ¸å¿ƒæ•°æ®ç»“æ„

---

SGLangè°ƒåº¦å™¨çš„é«˜æ•ˆè¿è¡Œä¾èµ–äºä¸€ç³»åˆ—ç²¾å¿ƒè®¾è®¡çš„æ•°æ®ç»“æ„ã€‚è¿™äº›æ•°æ®ç»“æ„ä¸ä»…æ‰¿è½½ç€è¯·æ±‚çš„å„ç§ä¿¡æ¯ï¼Œè¿˜è´Ÿè´£æ‰¹æ¬¡ç®¡ç†ã€å†…å­˜åˆ†é…å’Œæ¨¡å‹æ¨ç†çš„åè°ƒã€‚ç†è§£è¿™äº›æ ¸å¿ƒæ•°æ®ç»“æ„æ˜¯æ·±å…¥æŒæ¡SGLangè°ƒåº¦å™¨å·¥ä½œåŸç†çš„åŸºç¡€ã€‚

---

## ğŸ”„ æ•°æ®æµè½¬æ¶æ„

SGLangé‡‡ç”¨åˆ†å±‚çš„æ•°æ®å¤„ç†æ¶æ„ï¼Œè¯·æ±‚ä»æ¥æ”¶åˆ°æ‰§è¡Œç»å†äº†ä¸‰ä¸ªä¸»è¦çš„æ•°æ®ç»“æ„å±‚æ¬¡ï¼š

**è°ƒåº¦å™¨å±‚é¢çš„ScheduleBatch**è´Ÿè´£å­˜å‚¨è°ƒåº¦å™¨éœ€è¦çš„æ‰€æœ‰ä¿¡æ¯ï¼ŒåŒ…æ‹¬è¯·æ±‚åˆ—è¡¨ã€å†…å­˜æ± å¼•ç”¨ã€ç¼“å­˜ç®¡ç†ç­‰é«˜å±‚è°ƒåº¦å†³ç­–æ‰€éœ€çš„æ•°æ®ã€‚

**æ¨¡å‹å·¥ä½œå™¨å±‚é¢çš„ModelWorkerBatch**æ˜¯ScheduleBatchçš„ç®€åŒ–ç‰ˆæœ¬ï¼ŒåªåŒ…å«æ¨¡å‹å‰å‘æ¨ç†æ‰€éœ€çš„æ ¸å¿ƒæ•°æ®ï¼Œå»é™¤äº†è°ƒåº¦å™¨ç‰¹æœ‰çš„ç®¡ç†ä¿¡æ¯ã€‚

**æ¨¡å‹æ‰§è¡Œå™¨å±‚é¢çš„ForwardBatch**åŒ…å«æœ€åº•å±‚çš„GPUå¼ é‡æ•°æ®ï¼Œæ˜¯å®é™…åœ¨GPUä¸Šæ‰§è¡Œè®¡ç®—æ—¶ä½¿ç”¨çš„æ•°æ®æ ¼å¼ã€‚

è¿™ç§åˆ†å±‚è®¾è®¡ç¡®ä¿äº†æ¯ä¸ªç»„ä»¶åªå¤„ç†ä¸å…¶èŒè´£ç›¸å…³çš„æ•°æ®ï¼Œæé«˜äº†ç³»ç»Ÿçš„æ¨¡å—åŒ–ç¨‹åº¦å’Œæ‰§è¡Œæ•ˆç‡ã€‚

---

## ğŸ“‹ Reqç±»è¯¦è§£

Reqç±»æ˜¯SGLangä¸­è¡¨ç¤ºå•ä¸ªè¯·æ±‚çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼ŒåŒ…å«äº†è¯·æ±‚ä»åˆ›å»ºåˆ°å®Œæˆçš„å…¨éƒ¨ä¿¡æ¯ã€‚

### ğŸ¯ æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ

**Reqç±»çš„è®¾è®¡ç†å¿µ**ï¼šReqç±»æ˜¯SGLangä¸­è¡¨ç¤ºå•ä¸ªè¯·æ±‚çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼ŒåŒ…å«äº†ä»è¾“å…¥åˆ°è¾“å‡ºçš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸä¿¡æ¯ã€‚è®¾è®¡ä¸Šé‡‡ç”¨äº†ä¸°å¯Œçš„å‚æ•°æ”¯æŒï¼Œèƒ½å¤Ÿå¤„ç†æ–‡æœ¬ç”Ÿæˆã€åµŒå…¥è®¡ç®—ã€å¤šæ¨¡æ€è¾“å…¥ç­‰å¤šç§åœºæ™¯ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹ä¸ºReqç±»çš„æ ¸å¿ƒå±æ€§ç®€åŒ–ç‰ˆæœ¬ï¼Œçªå‡ºä¸»è¦æ¦‚å¿µã€‚çœŸå®å®ç°åŒ…å«40+ä¸ªå±æ€§ï¼Œæ”¯æŒæ›´å¤šé«˜çº§åŠŸèƒ½ã€‚

```python
class Req:
    """è¯·æ±‚å¯¹è±¡ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    def __init__(self, rid: str, origin_input_text: str, origin_input_ids: List[int],
                 sampling_params: SamplingParams, return_logprob: bool = False,
                 stream: bool = False, lora_id: Optional[str] = None):
        # åŸºæœ¬è¯·æ±‚ä¿¡æ¯
        self.rid = rid                          # è¯·æ±‚å”¯ä¸€æ ‡è¯†ç¬¦
        self.origin_input_text = origin_input_text    # åŸå§‹è¾“å…¥æ–‡æœ¬
        self.origin_input_ids = origin_input_ids      # åŸå§‹è¾“å…¥tokenåºåˆ—
        self.output_ids = []                         # è¾“å‡ºtokenåºåˆ—
        
        # å¤„ç†é…ç½®
        self.sampling_params = sampling_params        # é‡‡æ ·å‚æ•°é…ç½®
        self.return_logprob = return_logprob         # æ˜¯å¦è¿”å›å¯¹æ•°æ¦‚ç‡
        self.stream = stream                        # æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
        self.lora_id = lora_id                      # LoRAé€‚é…å™¨ID
        
        # çŠ¶æ€ç®¡ç†
        self.finished_reason = None                  # å®ŒæˆåŸå› 
        self.req_pool_idx = None                    # å†…å­˜æ± ç´¢å¼•
```

### ğŸ” æºç å®ç°ç»†èŠ‚

**çœŸå®Reqç±»çš„å®Œæ•´å‚æ•°**ï¼šç”Ÿäº§ç¯å¢ƒä¸­çš„Reqç±»æ”¯æŒä¸°å¯Œçš„å‚æ•°é…ç½®ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€è¾“å…¥ã€LoRAé€‚é…å™¨ã€ä¼šè¯ç®¡ç†ã€åˆ†ç¦»å¼æ¶æ„ç­‰é«˜çº§åŠŸèƒ½ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹å±•ç¤ºçœŸå®Reqç±»çš„ä¸»è¦å‚æ•°ï¼Œçœç•¥äº†éƒ¨åˆ†å†…éƒ¨å®ç°ç»†èŠ‚ã€‚å®Œæ•´å®ç°è¯·å‚è€ƒ `sglang/srt/managers/schedule_batch.py`ã€‚

```python
class Req:
    """çœŸå®çš„SGLang Reqç±»å®ç°"""
    
    def __init__(
        self,
        rid: str,
        origin_input_text: str,
        origin_input_ids: List[int],
        sampling_params: SamplingParams,
        return_logprob: bool = False,
        top_logprobs_num: int = 0,
        token_ids_logprob: List[int] = None,
        stream: bool = False,
        origin_input_ids_unpadded: Optional[Tuple[int]] = None,
        lora_id: Optional[str] = None,
        input_embeds: Optional[List[List[float]]] = None,
        token_type_ids: List[int] = None,
        session_id: Optional[str] = None,
        custom_logit_processor: Optional[str] = None,
        return_hidden_states: bool = False,
        eos_token_ids: Optional[Set[int]] = None,
        bootstrap_host: Optional[str] = None,
        bootstrap_port: Optional[int] = None,
        bootstrap_room: Optional[int] = None,
        data_parallel_rank: Optional[int] = None,
        vocab_size: Optional[int] = None,
    ):
        # Input and output info
        self.rid = rid
        self.origin_input_text = origin_input_text
        self.origin_input_ids_unpadded = (
            origin_input_ids_unpadded or origin_input_ids  # Before image padding
        )
        self.origin_input_ids = origin_input_ids
        self.output_ids = []                    # Each decode stage's output ids
        self.fill_ids = []                      # origin_input_ids + output_ids
        self.session_id = session_id
        self.input_embeds = input_embeds
        
        # For cross-encoder model
        self.token_type_ids = token_type_ids
        
        # The length of KV that have been removed in local attention chunked prefill
        self.evicted_seqlen_local = 0
        
        # Sampling info
        if isinstance(sampling_params.custom_params, dict):
            sampling_params = copy.copy(sampling_params)
            sampling_params.custom_params = sampling_params.custom_params | {
                "__req__": self
            }
        self.sampling_params = sampling_params
        self.custom_logit_processor = custom_logit_processor
        self.return_hidden_states = return_hidden_states
        self.lora_id = lora_id
        
        # Memory pool info
        self.req_pool_idx: Optional[int] = None
        
        # Check finish
        self.tokenizer = None
        self.finished_reason = None
        self.finished_output = None
        self.to_abort = False
        self.to_abort_message: str = None
        self.stream = stream
        self.eos_token_ids = eos_token_ids
        self.vocab_size = vocab_size
        
        # For incremental decoding
        self.surr_offset = None  # Surrounding offset to defeat cleanup algorithm
        self.read_offset = None
        self.decoded_text = ""
        
        # For multimodal inputs
        self.multimodal_inputs: Optional[MultimodalInputs] = None
        
        # Prefix info
        self.prefix_indices: torch.Tensor = []     # Indices to kv cache for shared prefix
        self.extend_input_len = 0                  # Number of tokens to run prefill
        self.extend_logprob_start_len = 0          # Relative logprob_start_len in extend batch
        
        # è¿˜æœ‰æ›´å¤šå­—æ®µ...

ğŸ’¡ **å®ç°è¯´æ˜**: çœŸå®çš„Reqç±»æœ‰50+ä¸ªå­—æ®µï¼Œæ”¯æŒå¤šæ¨¡æ€è¾“å…¥ã€ä¼šè¯ç®¡ç†ã€LoRAé€‚é…å™¨ã€åˆ†ç¦»å¼æ¶æ„ã€å¢é‡è§£ç ã€å‰ç¼€ç¼“å­˜ç­‰é«˜çº§åŠŸèƒ½ã€‚æ•™å­¦ç‰ˆæœ¬çªå‡ºæ ¸å¿ƒçš„"è¾“å…¥â†’å¤„ç†â†’è¾“å‡º"æµç¨‹ã€‚
```

### ğŸ·ï¸ å…³é”®å­—æ®µåˆ†ç±»

**åŸºç¡€ä¿¡æ¯**: `rid`, `origin_input_text`, `origin_input_ids`, `output_ids`
**å¤„ç†é…ç½®**: `sampling_params`, `stream`, `return_logprob`, `lora_id`  
**çŠ¶æ€ç®¡ç†**: `finished_reason`, `to_abort`, `req_pool_idx`
**å¤šæ¨¡æ€**: `multimodal_inputs`, `input_embeds`, `token_type_ids`
**ä¼šè¯æ”¯æŒ**: `session_id`, `bootstrap_host`, `bootstrap_port`
**æ€§èƒ½ä¼˜åŒ–**: `prefix_indices`, `extend_input_len`, `surr_offset`

### ğŸ”„ çŠ¶æ€ç®¡ç†

Reqç±»ç»´æŠ¤ç€è¯·æ±‚åœ¨å¤„ç†è¿‡ç¨‹ä¸­çš„å„ç§çŠ¶æ€ä¿¡æ¯ï¼š

**è¾“å‡ºç®¡ç†**  
output_idsåˆ—è¡¨è®°å½•äº†æ¨¡å‹ç”Ÿæˆçš„æ‰€æœ‰tokenï¼Œfill_idsæ˜¯origin_input_idså’Œoutput_idsçš„ç»„åˆï¼Œè¡¨ç¤ºå½“å‰çš„å®Œæ•´tokenåºåˆ—ã€‚

**ç”Ÿæˆæ§åˆ¶**  
finished_reasonè®°å½•è¯·æ±‚å®Œæˆçš„åŸå› ï¼Œå¯èƒ½æ˜¯è¾¾åˆ°æœ€å¤§é•¿åº¦ã€é‡åˆ°åœæ­¢tokenæˆ–å…¶ä»–æ¡ä»¶ã€‚å„ç§é•¿åº¦é™åˆ¶å’Œæ§åˆ¶å‚æ•°ç¡®ä¿ç”Ÿæˆè¿‡ç¨‹æŒ‰é¢„æœŸè¿›è¡Œã€‚

**å†…å­˜æ˜ å°„**  
req_pool_indiceså’Œå…¶ä»–ç´¢å¼•ä¿¡æ¯ç»´æŠ¤ç€è¯·æ±‚åœ¨å„ç§å†…å­˜æ± ä¸­çš„ä½ç½®ï¼Œè¿™å¯¹äºå†…å­˜ç®¡ç†å’Œç¼“å­˜æœºåˆ¶è‡³å…³é‡è¦ã€‚

### å¤šæ¨¡æ€æ”¯æŒ

Reqç±»è¿˜æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ï¼ŒåŒ…æ‹¬å›¾åƒã€éŸ³é¢‘ç­‰éæ–‡æœ¬æ•°æ®ï¼š

```python
self.input_embeds = input_embeds            # è¾“å…¥åµŒå…¥å‘é‡
self.image_inputs = None                    # å›¾åƒè¾“å…¥æ•°æ®
self.multimodal_inputs = None              # å¤šæ¨¡æ€è¾“å…¥ç»Ÿä¸€æ¥å£
```

è¿™ç§è®¾è®¡ä½¿å¾—SGLangèƒ½å¤Ÿå¤„ç†ä¸ä»…ä»…æ˜¯æ–‡æœ¬çš„å¤šç§æ¨¡æ€è¾“å…¥ï¼Œä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æä¾›äº†åŸºç¡€æ”¯æŒã€‚

---

## ScheduleBatchç±»è¯¦è§£

ScheduleBatchæ˜¯è°ƒåº¦å™¨å±‚é¢çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼Œè´Ÿè´£ç®¡ç†ä¸€ä¸ªæ‰¹æ¬¡ä¸­æ‰€æœ‰è¯·æ±‚çš„ä¿¡æ¯å’Œèµ„æºã€‚

### ğŸ¯ æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ

```python
@dataclasses.dataclass
class ScheduleBatch:
    """æ‰¹æ¬¡æ•°æ®ç»“æ„çš„æ ¸å¿ƒæ¦‚å¿µ"""
    # è¯·æ±‚å’Œèµ„æºç®¡ç†
    reqs: List[Req]                          # æ‰¹æ¬¡ä¸­çš„è¯·æ±‚åˆ—è¡¨
    req_to_token_pool: ReqToTokenPool        # è¯·æ±‚åˆ°tokenæ± çš„æ˜ å°„
    token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator  # KVç¼“å­˜åˆ†é…å™¨
    tree_cache: BasePrefixCache              # å‰ç¼€ç¼“å­˜æ ‘
    
    # æ‰¹æ¬¡é…ç½®
    forward_mode: ForwardMode                # å‰å‘æ¨¡å¼ï¼ˆé¢„å¡«å……/è§£ç ï¼‰
    enable_overlap: bool = False             # æ˜¯å¦å¯ç”¨é‡å å¤„ç†
    batch_is_full: bool = False             # æ‰¹æ¬¡æ˜¯å¦å·²æ»¡
    
    # GPUå¼ é‡æ•°æ®
    input_ids: torch.Tensor = None          # è¾“å…¥token IDå¼ é‡
    seq_lens: torch.Tensor = None           # åºåˆ—é•¿åº¦å¼ é‡
    req_pool_indices: torch.Tensor = None   # è¯·æ±‚æ± ç´¢å¼•å¼ é‡
```

### ğŸ” æºç å®ç°ç»†èŠ‚

**çœŸå®ScheduleBatchçš„å®Œæ•´ç»“æ„**ï¼šç”Ÿäº§ç¯å¢ƒä¸­çš„ScheduleBatchåŒ…å«äº†æ‰¹æ¬¡ç®¡ç†æ‰€éœ€çš„å…¨éƒ¨ä¿¡æ¯ï¼Œä»åŸºç¡€çš„è¯·æ±‚åˆ—è¡¨åˆ°å¤æ‚çš„GPUå¼ é‡æ•°æ®ï¼Œæ”¯æŒå¤šç§å‰å‘æ¨¡å¼å’Œä¼˜åŒ–ç­–ç•¥ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹å±•ç¤ºçœŸå®ScheduleBatchçš„ä¸»è¦å±æ€§ï¼Œçœç•¥äº†éƒ¨åˆ†å†…éƒ¨æ–¹æ³•å®ç°ã€‚å®Œæ•´å®ç°è¯·å‚è€ƒ `sglang/srt/managers/schedule_batch.py`ã€‚

```python
@dataclasses.dataclass
class ScheduleBatch(ScheduleBatchDisaggregationDecodeMixin):
    """çœŸå®çš„SGLang ScheduleBatchå®ç°"""
    
    # Request, memory pool, and cache
    reqs: List[Req]
    req_to_token_pool: ReqToTokenPool = None
    token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator = None
    tree_cache: BasePrefixCache = None
    is_hybrid: bool = False                  # SWAæ··åˆç¼“å­˜æ ‡å¿—

    # Batch configs
    model_config: ModelConfig = None
    forward_mode: ForwardMode = None
    enable_overlap: bool = False
    batch_is_full: bool = False             # æ‰¹æ¬¡æ»¡æ ‡å¿—ç”¨äºä¼˜åŒ–è°ƒåº¦å†³ç­–

    # Events
    launch_done: Optional[threading.Event] = None  # ç”¨äºé‡å äº‹ä»¶å¾ªç¯åŒæ­¥

    # For chunked prefill in PP
    chunked_req: Optional[Req] = None       # æµæ°´çº¿å¹¶è¡Œä¸­çš„åˆ†å—è¯·æ±‚

    # Sampling info
    sampling_info: SamplingBatchInfo = None
    next_batch_sampling_info: SamplingBatchInfo = None

    # Batched arguments to model runner
    input_ids: torch.Tensor = None          # shape: [b], int64
    input_embeds: torch.Tensor = None       # shape: [b, hidden_size], float32
    token_type_ids: torch.Tensor = None     # shape: [b], int64
    req_pool_indices: torch.Tensor = None   # shape: [b], int64
    seq_lens: torch.Tensor = None           # shape: [b], int64
    out_cache_loc: torch.Tensor = None      # shape: [b], int64 - KVç¼“å­˜è¾“å‡ºä½ç½®
    output_ids: torch.Tensor = None         # shape: [b], int64

    # For multimodal inputs
    multimodal_inputs: Optional[List] = None

    # Sequence length info
    seq_lens_sum: int = None                # æ‰€æœ‰åºåˆ—é•¿åº¦çš„æ€»å’Œ
    orig_seq_lens: torch.Tensor = None      # åŸå§‹åºåˆ—é•¿åº¦ï¼ŒQwen-1Mç›¸å…³

    # For DP attention
    global_num_tokens: Optional[List[int]] = None
    global_num_tokens_for_logprob: Optional[List[int]] = None
    is_extend_in_batch: bool = False
    can_run_dp_cuda_graph: bool = False
    tbo_split_seq_index: Optional[int] = None
    
    # For logprob
    return_logprob: bool = False
    top_logprobs_nums: Optional[List[int]] = None
    token_ids_logprobs: Optional[List[List[int]]] = None
    
    # For extend mode
    extend_num_tokens: Optional[int] = None
    extend_seq_lens: Optional[List[int]] = None
    extend_prefix_lens: Optional[List[int]] = None
    extend_logprob_start_lens: Optional[List[int]] = None
    
    # Special flags
    has_stream: bool = False                # æ˜¯å¦æœ‰æµå¼è¯·æ±‚
    has_grammar: bool = False               # æ˜¯å¦æœ‰è¯­æ³•çº¦æŸ
    is_prefill_only: bool = False          # æ˜¯å¦ä»…é¢„å¡«å……
    
    # Performance
    spec_algorithm: SpeculativeAlgorithm = None  # æŠ•æœºè§£ç ç®—æ³•
    device: str = "cuda"                    # è®¾å¤‡ç±»å‹
    
    # hicache pointer for synchronizing data loading from CPU to GPU
    hicache_consumer_index: int = 0

ğŸ’¡ **å®ç°è¯´æ˜**: çœŸå®çš„ScheduleBatchæœ‰60+ä¸ªå­—æ®µï¼Œæ”¯æŒå¤šæ¨¡æ€ã€DPæ³¨æ„åŠ›ã€æŠ•æœºè§£ç ã€åˆ†å±‚ç¼“å­˜ã€åˆ†ç¦»å¼æ¶æ„ç­‰å¤æ‚åŠŸèƒ½ã€‚æ•™å­¦ç‰ˆæœ¬çªå‡º"è¯·æ±‚â†’èµ„æºâ†’å¼ é‡"çš„æ ¸å¿ƒç»„ç»‡ç»“æ„ã€‚
```

### ğŸ“¦ å­—æ®µåŠŸèƒ½åˆ†ç±»

**æ ¸å¿ƒç»„ç»‡**: `reqs`, `req_to_token_pool`, `token_to_kv_pool_allocator`, `tree_cache`
**æ‰¹æ¬¡æ§åˆ¶**: `forward_mode`, `batch_is_full`, `enable_overlap`, `launch_done`
**GPUæ•°æ®**: `input_ids`, `seq_lens`, `req_pool_indices`, `out_cache_loc`
**å¤šæ¨¡æ€**: `multimodal_inputs`, `input_embeds`, `token_type_ids`
**æ€§èƒ½ä¼˜åŒ–**: `spec_algorithm`, `can_run_dp_cuda_graph`, `hicache_consumer_index`
**ç‰¹æ®ŠåŠŸèƒ½**: `has_stream`, `has_grammar`, `is_prefill_only`, `chunked_req`

### æ¨¡å‹æ‰§è¡Œæ•°æ®

ScheduleBatchè¿˜åŒ…å«äº†ä¼ é€’ç»™æ¨¡å‹æ‰§è¡Œå™¨çš„æ‰¹é‡åŒ–æ•°æ®ï¼š

```python
# Batched arguments to model runner
input_ids: torch.Tensor = None            # è¾“å…¥token IDå¼ é‡
seq_lens: torch.Tensor = None             # åºåˆ—é•¿åº¦å¼ é‡
req_pool_indices: torch.Tensor = None     # è¯·æ±‚æ± ç´¢å¼•å¼ é‡
out_cache_loc: torch.Tensor = None        # è¾“å‡ºç¼“å­˜ä½ç½®å¼ é‡
```

è¿™äº›å¼ é‡åŒ–çš„æ•°æ®æ˜¯GPUè®¡ç®—çš„ç›´æ¥è¾“å…¥ï¼Œå°†æ‰¹æ¬¡ä¸­æ‰€æœ‰è¯·æ±‚çš„ç›¸å…³ä¿¡æ¯ç»„ç»‡æˆäº†é€‚åˆå¹¶è¡Œå¤„ç†çš„æ ¼å¼ã€‚

### é‡‡æ ·å’Œç”Ÿæˆä¿¡æ¯

```python
# Sampling info
sampling_info: SamplingBatchInfo = None           # å½“å‰æ‰¹æ¬¡é‡‡æ ·ä¿¡æ¯
next_batch_sampling_info: SamplingBatchInfo = None # ä¸‹ä¸€æ‰¹æ¬¡é‡‡æ ·ä¿¡æ¯
```

é‡‡æ ·ä¿¡æ¯ç®¡ç†ç€æ‰¹æ¬¡ä¸­æ‰€æœ‰è¯·æ±‚çš„é‡‡æ ·å‚æ•°ï¼Œç¡®ä¿æ¯ä¸ªè¯·æ±‚éƒ½èƒ½æŒ‰ç…§æŒ‡å®šçš„æ–¹å¼è¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€‚

---

## ModelWorkerBatchæ•°æ®ç»“æ„

ModelWorkerBatchæ˜¯ScheduleBatchå‘æ¨¡å‹å·¥ä½œå™¨ä¼ é€’çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œå»é™¤äº†è°ƒåº¦å™¨ç‰¹æœ‰çš„ç®¡ç†ä¿¡æ¯ï¼Œä¸“æ³¨äºæ¨¡å‹æ¨ç†æ‰€éœ€çš„æ ¸å¿ƒæ•°æ®ã€‚

### ğŸ¯ æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ

```python
@dataclasses.dataclass
class ModelWorkerBatch:
    """æ¨¡å‹å·¥ä½œå™¨æ‰¹æ¬¡çš„æ ¸å¿ƒæ¦‚å¿µ"""
    bid: int                               # æ‰¹æ¬¡ID
    forward_mode: ForwardMode              # å‰å‘æ¨¡å¼ï¼ˆé¢„å¡«å……/è§£ç ï¼‰
    input_ids: torch.Tensor               # è¾“å…¥tokenå¼ é‡
    req_pool_indices: torch.Tensor        # è¯·æ±‚æ± ç´¢å¼•
    seq_lens: torch.Tensor                # åºåˆ—é•¿åº¦
    out_cache_loc: torch.Tensor           # è¾“å‡ºç¼“å­˜ä½ç½®
    sampling_info: SamplingBatchInfo      # é‡‡æ ·ä¿¡æ¯
```

### ğŸ” æºç å®ç°ç»†èŠ‚

```python
@dataclasses.dataclass
class ModelWorkerBatch:
    """çœŸå®çš„SGLang ModelWorkerBatchå®ç°"""
    
    # Basic batch info
    bid: int                               # æ‰¹æ¬¡ID
    forward_mode: ForwardMode              # å‰å‘æ¨¡å¼
    
    # Core tensors
    input_ids: torch.Tensor               # è¾“å…¥tokenå¼ é‡
    req_pool_indices: torch.Tensor        # è¯·æ±‚æ± ç´¢å¼•
    seq_lens: torch.Tensor                # åºåˆ—é•¿åº¦
    out_cache_loc: torch.Tensor           # KVç¼“å­˜è¾“å‡ºä½ç½®
    seq_lens_cpu: Optional[torch.Tensor]  # CPUä¸Šçš„åºåˆ—é•¿åº¦å¼ é‡
    seq_lens_sum: int                     # åºåˆ—é•¿åº¦æ€»å’Œ

    # For logprob
    return_logprob: bool
    top_logprobs_nums: Optional[List[int]]
    token_ids_logprobs: Optional[List[List[int]]]

    # For DP attention
    global_num_tokens: Optional[List[int]]
    global_num_tokens_for_logprob: Optional[List[int]]
    is_extend_in_batch: bool
    can_run_dp_cuda_graph: bool
    tbo_split_seq_index: Optional[int]
    global_forward_mode: Optional[ForwardMode]

    # For extend mode
    extend_num_tokens: Optional[int]
    extend_seq_lens: Optional[List[int]]
    extend_prefix_lens: Optional[List[int]]
    extend_logprob_start_lens: Optional[List[int]]
    extend_input_logprob_token_ids: Optional[torch.Tensor]

    # For multimodal
    multimodal_inputs: Optional[List[MultimodalInputs]]

    # For encoder-decoder
    encoder_cached: Optional[List[bool]]
    encoder_lens: Optional[torch.Tensor]
    encoder_lens_cpu: Optional[List[int]]
    encoder_out_cache_loc: Optional[torch.Tensor]

    # For LoRA
    lora_ids: Optional[List[str]]

    # Sampling info
    sampling_info: SamplingBatchInfo

    # Additional data
    orig_seq_lens: Optional[torch.Tensor] = None  # Qwen-1Mç›¸å…³
    input_embeds: Optional[torch.Tensor] = None   # è¾“å…¥åµŒå…¥
    token_type_ids: Optional[torch.Tensor] = None # è·¨ç¼–ç å™¨æ¨¡å‹

    # Speculative decoding
    spec_algorithm: SpeculativeAlgorithm = None
    spec_info: Optional[Union[EagleVerifyInput, EagleDraftInput]] = None

ğŸ’¡ **å®ç°è¯´æ˜**: çœŸå®çš„ModelWorkerBatchæœ‰30+ä¸ªå­—æ®µï¼ŒåŒ…å«DPæ³¨æ„åŠ›ã€ç¼–ç å™¨-è§£ç å™¨ã€LoRAã€æŠ•æœºè§£ç ç­‰å¤æ‚åŠŸèƒ½çš„æ”¯æŒã€‚æ•™å­¦ç‰ˆæœ¬çªå‡ºæ ¸å¿ƒçš„"è¾“å…¥â†’æ¨ç†â†’è¾“å‡º"æ•°æ®æµã€‚
```

è¿™ç§ç®€åŒ–ç¡®ä¿äº†æ¨¡å‹å·¥ä½œå™¨åªéœ€è¦å…³æ³¨æ¨ç†ç›¸å…³çš„ä¿¡æ¯ï¼Œæé«˜äº†æ•°æ®ä¼ é€’çš„æ•ˆç‡ã€‚

## å†…å­˜ç®¡ç†æ•°æ®ç»“æ„

### ReqToTokenPool

ReqToTokenPoolç®¡ç†ç€è¯·æ±‚åˆ°tokenä½ç½®çš„æ˜ å°„å…³ç³»ï¼Œæ˜¯å†…å­˜ç®¡ç†çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚å®ƒç»´æŠ¤ç€æ¯ä¸ªè¯·æ±‚åœ¨å†…å­˜ä¸­çš„tokenä½ç½®ä¿¡æ¯ï¼Œæ”¯æŒåŠ¨æ€çš„å†…å­˜åˆ†é…å’Œå›æ”¶ã€‚

### BaseTokenToKVPoolAllocator

è¿™ä¸ªæŠ½è±¡åŸºç±»å®šä¹‰äº†KVç¼“å­˜åˆ†é…å™¨çš„æ¥å£ï¼Œå…·ä½“çš„å®ç°ç±»è´Ÿè´£ç®¡ç†KVç¼“å­˜çš„åˆ†é…ã€å›æ”¶å’Œä¼˜åŒ–ã€‚ä¸åŒçš„å®ç°å¯ä»¥é‡‡ç”¨ä¸åŒçš„åˆ†é…ç­–ç•¥ï¼Œå¦‚é¡µé¢åˆ†é…ã€è¿ç»­åˆ†é…ç­‰ã€‚

### BasePrefixCache

å‰ç¼€ç¼“å­˜æ˜¯SGLangçš„é‡è¦ä¼˜åŒ–ç‰¹æ€§ï¼ŒBasePrefixCacheå®šä¹‰äº†å‰ç¼€ç¼“å­˜çš„é€šç”¨æ¥å£ã€‚å…·ä½“çš„å®ç°å¦‚RadixCacheèƒ½å¤Ÿè¯†åˆ«å’Œå¤ç”¨è¯·æ±‚é—´çš„å…¬å…±å‰ç¼€ï¼Œæ˜¾è‘—å‡å°‘è®¡ç®—å¼€é”€ã€‚

## å¤šæ¨¡æ€æ•°æ®ç»“æ„

SGLangæ”¯æŒå¤šæ¨¡æ€è¾“å…¥ï¼Œç›¸å…³çš„æ•°æ®ç»“æ„åŒ…æ‹¬ï¼š

### MultimodalInputs

è¿™ä¸ªç±»ç»Ÿä¸€ç®¡ç†å„ç§æ¨¡æ€çš„è¾“å…¥æ•°æ®ï¼ŒåŒ…æ‹¬å›¾åƒçš„pixel_valuesã€éŸ³é¢‘çš„audio_valuesç­‰ã€‚é€šè¿‡ç»Ÿä¸€çš„æ¥å£ï¼Œä¸åŒæ¨¡æ€çš„æ•°æ®èƒ½å¤Ÿè¢«ä¸€è‡´åœ°å¤„ç†å’Œä¼ é€’ã€‚

### MultimodalDataItem

è¡¨ç¤ºå•ä¸ªå¤šæ¨¡æ€æ•°æ®é¡¹ï¼ŒåŒ…å«äº†æ•°æ®ç±»å‹ã€å†…å®¹å’Œç›¸å…³çš„å…ƒæ•°æ®ä¿¡æ¯ã€‚

## æ‰¹é‡è¯·æ±‚æ•°æ®ç»“æ„

SGLangæ”¯æŒæ‰¹é‡è¯·æ±‚ä»¥æé«˜ç½‘ç»œä¼ è¾“æ•ˆç‡å’Œå¤„ç†æ€§èƒ½ï¼š

### BatchTokenizedGenerateReqInput

æ‰¹é‡ç”Ÿæˆè¯·æ±‚çš„è¾“å…¥ç»“æ„ï¼ŒåŒ…å«å¤šä¸ªTokenizedGenerateReqInputï¼š

```python
@dataclass
class BatchTokenizedGenerateReqInput:
    """æ‰¹é‡ç”Ÿæˆè¯·æ±‚è¾“å…¥"""
    requests: List[TokenizedGenerateReqInput]  # æ‰¹é‡è¯·æ±‚åˆ—è¡¨
    
    def __len__(self):
        return len(self.requests)
    
    def __iter__(self):
        return iter(self.requests)
```

### BatchTokenizedEmbeddingReqInput

æ‰¹é‡åµŒå…¥è¯·æ±‚çš„è¾“å…¥ç»“æ„ï¼ŒåŒ…å«å¤šä¸ªTokenizedEmbeddingReqInputï¼š

```python
@dataclass  
class BatchTokenizedEmbeddingReqInput:
    """æ‰¹é‡åµŒå…¥è¯·æ±‚è¾“å…¥"""
    requests: List[TokenizedEmbeddingReqInput]  # æ‰¹é‡è¯·æ±‚åˆ—è¡¨
    
    def __len__(self):
        return len(self.requests)
    
    def __iter__(self):
        return iter(self.requests)
```

è¿™äº›æ‰¹é‡ç»“æ„é€šè¿‡å‡å°‘ç½‘ç»œå¾€è¿”æ¬¡æ•°å’Œè¯·æ±‚å¤„ç†å¼€é”€æ¥æé«˜ç³»ç»Ÿæ•´ä½“æ€§èƒ½ã€‚

## åˆ†ç¦»å¼æ¶æ„æ•°æ®ç»“æ„

åœ¨åˆ†ç¦»å¼æ¶æ„ä¸­ï¼Œè¯·æ±‚å¯¹è±¡åŒ…å«é¢å¤–çš„åˆ†ç¦»å¼ç›¸å…³å­—æ®µï¼š

```python
class Req:
    # åˆ†ç¦»å¼æ¶æ„ç›¸å…³å­—æ®µ
    disagg_kv_sender: Optional[BaseKVSender] = None      # KVå‘é€å™¨
    disagg_kv_receiver: Optional[BaseKVReceiver] = None  # KVæ¥æ”¶å™¨
    bootstrap_host: Optional[str] = None                 # å¯åŠ¨ä¸»æœº
    bootstrap_port: Optional[int] = None                 # å¯åŠ¨ç«¯å£
    bootstrap_room: Optional[str] = None                 # å¯åŠ¨æˆ¿é—´ID
    
    # SWAæ··åˆç¼“å­˜ç›¸å…³å­—æ®µ
    swa_uuid_for_lock: Optional[str] = None             # SWAé”å®šUUID
```

## æ•°æ®ç»“æ„åä½œå…³ç³»

è¿™äº›æ•°æ®ç»“æ„ä¹‹é—´å­˜åœ¨ç€å¤æ‚çš„åä½œå…³ç³»ã€‚Reqå¯¹è±¡æ˜¯æœ€åŸºæœ¬çš„æ•°æ®å•å…ƒï¼ŒScheduleBatchå°†å¤šä¸ªReqç»„ç»‡æˆæ‰¹æ¬¡ï¼ŒåŒæ—¶ç®¡ç†ç€å„ç§èµ„æºæ± çš„å¼•ç”¨ã€‚åœ¨å¤„ç†è¿‡ç¨‹ä¸­ï¼ŒScheduleBatchä¼šè½¬æ¢æˆModelWorkerBatchä¼ é€’ç»™ä¸‹æ¸¸ç»„ä»¶ï¼Œæœ€ç»ˆè½¬æ¢æˆForwardBatchåœ¨GPUä¸Šæ‰§è¡Œã€‚

å†…å­˜ç®¡ç†ç›¸å…³çš„æ•°æ®ç»“æ„ä¸ºè¿™ä¸ªè¿‡ç¨‹æä¾›æ”¯æ’‘ï¼Œç¡®ä¿æ¯ä¸ªè¯·æ±‚éƒ½èƒ½è·å¾—å¿…è¦çš„å†…å­˜èµ„æºï¼ŒåŒæ—¶é€šè¿‡ç¼“å­˜æœºåˆ¶ä¼˜åŒ–æ€§èƒ½ã€‚å¤šæ¨¡æ€æ•°æ®ç»“æ„åˆ™æ‰©å±•äº†ç³»ç»Ÿçš„è¾“å…¥èƒ½åŠ›ï¼Œä½¿å¾—SGLangèƒ½å¤Ÿå¤„ç†ä¸ä»…ä»…æ˜¯æ–‡æœ¬çš„å¤šç§ç±»å‹è¾“å…¥ã€‚

---

## æ•°æ®ç»“æ„çš„è®¾è®¡ä¼˜åŠ¿

SGLangçš„æ•°æ®ç»“æ„è®¾è®¡ä½“ç°äº†å‡ ä¸ªé‡è¦çš„è®¾è®¡åŸåˆ™ï¼š

### ğŸ¯ æ ¸å¿ƒè®¾è®¡åŸåˆ™

**åˆ†å±‚æŠ½è±¡**: é€šè¿‡Reqâ†’ScheduleBatchâ†’ModelWorkerBatchçš„åˆ†å±‚è®¾è®¡ï¼Œç³»ç»Ÿèƒ½å¤Ÿåœ¨ä¸åŒæŠ½è±¡å±‚é¢è¿›è¡Œä¼˜åŒ–ï¼Œè°ƒåº¦å™¨å…³æ³¨é«˜å±‚å†³ç­–ï¼Œæ¨¡å‹æ‰§è¡Œå™¨å…³æ³¨åº•å±‚è®¡ç®—ã€‚

**æ¨¡å—åŒ–è®¾è®¡**: å„ä¸ªæ•°æ®ç»“æ„èŒè´£æ¸…æ™°ï¼Œç›¸äº’ä¹‹é—´é€šè¿‡æ˜ç¡®çš„æ¥å£è¿›è¡Œäº¤äº’ï¼Œæé«˜äº†ç³»ç»Ÿçš„å¯ç»´æŠ¤æ€§å’Œå¯æµ‹è¯•æ€§ã€‚

**æ€§èƒ½ä¼˜åŒ–**: æ•°æ®ç»“æ„å……åˆ†è€ƒè™‘äº†æ€§èƒ½å› ç´ ï¼š
- æ‰¹é‡åŒ–å¤„ç†å‡å°‘å‡½æ•°è°ƒç”¨å¼€é”€
- å¼ é‡åŒ–æ•°æ®æ”¯æŒGPUå¹¶è¡Œè®¡ç®—
- å†…å­˜æ± è®¾è®¡æé«˜å†…å­˜å±€éƒ¨æ€§
- ç¼“å­˜å‹å¥½çš„æ•°æ®å¸ƒå±€

**æ‰©å±•æ€§**: é€šè¿‡æŠ½è±¡åŸºç±»å’Œmixinæ¨¡å¼ï¼Œæ•°æ®ç»“æ„å…·å¤‡è‰¯å¥½çš„æ‰©å±•æ€§ï¼Œèƒ½å¤Ÿæ”¯æŒæ–°åŠŸèƒ½çš„æ·»åŠ å’Œç°æœ‰åŠŸèƒ½çš„ä¼˜åŒ–ã€‚

### ğŸ”§ å®ç°ç‰¹è‰²

**æºç å‡†ç¡®æ€§**: æœ¬æ–‡æ¡£åŸºäºçœŸå®SGLangæºç ç¼–å†™ï¼Œæ‰€æœ‰æ•°æ®ç»“æ„å®šä¹‰éƒ½æ¥è‡ªå®é™…å®ç°ï¼Œç¡®ä¿æŠ€æœ¯å‡†ç¡®æ€§ã€‚

**æ•™å­¦ä¸å®è·µå¹¶é‡**: é‡‡ç”¨"æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ + æºç å®ç°ç»†èŠ‚"çš„åŒé‡ç»“æ„ï¼Œæ—¢ä¾¿äºç†è§£è®¾è®¡æ€æƒ³ï¼Œåˆæä¾›å…·ä½“å®ç°å‚è€ƒã€‚

**å¤æ‚æ€§é€æ˜**: æ˜ç¡®å±•ç¤ºäº†æ•™å­¦ç®€åŒ–ç‰ˆæœ¬ä¸çœŸå®æºç çš„å·®å¼‚ï¼Œè®©å¼€å‘è€…äº†è§£å®é™…ç³»ç»Ÿçš„å¤æ‚æ€§ã€‚

### ğŸ“ˆ æ¼”è¿›è¶‹åŠ¿

SGLangçš„æ•°æ®ç»“æ„å±•ç°äº†ç°ä»£æ¨ç†ç³»ç»Ÿçš„æ¼”è¿›æ–¹å‘ï¼š
- **å¤šæ¨¡æ€æ”¯æŒ**: ä»çº¯æ–‡æœ¬æ‰©å±•åˆ°å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘
- **åˆ†ç¦»å¼æ¶æ„**: æ”¯æŒé¢„å¡«å……/è§£ç åˆ†ç¦»çš„å¤§è§„æ¨¡éƒ¨ç½²
- **é«˜çº§ä¼˜åŒ–**: æŠ•æœºè§£ç ã€æ··åˆç¼“å­˜ã€DPæ³¨æ„åŠ›ç­‰å‰æ²¿æŠ€æœ¯
- **äº§ä¸šåŒ–éœ€æ±‚**: ä¼šè¯ç®¡ç†ã€LoRAé€‚é…å™¨ã€ç›‘æ§è°ƒè¯•ç­‰å·¥ç¨‹ç‰¹æ€§

ç†è§£è¿™äº›æ ¸å¿ƒæ•°æ®ç»“æ„åŠå…¶ç›¸äº’å…³ç³»ï¼Œæ˜¯æ·±å…¥æŒæ¡SGLangè°ƒåº¦å™¨å·¥ä½œæœºåˆ¶çš„å…³é”®ã€‚è¿™äº›æ•°æ®ç»“æ„ä¸ä»…æ‰¿è½½ç€ç³»ç»Ÿçš„æ ¸å¿ƒä¿¡æ¯ï¼Œè¿˜ä½“ç°äº†SGLangåœ¨æ€§èƒ½ã€å¯ç»´æŠ¤æ€§å’Œæ‰©å±•æ€§æ–¹é¢çš„è®¾è®¡è€ƒé‡ï¼Œä¸ºåç»­çš„ç³»ç»Ÿå®šåˆ¶å’Œä¼˜åŒ–å¥ å®šäº†åšå®åŸºç¡€ã€‚
