# è¯·æ±‚å¤„ç†æœºåˆ¶

---

SGLangè°ƒåº¦å™¨é€šè¿‡ç»“æ„åŒ–çš„è¯·æ±‚å¤„ç†æµç¨‹æ¥ç®¡ç†å„ç§ç±»å‹çš„è¯·æ±‚ã€‚æœ¬ç« æ·±å…¥ä»‹ç»è°ƒåº¦å™¨çš„è¯·æ±‚æ¥æ”¶ã€åˆ†å‘å’Œå¤„ç†æœºåˆ¶ï¼Œæ­ç¤ºSGLangå¦‚ä½•é«˜æ•ˆå¤„ç†å¤šç§ç±»å‹çš„æ¨ç†è¯·æ±‚ã€‚

---

## 1. è¯·æ±‚å¤„ç†æ¶æ„æ€»è§ˆ

### 1.1 è¯·æ±‚å¤„ç†æµç¨‹å¯è§†åŒ–

```mermaid
graph TD
    subgraph "ğŸŒ è¯·æ±‚æ¥æº"
        direction LR
        A1["ğŸ”— Tokenizer<br/>ZMQç®¡é“"]
        A2["ğŸ”Œ RPCæ¥å£<br/>ZMQç®¡é“"]
    end

    subgraph "ğŸ“¥ è¯·æ±‚æ¥æ”¶å±‚"
        direction TB
        B1["recv_requests()<br/>éé˜»å¡æ¥æ”¶"]
        B2["SchedulerInputBlocker<br/>æµé‡æ§åˆ¶"]
        B3["process_input_requests()<br/>é¢„å¤„ç†å’Œè¿‡æ»¤"]
    end

    subgraph "ğŸ¯ è¯·æ±‚åˆ†å‘å±‚"
        direction TB
        C1["TypeBasedDispatcher<br/>ç±»å‹è·¯ç”±"]
        C2{"è¯·æ±‚ç±»å‹åˆ¤æ–­"}
        C3["20+ç§å¤„ç†æ–¹æ³•<br/>è‡ªåŠ¨åˆ†å‘"]
    end

    subgraph "ğŸ”„ è¯·æ±‚å¤„ç†å±‚"
        direction LR
        D1["handle_generate_request<br/>ç”Ÿæˆè¯·æ±‚"]
        D2["handle_embedding_request<br/>åµŒå…¥è¯·æ±‚"]
        D3["handle_batch_*_request<br/>æ‰¹é‡è¯·æ±‚"]
        D4["å…¶ä»–æ§åˆ¶è¯·æ±‚<br/>ç³»ç»Ÿç®¡ç†"]
    end

    subgraph "ğŸ“‹ é˜Ÿåˆ—ç®¡ç†å±‚"
        direction TB
        E1["waiting_queue<br/>ç­‰å¾…é˜Ÿåˆ—"]
        E2["grammar_queue<br/>è¯­æ³•é˜Ÿåˆ—"]
        E3["sessions<br/>ä¼šè¯ç®¡ç†"]
    end

    A1 --> B1
    A2 --> B1
    B1 --> B2
    B2 --> B3
    B3 --> C1
    C1 --> C2
    C2 --> C3
    C3 --> D1
    C3 --> D2
    C3 --> D3
    C3 --> D4
    D1 --> E1
    D1 --> E2
    D2 --> E1
    D3 --> E1
    D4 --> E3

    style A1 fill:#e3f2fd,color:#000000,stroke:#333
    style A2 fill:#e3f2fd,color:#000000,stroke:#333
    style B1 fill:#f1f8e9,color:#000000,stroke:#333
    style B2 fill:#f1f8e9,color:#000000,stroke:#333
    style B3 fill:#f1f8e9,color:#000000,stroke:#333
    style C1 fill:#fff3e0,color:#000000,stroke:#333
    style C2 fill:#fff3e0,color:#000000,stroke:#333
    style C3 fill:#fff3e0,color:#000000,stroke:#333
    style D1 fill:#ffebee,color:#000000,stroke:#333
    style D2 fill:#ffebee,color:#000000,stroke:#333
    style D3 fill:#ffebee,color:#000000,stroke:#333
    style D4 fill:#ffebee,color:#000000,stroke:#333
    style E1 fill:#f3e5f5,color:#000000,stroke:#333
    style E2 fill:#f3e5f5,color:#000000,stroke:#333
    style E3 fill:#f3e5f5,color:#000000,stroke:#333
```

**å›¾ç¤ºè¯´æ˜**ï¼šè“è‰²è¡¨ç¤ºè¯·æ±‚æ¥æºï¼Œç»¿è‰²è¡¨ç¤ºæ¥æ”¶å±‚ï¼Œæ©™è‰²è¡¨ç¤ºåˆ†å‘å±‚ï¼Œçº¢è‰²è¡¨ç¤ºå¤„ç†å±‚ï¼Œç´«è‰²è¡¨ç¤ºé˜Ÿåˆ—ç®¡ç†ã€‚æ•´ä¸ªæµç¨‹ä½“ç°äº†SGLangè¯·æ±‚å¤„ç†çš„å±‚æ¬¡åŒ–å’Œæ¨¡å—åŒ–è®¾è®¡ã€‚

---

## 2. è¯·æ±‚æ¥æ”¶æµç¨‹

### 2.1 recv_requestsæ–¹æ³•

**ç½‘ç»œè¯·æ±‚æ¥æ”¶çš„æ ¸å¿ƒæœºåˆ¶**ï¼šè°ƒåº¦å™¨é€šè¿‡`recv_requests()`æ–¹æ³•ä»tokenizerå’ŒRPCæ¥å£æ¥æ”¶è¯·æ±‚ï¼Œæ”¯æŒå¼ é‡å¹¶è¡Œã€æµæ°´çº¿å¹¶è¡Œå’Œæ•°æ®å¹¶è¡Œç­‰å¤æ‚æ¶æ„ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹ä¸ºè¯·æ±‚æ¥æ”¶çš„çœŸå®å®ç°ï¼Œå±•ç¤ºäº†SGLangåœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹çš„å¤æ‚æ¥æ”¶é€»è¾‘ã€‚å®Œæ•´å®ç°è¯·å‚è€ƒ `sglang/srt/managers/scheduler.py`ã€‚

```python
def recv_requests(self) -> List[Req]:
    """æ¥æ”¶æ¥è‡ªtokenizerçš„è¯·æ±‚ï¼ˆçœŸå®SGLangå®ç°ï¼‰"""
    
    # æ¥æ”¶è·³è¿‡å™¨ï¼šæ ¹æ®å‰å‘æ¨¡å¼å†³å®šæ˜¯å¦è·³è¿‡æ¥æ”¶
    if self.recv_skipper is not None:
        last_forward_mode = (
            self.last_batch.forward_mode if self.last_batch is not None else None
        )
        if not self.recv_skipper.handle(last_forward_mode):
            return []

    # æµæ°´çº¿å¹¶è¡Œï¼šåªæœ‰ç¬¬ä¸€ä¸ªPP rankæ¥æ”¶è¯·æ±‚
    if self.pp_rank == 0:
        if self.attn_tp_rank == 0:  # æ³¨æ„åŠ›å¼ é‡å¹¶è¡Œçš„ä¸»rank
            recv_reqs = []

            # ä»tokenizeræ¥æ”¶è¯·æ±‚ï¼ˆéé˜»å¡æ¨¡å¼ï¼‰
            while True:
                try:
                    recv_req = self.recv_from_tokenizer.recv_pyobj(zmq.NOBLOCK)
                except zmq.ZMQError:
                    break
                recv_reqs.append(recv_req)

            # ä»RPCæ¥å£æ¥æ”¶è¯·æ±‚ï¼ˆéé˜»å¡æ¨¡å¼ï¼‰
            while True:
                try:
                    recv_rpc = self.recv_from_rpc.recv_pyobj(zmq.NOBLOCK)
                except zmq.ZMQError:
                    break
                recv_reqs.append(recv_rpc)
        else:
            recv_reqs = None
    else:
        # éç¬¬ä¸€ä¸ªPP ranké€šè¿‡ç‚¹å¯¹ç‚¹é€šä¿¡æ¥æ”¶è¯·æ±‚
        if self.attn_tp_rank == 0:
            dp_offset = self.attn_dp_rank * self.attn_tp_size
            recv_reqs = point_to_point_pyobj(
                [],
                self.pp_rank * self.tp_size + dp_offset,
                self.world_group.device_group,
                (self.pp_rank - 1) * self.tp_size + dp_offset,
                self.pp_rank * self.tp_size + dp_offset,
            )
        else:
            recv_reqs = None

    # è¾“å…¥é˜»å¡å™¨å¤„ç†ï¼šæµé‡æ§åˆ¶æœºåˆ¶
    if self.input_blocker is not None:
        recv_reqs = self.input_blocker.handle(recv_reqs)

    # æ•°æ®å¹¶è¡Œæ³¨æ„åŠ›ï¼šå¹¿æ’­è¯·æ±‚åˆ°æ‰€æœ‰DP ranks
    if self.server_args.enable_dp_attention:
        recv_reqs = broadcast_pyobj(
            recv_reqs,
            self.world_group.device_group,
            self.attn_dp_rank * self.attn_tp_size,
        )
    
    return recv_reqs
```

### 2.2 process_input_requestsæ–¹æ³•

**è¯·æ±‚å¤„ç†çš„åˆ†å‘å’Œè¿‡æ»¤æœºåˆ¶**ï¼šæ¥æ”¶åˆ°è¯·æ±‚åï¼Œè°ƒåº¦å™¨é€šè¿‡`process_input_requests()`æ–¹æ³•è¿›è¡Œå¤„ç†ï¼ŒåŒ…æ‹¬å¥åº·æ£€æŸ¥è¿‡æ»¤ã€é˜Ÿåˆ—å®¹é‡æ£€æŸ¥å’Œç±»å‹åˆ†å‘ç­‰æ­¥éª¤ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹ä¸ºè¯·æ±‚å¤„ç†çš„çœŸå®å®ç°ï¼Œå±•ç¤ºäº†SGLangçš„å¥åº·æ£€æŸ¥ã€é˜Ÿåˆ—ç®¡ç†å’Œè¯·æ±‚åˆ†å‘é€»è¾‘ã€‚å®Œæ•´å®ç°è¯·å‚è€ƒ `sglang/srt/managers/scheduler.py`ã€‚

```python
def process_input_requests(self, recv_reqs: List):
    """å¤„ç†è¾“å…¥è¯·æ±‚åˆ—è¡¨ï¼ˆçœŸå®SGLangå®ç°ï¼‰"""
    for recv_req in recv_reqs:
        # å¥åº·æ£€æŸ¥è¯·æ±‚çš„ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæœ‰æ­£åœ¨è¿è¡Œçš„è¯·æ±‚æˆ–åˆ†å—è¯·æ±‚ï¼Œåˆ™å¿½ç•¥å¥åº·æ£€æŸ¥
        if is_health_check_generate_req(recv_req) and (
            self.chunked_req is not None           # æœ‰åˆ†å—è¯·æ±‚æ­£åœ¨å¤„ç†
            or not self.running_batch.is_empty()   # æœ‰æ‰¹æ¬¡æ­£åœ¨è¿è¡Œ
            or len(self.offload_tags) > 0          # æœ‰ç¦»è½½æ ‡ç­¾
        ):
            self.return_health_check_ct += 1
            continue

        # å·¥ä½œè¯·æ±‚çš„é˜Ÿåˆ—å¤§å°æ£€æŸ¥ï¼šé˜²æ­¢é˜Ÿåˆ—è¿‡è½½
        if is_work_request(recv_req):
            if len(self.waiting_queue) + 1 > self.max_queued_requests:
                # é˜Ÿåˆ—å·²æ»¡ï¼Œå‘é€ä¸­æ­¢è¯·æ±‚
                abort_req = AbortReq(
                    recv_req.rid,
                    finished_reason={
                        "type": "abort",
                        "status_code": HTTPStatus.SERVICE_UNAVAILABLE,
                        "message": "The request queue is full.",
                    },
                )
                self.send_to_tokenizer.send_pyobj(abort_req)
                continue
        
        # ä½¿ç”¨ç±»å‹åˆ†å‘å™¨å¤„ç†è¯·æ±‚ï¼šè‡ªåŠ¨è·¯ç”±åˆ°å¯¹åº”çš„å¤„ç†æ–¹æ³•
        output = self._request_dispatcher(recv_req)
        if output is not None:
            # æ ¹æ®è¾“å‡ºç±»å‹é€‰æ‹©å‘é€ç›®æ ‡
            if isinstance(output, RpcReqOutput):
                # RPCè¾“å‡ºå‘é€åˆ°RPCç®¡é“
                if self.recv_from_rpc is not None:
                    self.recv_from_rpc.send_pyobj(output)
            else:
                # å…¶ä»–è¾“å‡ºå‘é€åˆ°tokenizer
                self.send_to_tokenizer.send_pyobj(output)
```

---

## 3. è¯·æ±‚åˆ†å‘æœºåˆ¶

### 3.1 TypeBasedDispatcheræ ¸å¿ƒè®¾è®¡

**ç±»å‹é©±åŠ¨çš„è¯·æ±‚è·¯ç”±æœºåˆ¶**ï¼šSGLangä½¿ç”¨TypeBasedDispatcherå®ç°è¯·æ±‚ç±»å‹åˆ°å¤„ç†æ–¹æ³•çš„è‡ªåŠ¨æ˜ å°„ï¼Œæ”¯æŒ20+ç§ä¸åŒç±»å‹çš„è¯·æ±‚å¤„ç†ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹å±•ç¤ºçœŸå®çš„TypeBasedDispatcherå®ç°å’Œå®Œæ•´çš„è¯·æ±‚ç±»å‹æ˜ å°„ã€‚å®Œæ•´å®ç°è¯·å‚è€ƒ `sglang/utils.py` å’Œ `sglang/srt/managers/scheduler.py`ã€‚

```python
# TypeBasedDispatcherçš„çœŸå®å®ç°ï¼ˆæ¥è‡ªsglang/utils.pyï¼‰
class TypeBasedDispatcher:
    def __init__(self, mapping: List[Tuple[Type, Callable]]):
        self._mapping = mapping

    def __call__(self, obj: Any):
        for ty, fn in self._mapping:
            if isinstance(obj, ty):
                return fn(obj)
        raise ValueError(f"Invalid object: {obj}")
```

### 3.2 è¯·æ±‚åˆ†å‘å™¨é…ç½®

**å®Œæ•´çš„è¯·æ±‚ç±»å‹æ˜ å°„è¡¨**ï¼šè°ƒåº¦å™¨åˆå§‹åŒ–æ—¶é…ç½®äº†æ‰€æœ‰æ”¯æŒçš„è¯·æ±‚ç±»å‹åŠå…¶å¯¹åº”çš„å¤„ç†æ–¹æ³•ã€‚

```python
# çœŸå®çš„è°ƒåº¦å™¨è¯·æ±‚åˆ†å‘å™¨å®Œæ•´é…ç½®ï¼ˆæ¥è‡ªscheduler.__init__ï¼‰
self._request_dispatcher = TypeBasedDispatcher([
    # åŸºç¡€æ¨ç†è¯·æ±‚
    (TokenizedGenerateReqInput, self.handle_generate_request),
    (TokenizedEmbeddingReqInput, self.handle_embedding_request),
    (BatchTokenizedGenerateReqInput, self.handle_batch_generate_request),
    (BatchTokenizedEmbeddingReqInput, self.handle_batch_embedding_request),
    
    # ç¼“å­˜å’Œä¼šè¯ç®¡ç†
    (FlushCacheReqInput, self.flush_cache_wrapped),
    (AbortReq, self.abort_request),
    (OpenSessionReqInput, self.open_session),
    (CloseSessionReqInput, self.close_session),
    
    # æƒé‡æ›´æ–°å’Œæ¨¡å‹ç®¡ç†
    (UpdateWeightFromDiskReqInput, self.update_weights_from_disk),
    (InitWeightsUpdateGroupReqInput, self.init_weights_update_group),
    (UpdateWeightsFromDistributedReqInput, self.update_weights_from_distributed),
    (UpdateWeightsFromTensorReqInput, self.update_weights_from_tensor),
    (GetWeightsByNameReqInput, self.get_weights_by_name),
    
    # ç³»ç»Ÿæ§åˆ¶å’Œç›‘æ§
    (ReleaseMemoryOccupationReqInput, self.release_memory_occupation),
    (ResumeMemoryOccupationReqInput, self.resume_memory_occupation),
    (SlowDownReqInput, self.slow_down),
    (ProfileReq, self.profile),
    (FreezeGCReq, self.handle_freeze_gc),
    
    # çŠ¶æ€ç®¡ç†å’Œè°ƒè¯•
    (GetInternalStateReq, self.get_internal_state),
    (SetInternalStateReq, self.set_internal_state),
    (RpcReqInput, self.handle_rpc_request),
    
    # é«˜çº§åŠŸèƒ½
    (ExpertDistributionReq, self.expert_distribution_handle),
    (LoadLoRAAdapterReqInput, self.load_lora_adapter),
    (UnloadLoRAAdapterReqInput, self.unload_lora_adapter),
])
```

### 3.3 è¯·æ±‚ç±»å‹åˆ†ç±»

| ç±»åˆ« | è¯·æ±‚ç±»å‹ | å¤„ç†æ–¹æ³• | åŠŸèƒ½æè¿° |
|------|----------|----------|----------|
| **åŸºç¡€æ¨ç†** | TokenizedGenerateReqInput | handle_generate_request | æ–‡æœ¬ç”Ÿæˆè¯·æ±‚ |
| | TokenizedEmbeddingReqInput | handle_embedding_request | åµŒå…¥è®¡ç®—è¯·æ±‚ |
| | BatchTokenized*ReqInput | handle_batch_*_request | æ‰¹é‡è¯·æ±‚å¤„ç† |
| **ä¼šè¯ç®¡ç†** | OpenSessionReqInput | open_session | æ‰“å¼€æ–°ä¼šè¯ |
| | CloseSessionReqInput | close_session | å…³é—­ä¼šè¯ |
| | AbortReq | abort_request | ä¸­æ­¢è¯·æ±‚ |
| **æƒé‡ç®¡ç†** | UpdateWeightFromDiskReqInput | update_weights_from_disk | ä»ç£ç›˜æ›´æ–°æƒé‡ |
| | UpdateWeightsFromDistributedReqInput | update_weights_from_distributed | åˆ†å¸ƒå¼æƒé‡æ›´æ–° |
| | GetWeightsByNameReqInput | get_weights_by_name | è·å–æŒ‡å®šæƒé‡ |
| **ç³»ç»Ÿæ§åˆ¶** | FlushCacheReqInput | flush_cache_wrapped | åˆ·æ–°ç¼“å­˜ |
| | ProfileReq | profile | æ€§èƒ½åˆ†æ |
| | SlowDownReqInput | slow_down | é™é€Ÿæ§åˆ¶ |
| **LoRAç®¡ç†** | LoadLoRAAdapterReqInput | load_lora_adapter | åŠ è½½LoRAé€‚é…å™¨ |
| | UnloadLoRAAdapterReqInput | unload_lora_adapter | å¸è½½LoRAé€‚é…å™¨ |

---

## 4. ä¸»è¦è¯·æ±‚ç±»å‹å¤„ç†

### 4.1 ç”Ÿæˆè¯·æ±‚å¤„ç†

#### 4.1.1 æ ¸å¿ƒå¤„ç†æµç¨‹

**æ–‡æœ¬ç”Ÿæˆè¯·æ±‚çš„å®Œæ•´å¤„ç†æµç¨‹**ï¼šhandle_generate_requestæ–¹æ³•æ˜¯SGLangå¤„ç†æ–‡æœ¬ç”Ÿæˆè¯·æ±‚çš„æ ¸å¿ƒï¼ŒåŒ…å«ä¼šè¯ç®¡ç†ã€å¤šæ¨¡æ€æ”¯æŒã€è¯­æ³•çº¦æŸã€è¾“å…¥éªŒè¯ç­‰å¤æ‚é€»è¾‘ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹ä¸ºç”Ÿæˆè¯·æ±‚å¤„ç†çš„çœŸå®å®ç°æ ¸å¿ƒéƒ¨åˆ†ï¼Œå±•ç¤ºäº†SGLangçš„å®Œæ•´å¤„ç†é€»è¾‘ã€‚å®Œæ•´å®ç°è¯·å‚è€ƒ `sglang/srt/managers/scheduler.py`ã€‚

```python
def handle_generate_request(self, recv_req: TokenizedGenerateReqInput):
    """å¤„ç†æ–‡æœ¬ç”Ÿæˆè¯·æ±‚ï¼ˆçœŸå®SGLangå®ç°ï¼‰"""
    
    # æ•°æ®å¹¶è¡Œè´Ÿè½½å‡è¡¡ï¼šè®°å½•è¯·æ±‚çš„è´Ÿè½½å‡è¡¡ID
    if (self.server_args.enable_dp_attention 
        and self.server_args.load_balance_method == "minimum_tokens"):
        self.recv_dp_balance_id_this_term.append(recv_req.dp_balance_id)

    # ä¼šè¯ç®¡ç†ï¼šåˆ›å»ºæ–°è¯·æ±‚æˆ–ä½¿ç”¨ç°æœ‰ä¼šè¯
    if (recv_req.session_params is None 
        or recv_req.session_params.id is None
        or recv_req.session_params.id not in self.sessions):
        
        # å¤„ç†è¾“å…¥åµŒå…¥çš„ç‰¹æ®Šæƒ…å†µ
        if recv_req.input_embeds is not None:
            # ä¸ºè¾“å…¥åµŒå…¥ç”Ÿæˆè™šæ‹Ÿtoken IDs
            seq_length = len(recv_req.input_embeds)
            fake_input_ids = [1] * seq_length
            recv_req.input_ids = fake_input_ids

        # åˆ†ç¦»å¼æ¶æ„ï¼šè®¾ç½®é»˜è®¤bootstrapç«¯å£
        if recv_req.bootstrap_port is None:
            recv_req.bootstrap_port = self.server_args.disaggregation_bootstrap_port

        # åˆ›å»ºæ–°çš„Reqå¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰å‚æ•°
        req = Req(
            recv_req.rid,                           # è¯·æ±‚ID
            recv_req.input_text,                   # è¾“å…¥æ–‡æœ¬
            recv_req.input_ids,                    # è¾“å…¥token IDs
            recv_req.sampling_params,              # é‡‡æ ·å‚æ•°
            return_logprob=recv_req.return_logprob,        # æ˜¯å¦è¿”å›å¯¹æ•°æ¦‚ç‡
            top_logprobs_num=recv_req.top_logprobs_num,    # top-kå¯¹æ•°æ¦‚ç‡æ•°é‡
            token_ids_logprob=recv_req.token_ids_logprob,  # æŒ‡å®štokençš„å¯¹æ•°æ¦‚ç‡
            stream=recv_req.stream,                        # æ˜¯å¦æµå¼è¾“å‡º
            lora_id=recv_req.lora_id,                     # LoRAé€‚é…å™¨ID
            input_embeds=recv_req.input_embeds,           # è¾“å…¥åµŒå…¥
            custom_logit_processor=recv_req.custom_logit_processor,  # è‡ªå®šä¹‰logitå¤„ç†å™¨
            return_hidden_states=recv_req.return_hidden_states,      # æ˜¯å¦è¿”å›éšè—çŠ¶æ€
            eos_token_ids=self.model_config.hf_eos_token_id,        # EOS token IDs
            bootstrap_host=recv_req.bootstrap_host,        # åˆ†ç¦»å¼æ¨ç†ä¸»æœº
            bootstrap_port=recv_req.bootstrap_port,        # åˆ†ç¦»å¼æ¨ç†ç«¯å£
            bootstrap_room=recv_req.bootstrap_room,        # åˆ†ç¦»å¼æ¨ç†æˆ¿é—´
            data_parallel_rank=recv_req.data_parallel_rank, # æ•°æ®å¹¶è¡Œrank
            vocab_size=self.model_config.vocab_size,       # è¯æ±‡è¡¨å¤§å°
        )
        req.tokenizer = self.tokenizer
```

#### 4.1.2 å¤šæ¨¡æ€è¾“å…¥å¤„ç†

**å¤šæ¨¡æ€æ•°æ®çš„é¢„å¤„ç†é€»è¾‘**ï¼šSGLangæ”¯æŒå›¾åƒã€è§†é¢‘ã€éŸ³é¢‘ç­‰å¤šæ¨¡æ€è¾“å…¥ï¼Œåœ¨ç”Ÿæˆè¯·æ±‚å¤„ç†ä¸­éœ€è¦ç‰¹æ®Šçš„é¢„å¤„ç†æ­¥éª¤ã€‚

```python
# å¤šæ¨¡æ€è¾“å…¥å¤„ç†ï¼ˆæ¥è‡ªhandle_generate_requestçš„çœŸå®å®ç°ï¼‰
if recv_req.mm_inputs is not None:
    # ä»å­—å…¸åˆ›å»ºå¤šæ¨¡æ€è¾“å…¥å¯¹è±¡
    mm_inputs = MultimodalInputs.from_dict(recv_req.mm_inputs)
    # å¡«å……è¾“å…¥IDsä»¥é€‚åº”å¤šæ¨¡æ€token
    req.origin_input_ids = self.pad_input_ids_func(
        req.origin_input_ids, mm_inputs
    )
    # æ‰©å±•å›¾åƒè¾“å…¥åˆ°è¯·æ±‚ä¸­
    req.extend_image_inputs(mm_inputs)

# è¾“å…¥é•¿åº¦éªŒè¯
error_msg = validate_input_length(
    req,
    self.max_req_input_len,
    self.server_args.allow_auto_truncate,
)
if error_msg:
    req.set_finish_with_abort(error_msg)
```

#### 4.1.3 è¯­æ³•çº¦æŸå¤„ç†

**ç»“æ„åŒ–è¾“å‡ºçš„è¯­æ³•çº¦æŸ**ï¼šSGLangæ”¯æŒJSON Schemaã€æ­£åˆ™è¡¨è¾¾å¼ã€EBNFç­‰å¤šç§è¯­æ³•çº¦æŸã€‚

```python
# è¯­æ³•çº¦æŸå¤„ç†ï¼ˆæ¥è‡ªhandle_generate_requestçš„çœŸå®å®ç°ï¼‰
def _handle_grammar_constraints(self, req: Req):
    """å¤„ç†è¯­æ³•çº¦æŸçš„çœŸå®æ–¹æ³•"""
    add_to_grammar_queue = False
    if (
        req.sampling_params.json_schema is not None
        or req.sampling_params.regex is not None
        or req.sampling_params.ebnf is not None
        or req.sampling_params.structural_tag is not None
    ):
        assert self.grammar_backend is not None
        
        # æ„å»ºè¯­æ³•ç¼“å­˜é”®
        if req.sampling_params.json_schema is not None:
            key = ("json", req.sampling_params.json_schema)
        elif req.sampling_params.regex is not None:
            key = ("regex", req.sampling_params.regex)
        elif req.sampling_params.ebnf is not None:
            key = ("ebnf", req.sampling_params.ebnf)
        elif req.sampling_params.structural_tag:
            key = ("structural_tag", req.sampling_params.structural_tag)

        # è·å–ç¼“å­˜æˆ–åˆ›å»ºæ–°çš„è¯­æ³•å¯¹è±¡
        value, cache_hit = self.grammar_backend.get_cached_or_future_value(key)
        req.grammar = value

        if not cache_hit:
            req.grammar_key = key
            add_to_grammar_queue = True
        else:
            if value is INVALID_GRAMMAR_OBJ:  # ç¼“å­˜çš„æ— æ•ˆè¯­æ³•
                error_msg = f"Invalid grammar request with cache hit: {key=}"
                req.set_finish_with_abort(error_msg)

    # æ ¹æ®è¯­æ³•çŠ¶æ€å†³å®šé˜Ÿåˆ—åˆ†é…
    if add_to_grammar_queue:
        req.queue_time_start = time.perf_counter()
        self.grammar_queue.append(req)
    else:
        self._add_request_to_queue(req)
```

### 4.2 åµŒå…¥è¯·æ±‚å¤„ç†

#### 4.2.1 æ ¸å¿ƒå¤„ç†é€»è¾‘

**åµŒå…¥è®¡ç®—è¯·æ±‚çš„å¤„ç†**ï¼šç›¸æ¯”ç”Ÿæˆè¯·æ±‚ï¼ŒåµŒå…¥è¯·æ±‚å¤„ç†ç›¸å¯¹ç®€å•ï¼Œä¸»è¦å…³æ³¨è¾“å…¥å¤„ç†å’Œé•¿åº¦éªŒè¯ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹å±•ç¤ºçœŸå®çš„åµŒå…¥è¯·æ±‚å¤„ç†å®ç°ã€‚å®Œæ•´å®ç°è¯·å‚è€ƒ `sglang/srt/managers/scheduler.py`ã€‚

```python
def handle_embedding_request(self, recv_req: TokenizedEmbeddingReqInput):
    """çœŸå®çš„SGLangåµŒå…¥è¯·æ±‚å¤„ç†å®ç°"""
    req = Req(
        recv_req.rid,
        recv_req.input_text,
        recv_req.input_ids,
        recv_req.sampling_params,
        token_type_ids=recv_req.token_type_ids,
    )
    req.tokenizer = self.tokenizer

    # å¤šæ¨¡æ€è¾“å…¥å¤„ç†
    if recv_req.image_inputs is not None:
        image_inputs = MultimodalInputs.from_dict(recv_req.image_inputs)
        # å°†å•ä¸ªå›¾åƒtokenæ‰©å±•ä¸ºå¤šä¸ªè™šæ‹Ÿtokenä»¥æ¥æ”¶å›¾åƒåµŒå…¥
        req.origin_input_ids = self.pad_input_ids_func(
            req.origin_input_ids, image_inputs
        )
        req.extend_image_inputs(image_inputs)

    # è¾“å…¥é•¿åº¦éªŒè¯
    error_msg = validate_input_length(
        req,
        self.max_req_input_len,
        self.server_args.allow_auto_truncate,
    )
    if error_msg:
        req.set_finish_with_abort(error_msg)
        self._add_request_to_queue(req)
        return

    # è®¾ç½®logprobèµ·å§‹é•¿åº¦
    req.logprob_start_len = len(req.origin_input_ids) - 1
    self._add_request_to_queue(req)
```

### 4.3 æ‰¹é‡è¯·æ±‚å¤„ç†

**æ‰¹é‡è¯·æ±‚ä¼˜åŒ–**ï¼šSGLangæ”¯æŒæ‰¹é‡è¯·æ±‚å¤„ç†ä»¥æé«˜ç½‘ç»œæ•ˆç‡å’Œå¤„ç†æ€§èƒ½ã€‚

```python
def handle_batch_generate_request(self, recv_req: BatchTokenizedGenerateReqInput):
    """å¤„ç†æ‰¹é‡ç”Ÿæˆè¯·æ±‚ä¼˜åŒ–ï¼ˆçœŸå®å®ç°ï¼‰"""
    logger.debug(f"Processing batch generate request with {len(recv_req)} requests")
    
    # æ‰¹é‡å¤„ç†æ¯ä¸ªè¯·æ±‚
    for tokenized_req in recv_req:
        self.handle_generate_request(tokenized_req)

def handle_batch_embedding_request(self, recv_req: BatchTokenizedEmbeddingReqInput):
    """å¤„ç†æ‰¹é‡åµŒå…¥è¯·æ±‚ä¼˜åŒ–ï¼ˆçœŸå®å®ç°ï¼‰"""
    logger.debug(f"Processing batch embedding request with {len(recv_req)} requests")
    
    # æ‰¹é‡å¤„ç†æ¯ä¸ªè¯·æ±‚
    for tokenized_req in recv_req:
        self.handle_embedding_request(tokenized_req)
```

---

## 5. ç³»ç»Ÿæ§åˆ¶å’Œç®¡ç†è¯·æ±‚

### 5.1 ä¼šè¯ç®¡ç†

**ä¼šè¯ç”Ÿå‘½å‘¨æœŸç®¡ç†**ï¼šSGLangæ”¯æŒæŒä¹…åŒ–ä¼šè¯ï¼Œç”¨äºè¿ç»­å¯¹è¯å’ŒçŠ¶æ€ä¿æŒã€‚

```python
def open_session(self, recv_req: OpenSessionReqInput):
    """æ‰“å¼€æ–°ä¼šè¯ï¼ˆçœŸå®å®ç°ï¼‰"""
    if recv_req.session_id not in self.sessions:
        self.sessions[recv_req.session_id] = Session(
            recv_req.capacity_of_str_len, recv_req.session_id
        )
    return OpenSessionReqOutput(recv_req.session_id)

def close_session(self, recv_req: CloseSessionReqInput):
    """å…³é—­ä¼šè¯ï¼ˆçœŸå®å®ç°ï¼‰"""
    if recv_req.session_id in self.sessions:
        session = self.sessions[recv_req.session_id]
        session.clear()
        del self.sessions[recv_req.session_id]
    return CloseSessionReqOutput(recv_req.session_id)
```

### 5.2 ç¼“å­˜ç®¡ç†

```python
def flush_cache_wrapped(self, recv_req: FlushCacheReqInput):
    """åˆ·æ–°ç¼“å­˜çš„åŒ…è£…æ–¹æ³•ï¼ˆçœŸå®å®ç°ï¼‰"""
    success = self.flush_cache()
    return FlushCacheReqOutput(success)

def abort_request(self, recv_req: AbortReq):
    """ä¸­æ­¢æŒ‡å®šçš„è¯·æ±‚ï¼ˆçœŸå®å®ç°ï¼‰"""
    # ä»ç­‰å¾…é˜Ÿåˆ—ä¸­ç§»é™¤
    for i, req in enumerate(self.waiting_queue):
        if req.rid == recv_req.rid:
            req.set_finish_with_abort(recv_req.finished_reason["message"])
            del self.waiting_queue[i]
            return
    
    # ä»è¿è¡Œæ‰¹æ¬¡ä¸­ç§»é™¤
    if self.running_batch:
        for req in self.running_batch.reqs:
            if req.rid == recv_req.rid:
                req.set_finish_with_abort(recv_req.finished_reason["message"])
                return
```

---

## 6. æµé‡æ§åˆ¶æœºåˆ¶

### 6.1 SchedulerInputBlocker

**è¯·æ±‚é˜»å¡å’Œæµé‡æ§åˆ¶**ï¼šè°ƒåº¦å™¨æ”¯æŒè¯·æ±‚é˜»å¡æœºåˆ¶ï¼Œç”¨äºç³»ç»Ÿç»´æŠ¤å’Œæµé‡æ§åˆ¶ã€‚

> ğŸ“ **ç®€åŒ–è¯´æ˜**ï¼šä»¥ä¸‹å±•ç¤ºçœŸå®çš„SchedulerInputBlockerå®ç°ã€‚å®Œæ•´å®ç°è¯·å‚è€ƒ `sglang/srt/managers/scheduler_input_blocker.py`ã€‚

```python
class SchedulerInputBlocker:
    """çœŸå®çš„SGLangæµé‡æ§åˆ¶å®ç°"""
    def __init__(self, noop: bool):
        self._state = _State.UNBLOCKED
        self._pending_reqs = []
        self._noop = noop
        self._global_unblock_barrier = PollBasedBarrier(noop=noop)

    def handle(self, recv_reqs: Optional[List[Any]]):
        """å¤„ç†æ¥æ”¶åˆ°çš„è¯·æ±‚ï¼Œæ ¹æ®é˜»å¡çŠ¶æ€å†³å®šæ˜¯å¦æ”¾è¡Œ"""
        assert (recv_reqs is None) == self._noop

        if not self._noop:
            output_reqs = []
            for recv_req in recv_reqs:
                output_reqs += self._handle_recv_req(recv_req)

        # æ£€æŸ¥å…¨å±€è§£é™¤é˜»å¡å±éšœ
        global_arrived_unblock_barrier = (
            self._global_unblock_barrier.poll_global_arrived()
        )
        if (self._state == _State.GLOBAL_UNBLOCK_BARRIER 
            and global_arrived_unblock_barrier):
            output_reqs += self._handle_arrive_unblock_barrier()

        if not self._noop:
            return output_reqs

    def _handle_recv_req(self, recv_req):
        """å¤„ç†å•ä¸ªè¯·æ±‚"""
        if isinstance(recv_req, BlockReqInput):
            if recv_req.type == BlockReqType.BLOCK:
                self._execute_block_req()
                return []
            elif recv_req.type == BlockReqType.UNBLOCK:
                self._execute_unblock_req()
                return []
            else:
                raise NotImplementedError(f"{recv_req=}")
        else:
            if self._state == _State.UNBLOCKED:
                return [recv_req]
            else:
                self._pending_reqs.append(recv_req)
                return []
```

---

## 7. æŠ€æœ¯ç‰¹è‰²æ€»ç»“

### 7.1 è®¾è®¡ä¼˜åŠ¿

SGLangè¯·æ±‚å¤„ç†æœºåˆ¶çš„æ ¸å¿ƒä¼˜åŠ¿ï¼š

**ğŸ¯ ç±»å‹åŒ–åˆ†å‘**
- ä½¿ç”¨TypeBasedDispatcherå®ç°O(1)å¤æ‚åº¦çš„è¯·æ±‚è·¯ç”±
- æ”¯æŒ20+ç§ä¸åŒç±»å‹çš„è¯·æ±‚å¤„ç†
- æ˜“äºæ‰©å±•æ–°çš„è¯·æ±‚ç±»å‹

**ğŸ”„ å¼‚æ­¥å¤„ç†**
- éé˜»å¡çš„ZMQé€šä¿¡æœºåˆ¶
- æ”¯æŒå¹¶å‘è¯·æ±‚å¤„ç†
- æµé‡æ§åˆ¶å’ŒèƒŒå‹ç®¡ç†

**ğŸ§© æ¨¡å—åŒ–è®¾è®¡**
- æ¸…æ™°çš„å±‚æ¬¡åˆ†ç¦»ï¼ˆæ¥æ”¶â†’åˆ†å‘â†’å¤„ç†â†’é˜Ÿåˆ—ï¼‰
- æ¯å±‚ä¸“æ³¨äºç‰¹å®šèŒè´£
- ä¾¿äºæµ‹è¯•å’Œç»´æŠ¤

**ğŸŒ åˆ†å¸ƒå¼æ”¯æŒ**
- åŸç”Ÿæ”¯æŒTPã€PPã€DPå¤šç§å¹¶è¡Œæ¨¡å¼
- åˆ†ç¦»å¼æ¶æ„çš„bootstrapæœºåˆ¶
- è·¨è¿›ç¨‹çš„è¯·æ±‚åŒæ­¥

### 7.2 æ€§èƒ½ä¼˜åŒ–

**å†…å­˜æ•ˆç‡**ï¼šSGLangé€šè¿‡å¤šç§æœºåˆ¶å®ç°é«˜æ•ˆçš„å†…å­˜ä½¿ç”¨

```python
# é›¶æ‹·è´çš„è¯·æ±‚ä¼ é€’ï¼ˆæ¥è‡ªrecv_requestsçš„çœŸå®å®ç°ï¼‰
def recv_requests(self) -> List[Req]:
    """é›¶æ‹·è´çš„ZMQå¯¹è±¡ä¼ é€’"""
    while True:
        try:
            # recv_pyobjä½¿ç”¨pickleè¿›è¡Œé«˜æ•ˆåºåˆ—åŒ–ï¼Œé¿å…ä¸å¿…è¦çš„æ‹·è´
            recv_req = self.recv_from_tokenizer.recv_pyobj(zmq.NOBLOCK)
            recv_reqs.append(recv_req)  # ç›´æ¥æ·»åŠ å¼•ç”¨ï¼Œæ— éœ€æ‹·è´
        except zmq.ZMQError:
            break

# æ™ºèƒ½çš„å¯¹è±¡æ± ç®¡ç†ï¼ˆæ¥è‡ªScheduleBatchçš„çœŸå®å®ç°ï¼‰
def alloc_req_slots(self, num_reqs: int):
    """æ™ºèƒ½çš„è¯·æ±‚æ§½ä½åˆ†é…"""
    req_pool_indices = self.req_to_token_pool.alloc(num_reqs)
    if req_pool_indices is None:
        # å†…å­˜ä¸è¶³æ—¶çš„æ™ºèƒ½é”™è¯¯å¤„ç†
        raise RuntimeError(
            f"alloc_req_slots runs out of memory. "
            f"Available: {self.req_to_token_pool.available_size()}, "
            f"Requested: {num_reqs}"
        )
    return req_pool_indices
```

**è®¡ç®—ä¼˜åŒ–**ï¼šé€šè¿‡æ‰¹é‡åŒ–å’Œç¼“å­˜æœºåˆ¶æå‡è®¡ç®—æ•ˆç‡

```python
# æ‰¹é‡è¯·æ±‚å¤„ç†ï¼ˆæ¥è‡ªhandle_batch_generate_requestçš„çœŸå®å®ç°ï¼‰
def handle_batch_generate_request(self, recv_req: BatchTokenizedGenerateReqInput):
    """æ‰¹é‡è¯·æ±‚å‡å°‘å‡½æ•°è°ƒç”¨å¼€é”€"""
    logger.debug(f"Processing batch with {len(recv_req)} requests")
    
    # ä¸€æ¬¡æ€§å¤„ç†å¤šä¸ªè¯·æ±‚ï¼Œå‡å°‘å¾ªç¯å¼€é”€
    for tokenized_req in recv_req:
        self.handle_generate_request(tokenized_req)

# è¯­æ³•çº¦æŸçš„ç¼“å­˜æœºåˆ¶ï¼ˆæ¥è‡ªgrammar_backendçš„çœŸå®å®ç°ï¼‰
def get_cached_or_future_value(self, key: Tuple[str, str]) -> Tuple[BaseGrammarObject, bool]:
    """è¯­æ³•å¯¹è±¡çš„æ™ºèƒ½ç¼“å­˜"""
    if key in self.cache:
        # ç¼“å­˜å‘½ä¸­ï¼Œç›´æ¥è¿”å›
        cache_entry = self.cache[key]
        cache_entry.event.wait()  # ç­‰å¾…å¼‚æ­¥æ„å»ºå®Œæˆ
        return cache_entry.value, True
    else:
        # ç¼“å­˜æœªå‘½ä¸­ï¼Œå¼‚æ­¥æ„å»ºæ–°å¯¹è±¡
        event = Event()
        self.cache[key] = CacheEntry(None, event)
        # åœ¨çº¿ç¨‹æ± ä¸­å¼‚æ­¥æ„å»ºè¯­æ³•å¯¹è±¡
        future = self.executor.submit(self._init_value_dispatch, key)
        return future, False
```

**ç½‘ç»œä¼˜åŒ–**ï¼šé«˜æ€§èƒ½çš„ç½‘ç»œé€šä¿¡å’Œæ•°æ®ä¼ è¾“

```python
# ZMQé«˜æ€§èƒ½æ¶ˆæ¯ä¼ é€’ï¼ˆæ¥è‡ªrecv_requestsçš„çœŸå®å®ç°ï¼‰
# éé˜»å¡æ¨¡å¼é¿å…çº¿ç¨‹é˜»å¡
recv_req = self.recv_from_tokenizer.recv_pyobj(zmq.NOBLOCK)

# æ•°æ®å¹¶è¡Œçš„å¹¿æ’­ä¼˜åŒ–ï¼ˆæ¥è‡ªrecv_requestsçš„çœŸå®å®ç°ï¼‰
if self.server_args.enable_dp_attention:
    # é«˜æ•ˆçš„é›†ä½“é€šä¿¡ï¼Œä¸€æ¬¡å¹¿æ’­åˆ°æ‰€æœ‰DP ranks
    recv_reqs = broadcast_pyobj(
        recv_reqs,
        self.world_group.device_group,
        self.attn_dp_rank * self.attn_tp_size,
    )
```

---

## 8. å¼€å‘è€…æŒ‡å—

### 8.1 æ‰©å±•æ–°è¯·æ±‚ç±»å‹

**å®Œæ•´çš„è¯·æ±‚ç±»å‹æ‰©å±•ç¤ºä¾‹**ï¼šä»¥æ·»åŠ è‡ªå®šä¹‰æ¨¡å‹æ£€æŸ¥è¯·æ±‚ä¸ºä¾‹

```python
# 1. å®šä¹‰è¯·æ±‚ç±»ï¼ˆåœ¨io_struct.pyä¸­ï¼‰
@dataclass
class CustomModelCheckReqInput:
    """è‡ªå®šä¹‰æ¨¡å‹æ£€æŸ¥è¯·æ±‚"""
    rid: str                    # è¯·æ±‚ID
    model_name: str            # è¦æ£€æŸ¥çš„æ¨¡å‹åç§°
    check_type: str            # æ£€æŸ¥ç±»å‹ï¼ˆweights/config/statusï¼‰

@dataclass  
class CustomModelCheckReqOutput:
    """è‡ªå®šä¹‰æ¨¡å‹æ£€æŸ¥è¾“å‡º"""
    rid: str                   # è¯·æ±‚ID
    status: str               # æ£€æŸ¥çŠ¶æ€
    details: dict             # è¯¦ç»†ä¿¡æ¯

# 2. å®ç°å¤„ç†æ–¹æ³•ï¼ˆåœ¨Schedulerä¸­ï¼‰
def handle_custom_model_check(self, recv_req: CustomModelCheckReqInput):
    """å¤„ç†è‡ªå®šä¹‰æ¨¡å‹æ£€æŸ¥è¯·æ±‚"""
    try:
        # æ‰§è¡Œæ¨¡å‹æ£€æŸ¥é€»è¾‘
        if recv_req.check_type == "weights":
            status = self._check_model_weights(recv_req.model_name)
        elif recv_req.check_type == "config":
            status = self._check_model_config(recv_req.model_name)
        else:
            status = self._check_model_status(recv_req.model_name)
            
        return CustomModelCheckReqOutput(
            rid=recv_req.rid,
            status="success",
            details=status
        )
    except Exception as e:
        return CustomModelCheckReqOutput(
            rid=recv_req.rid,
            status="error", 
            details={"error": str(e)}
        )

# 3. æ³¨å†Œåˆ†å‘å™¨ï¼ˆåœ¨__init__ä¸­æ·»åŠ ï¼‰
self._request_dispatcher = TypeBasedDispatcher([
    # ... ç°æœ‰æ˜ å°„ ...
    (CustomModelCheckReqInput, self.handle_custom_model_check),  # æ–°å¢æ˜ å°„
])
```

### 8.2 è°ƒè¯•æŠ€å·§

**è¯·æ±‚è¿½è¸ª**ï¼šä½¿ç”¨çœŸå®çš„è°ƒè¯•æ–¹æ³•è¿½è¸ªè¯·æ±‚æµç¨‹

```python
# è¯·æ±‚IDå…¨é“¾è·¯è¿½è¸ªï¼ˆæ¥è‡ªçœŸå®è°ƒè¯•å®ç°ï¼‰
def trace_request_flow(self, rid: str):
    """è¿½è¸ªè¯·æ±‚åœ¨ç³»ç»Ÿä¸­çš„æµè½¬çŠ¶æ€"""
    
    # æ£€æŸ¥ç­‰å¾…é˜Ÿåˆ—
    waiting_req = next((req for req in self.waiting_queue if req.rid == rid), None)
    if waiting_req:
        logger.info(f"Request {rid} found in waiting_queue, position: {self.waiting_queue.index(waiting_req)}")
    
    # æ£€æŸ¥è¯­æ³•é˜Ÿåˆ—
    grammar_req = next((req for req in self.grammar_queue if req.rid == rid), None)
    if grammar_req:
        logger.info(f"Request {rid} found in grammar_queue, grammar_key: {getattr(grammar_req, 'grammar_key', None)}")
    
    # æ£€æŸ¥è¿è¡Œæ‰¹æ¬¡
    if not self.running_batch.is_empty():
        running_req = next((req for req in self.running_batch.reqs if req.rid == rid), None)
        if running_req:
            logger.info(f"Request {rid} found in running_batch, req_pool_idx: {running_req.req_pool_idx}")

# é˜Ÿåˆ—çŠ¶æ€ç›‘æ§ï¼ˆæ¥è‡ªçœŸå®ç›‘æ§å®ç°ï¼‰
def get_queue_status(self):
    """è·å–é˜Ÿåˆ—çŠ¶æ€çš„è¯¦ç»†ä¿¡æ¯"""
    return {
        "waiting_queue_size": len(self.waiting_queue),
        "grammar_queue_size": len(self.grammar_queue),
        "running_batch_size": self.running_batch.batch_size(),
        "sessions_count": len(self.sessions),
        "max_queued_requests": self.max_queued_requests,
        "batch_is_full": self.running_batch.batch_is_full,
    }
```

**æ€§èƒ½åˆ†æ**ï¼šä½¿ç”¨SGLangå†…ç½®çš„æ€§èƒ½åˆ†æå·¥å…·

```python
# æ€§èƒ½åˆ†æè¯·æ±‚ï¼ˆæ¥è‡ªProfileReqçš„çœŸå®å®ç°ï¼‰
def profile(self, recv_req: ProfileReq):
    """æ‰§è¡Œæ€§èƒ½åˆ†æ"""
    if recv_req.action == "start":
        # å¯åŠ¨PyTorch Profiler
        if self.profiler is None:
            self.profiler = torch.profiler.profile(
                activities=[
                    torch.profiler.ProfilerActivity.CPU,
                    torch.profiler.ProfilerActivity.CUDA,
                ],
                record_shapes=True,
                profile_memory=True,
                with_stack=True,
            )
            self.profiler.start()
        return ProfileReqOutput("Profiler started")
    
    elif recv_req.action == "stop":
        # åœæ­¢å¹¶å¯¼å‡ºåˆ†æç»“æœ
        if self.profiler is not None:
            self.profiler.stop()
            trace = self.profiler.key_averages().table(sort_by="cuda_time_total", row_limit=10)
            self.profiler = None
            return ProfileReqOutput(f"Profiler stopped:\n{trace}")

# è¯·æ±‚å¤„ç†æ—¶é—´ç›‘æ§ï¼ˆæ¥è‡ªçœŸå®æŒ‡æ ‡æ”¶é›†ï¼‰
def log_request_processing_time(self, req: Req, stage: str):
    """è®°å½•è¯·æ±‚å¤„ç†æ—¶é—´"""
    current_time = time.perf_counter()
    if not hasattr(req, 'processing_times'):
        req.processing_times = {}
    
    if stage == "start":
        req.processing_times['queue_start'] = current_time
    elif stage == "dispatch":
        req.processing_times['dispatch_time'] = current_time
    elif stage == "process":
        req.processing_times['process_time'] = current_time
    elif stage == "complete":
        req.processing_times['complete_time'] = current_time
        
        # è®¡ç®—æ€»å¤„ç†æ—¶é—´
        total_time = current_time - req.processing_times['queue_start']
        logger.info(f"Request {req.rid} total processing time: {total_time:.3f}s")
```

è¿™ä¸ªå®Œæ•´çš„è¯·æ±‚å¤„ç†æœºåˆ¶å±•ç¤ºäº†SGLangè°ƒåº¦å™¨çš„å·¥ç¨‹å¤æ‚åº¦å’ŒæŠ€æœ¯æ·±åº¦ï¼Œä¸ºå¼€å‘è€…æä¾›äº†æ·±å…¥ç†è§£å’Œæ‰©å±•çš„åŸºç¡€ã€‚