# åˆ†ç¦»å¼æ¶æ„æ”¯æŒ

---

SGLangæ”¯æŒé¢„å¡«å……å’Œè§£ç åˆ†ç¦»çš„åˆ†ç¦»å¼æ¶æ„ï¼Œå…è®¸å°†é¢„å¡«å……å’Œè§£ç è¿‡ç¨‹åˆ†åˆ«éƒ¨ç½²åœ¨ä¸åŒçš„æœåŠ¡å™¨ä¸Šï¼Œä»¥æ”¯æŒè¶…å¤§æ¨¡å‹å’Œæé«˜èµ„æºåˆ©ç”¨ç‡ã€‚

---

## ğŸ¯ åˆ†ç¦»æ¨¡å¼ç±»å‹

SGLangå®šä¹‰äº†ä¸‰ç§åˆ†ç¦»æ¨¡å¼ï¼š

```python
class DisaggregationMode(Enum):
    NULL = "null"           # éåˆ†ç¦»æ¨¡å¼ï¼ˆæ ‡å‡†æ¨¡å¼ï¼‰
    PREFILL = "prefill"     # é¢„å¡«å……æœåŠ¡å™¨æ¨¡å¼
    DECODE = "decode"       # è§£ç æœåŠ¡å™¨æ¨¡å¼
```

---

## ğŸš€ é¢„å¡«å……åˆ†ç¦»æ¶æ„

### ğŸ“¦ PrefillBootstrapQueue

é¢„å¡«å……æœåŠ¡å™¨ä½¿ç”¨PrefillBootstrapQueueç®¡ç†ä¼ å…¥çš„è¯·æ±‚ï¼š

```python
class PrefillBootstrapQueue:
    """é¢„å¡«å……å¯åŠ¨é˜Ÿåˆ—ç®¡ç†å™¨"""
    
    def __init__(self, 
                 max_total_num_tokens: int,
                 kv_manager: BaseKVManager,
                 transfer_backend: TransferBackend,
                 bootstrap_port: int,
                 tp_rank: int,
                 pp_rank: int,
                 scheduler: Scheduler):
        self.max_total_num_tokens = max_total_num_tokens
        self.kv_manager = kv_manager
        self.transfer_backend = transfer_backend
        self.bootstrap_port = bootstrap_port
        self.tp_rank = tp_rank
        self.pp_rank = pp_rank
        self.scheduler = scheduler
        self.queue: List[Req] = []

    def add(self, req: Req, num_kv_heads: int) -> None:
        """æ·»åŠ è¯·æ±‚åˆ°é¢„å¡«å……é˜Ÿåˆ—"""
        if self._check_if_req_exceed_kv_capacity(req):
            return

        # åˆ›å»ºKVå‘é€å™¨
        if req.bootstrap_host == FAKE_BOOTSTRAP_HOST:
            kv_sender_class = get_kv_class(TransferBackend.FAKE, KVClassType.SENDER)
        else:
            kv_sender_class = get_kv_class(self.transfer_backend, KVClassType.SENDER)

        dest_tp_ranks = [self.tp_rank]

        req.disagg_kv_sender = kv_sender_class(
            mgr=self.kv_manager,
            bootstrap_addr=f"{req.bootstrap_host}:{self.bootstrap_port}",
            bootstrap_room=req.bootstrap_room,
            dest_tp_ranks=dest_tp_ranks,
            pp_rank=self.pp_rank,
        )
        self._process_req(req)
        self.queue.append(req)

    def extend(self, reqs: List[Req], num_kv_heads: int) -> None:
        """æ‰¹é‡æ·»åŠ è¯·æ±‚"""
        for req in reqs:
            self.add(req, num_kv_heads)

    def _check_if_req_exceed_kv_capacity(self, req: Req) -> bool:
        """æ£€æŸ¥è¯·æ±‚æ˜¯å¦è¶…å‡ºKVå®¹é‡é™åˆ¶"""
        if len(req.origin_input_ids) > self.max_total_num_tokens:
            message = f"Request {req.rid} exceeds the maximum number of tokens: {len(req.origin_input_ids)} > {self.max_total_num_tokens}"
            logger.error(message)
            prepare_abort(req, message, status_code=HTTPStatus.BAD_REQUEST)
            self.scheduler.stream_output([req], req.return_logprob)
            return True
        return False

    def _process_req(self, req: Req) -> None:
        """å¤„ç†è¯·æ±‚ï¼Œè®¾ç½®max_new_tokens=1ä»¥ä¾¿PrefillAdderå†…å­˜ä¼°ç®—å‡†ç¡®"""
        req.sampling_params.max_new_tokens = 1
```

### ğŸ”„ é¢„å¡«å……äº‹ä»¶å¾ªç¯

é¢„å¡«å……æœåŠ¡å™¨æœ‰ä¸“é—¨çš„äº‹ä»¶å¾ªç¯ï¼š

```python
@DynamicGradMode()
def event_loop_normal_disagg_prefill(self: Scheduler):
    """é¢„å¡«å……æœåŠ¡å™¨çš„æ ‡å‡†äº‹ä»¶å¾ªç¯"""
    while True:
        # æ¥æ”¶å’Œå¤„ç†è¯·æ±‚
        recv_reqs = self.recv_requests()
        self.process_input_requests(recv_reqs)

        # ä»å¯åŠ¨é˜Ÿåˆ—è·å–å¯è¿è¡Œçš„è¯·æ±‚
        can_run_list = self.disagg_prefill_bootstrap_queue.pop_bootstrapped()
        
        if can_run_list:
            # åˆ›å»ºé¢„å¡«å……æ‰¹æ¬¡
            batch = self.get_new_batch_prefill_from_disagg_queue(can_run_list)
            if batch:
                result = self.run_batch(batch)
                self.process_batch_result(batch, result)
        else:
            self.self_check_during_idle()

@DynamicGradMode()
def event_loop_overlap_disagg_prefill(self: Scheduler):
    """é¢„å¡«å……æœåŠ¡å™¨çš„é‡å äº‹ä»¶å¾ªç¯"""
    self.result_queue = deque()
    
    while True:
        # æ¥æ”¶å’Œå¤„ç†è¯·æ±‚
        recv_reqs = self.recv_requests()
        self.process_input_requests(recv_reqs)

        # è·å–å¯è¿è¡Œè¯·æ±‚å¹¶åˆ›å»ºæ‰¹æ¬¡
        can_run_list = self.disagg_prefill_bootstrap_queue.pop_bootstrapped()
        batch = None
        if can_run_list:
            batch = self.get_new_batch_prefill_from_disagg_queue(can_run_list)

        if batch:
            batch.launch_done = threading.Event()
            result = self.run_batch(batch)
            self.result_queue.append((batch.copy(), result))

        # å¤„ç†ä¸Šä¸€æ‰¹æ¬¡çš„ç»“æœ
        if self.last_batch:
            tmp_batch, tmp_result = self.result_queue.popleft()
            tmp_batch.next_batch_sampling_info = None
            self.process_batch_result(tmp_batch, tmp_result, 
                                    batch.launch_done if batch else None)
        elif batch is None:
            self.self_check_during_idle()

        self.last_batch = batch
```

---

## ğŸ¯ è§£ç åˆ†ç¦»æ¶æ„

### ğŸ“‹ è§£ç é˜Ÿåˆ—ç®¡ç†

è§£ç æœåŠ¡å™¨ä½¿ç”¨ä¸¤ä¸ªé˜Ÿåˆ—ç®¡ç†è¯·æ±‚ï¼š

```python
class DecodePreallocQueue:
    """è§£ç é¢„åˆ†é…é˜Ÿåˆ—"""
    
    def add(self, req: Req) -> None:
        """æ·»åŠ è¯·æ±‚åˆ°é¢„åˆ†é…é˜Ÿåˆ—"""
        self.queue.append(req)
    
    def extend(self, reqs: List[Req], is_retracted: bool = False) -> None:
        """æ‰¹é‡æ·»åŠ è¯·æ±‚"""
        if is_retracted:
            # å›é€€çš„è¯·æ±‚ä¼˜å…ˆå¤„ç†
            self.queue = reqs + self.queue
        else:
            self.queue.extend(reqs)

class DecodeTransferQueue:
    """è§£ç ä¼ è¾“é˜Ÿåˆ—"""
    
    def add(self, req: Req) -> None:
        """æ·»åŠ å·²å‡†å¤‡å¥½çš„è¯·æ±‚åˆ°ä¼ è¾“é˜Ÿåˆ—"""
        self.queue.append(req)
    
    def pop_ready(self) -> List[Req]:
        """å¼¹å‡ºå‡†å¤‡å¥½è¿›è¡Œè§£ç çš„è¯·æ±‚"""
        ready_reqs = []
        remaining_reqs = []
        
        for req in self.queue:
            if req.disagg_kv_receiver.is_ready():
                ready_reqs.append(req)
            else:
                remaining_reqs.append(req)
        
        self.queue = remaining_reqs
        return ready_reqs
```

### è§£ç æ‰¹æ¬¡åˆ›å»º

```python
def get_new_prebuilt_batch(self: Scheduler) -> Optional[ScheduleBatch]:
    """ä¸ºä¼ªé€ çš„å·²å®Œæˆé¢„å¡«å……åˆ›å»ºè°ƒåº¦æ‰¹æ¬¡"""
    if self.grammar_queue:
        self.move_ready_grammar_requests()

    if len(self.waiting_queue) == 0:
        return None

    curr_batch_size = self.running_batch.batch_size()
    batch_size = min(self.req_to_token_pool.size, self.max_running_requests)
    num_not_used_batch = batch_size - curr_batch_size

    # ä»ç­‰å¾…é˜Ÿåˆ—å¼¹å‡ºè¯·æ±‚
    can_run_list: List[Req] = []
    waiting_queue: List[Req] = []

    for i in range(len(self.waiting_queue)):
        req = self.waiting_queue[i]
        # åªèƒ½æ·»åŠ ä¸è¶…è¿‡num_not_used_batchçš„æ–°æ‰¹æ¬¡åˆ°è¿è¡Œé˜Ÿåˆ—
        if i < num_not_used_batch:
            can_run_list.append(req)
            req.init_next_round_input(self.tree_cache)
        else:
            waiting_queue.append(req)

    self.waiting_queue = waiting_queue
    if len(can_run_list) == 0:
        return None

    # ä½¿ç”¨è¿™äº›è¯·æ±‚æ„é€ è°ƒåº¦æ‰¹æ¬¡å¹¶æ ‡è®°ä¸ºè§£ç 
    new_batch = ScheduleBatch.init_new(
        can_run_list,
        self.req_to_token_pool,
        self.token_to_kv_pool_allocator,
        self.tree_cache,
        self.model_config,
        self.enable_overlap,
        self.spec_algorithm,
    )

    # æ„é€ ä¼ªé€ çš„å·²å®Œæˆé¢„å¡«å……
    new_batch.prepare_for_prebuilt_extend()
    new_batch.process_prebuilt_extend(self.server_args, self.model_config)

    return new_batch
```

### è§£ç äº‹ä»¶å¾ªç¯

```python
@DynamicGradMode()
def event_loop_normal_disagg_decode(self: Scheduler):
    """è§£ç æœåŠ¡å™¨çš„æ ‡å‡†äº‹ä»¶å¾ªç¯"""
    while True:
        # å¤„ç†è§£ç é˜Ÿåˆ—
        self.process_decode_queue()
        
        # æ¥æ”¶å’Œå¤„ç†è¯·æ±‚
        recv_reqs = self.recv_requests()
        self.process_input_requests(recv_reqs)

        # è·å–ä¸‹ä¸€ä¸ªè¦è¿è¡Œçš„æ‰¹æ¬¡
        batch = self.get_next_disagg_decode_batch_to_run()
        self.cur_batch = batch

        if batch:
            result = self.run_batch(batch)
            self.process_batch_result(batch, result)
        else:
            self.self_check_during_idle()

        self.last_batch = batch

@DynamicGradMode()
def event_loop_overlap_disagg_decode(self: Scheduler):
    """è§£ç æœåŠ¡å™¨çš„é‡å äº‹ä»¶å¾ªç¯"""
    result_queue = deque()
    
    while True:
        # å¤„ç†è§£ç é˜Ÿåˆ—
        self.process_decode_queue()
        
        # æ¥æ”¶å’Œå¤„ç†è¯·æ±‚
        recv_reqs = self.recv_requests()
        self.process_input_requests(recv_reqs)

        # è·å–æ‰¹æ¬¡
        batch = self.get_next_disagg_decode_batch_to_run()
        self.cur_batch = batch

        if batch:
            batch.launch_done = threading.Event()
            result = self.run_batch(batch)
            result_queue.append((batch.copy(), result))

        # å¤„ç†ä¸Šä¸€æ‰¹æ¬¡ç»“æœ
        if self.last_batch:
            tmp_batch, tmp_result = result_queue.popleft()
            tmp_batch.next_batch_sampling_info = (
                self.tp_worker.cur_sampling_info if batch else None
            )
            self.process_batch_result(
                tmp_batch, tmp_result, batch.launch_done if batch else None
            )
        elif batch is None:
            self.self_check_during_idle()

        self.last_batch = batch
```

## KVä¼ è¾“æœºåˆ¶

### KVå‘é€å™¨å’Œæ¥æ”¶å™¨

åˆ†ç¦»å¼æ¶æ„é€šè¿‡KVå‘é€å™¨å’Œæ¥æ”¶å™¨åœ¨é¢„å¡«å……å’Œè§£ç æœåŠ¡å™¨ä¹‹é—´ä¼ è¾“KVç¼“å­˜ï¼š

```python
class BaseKVSender:
    """KVç¼“å­˜å‘é€å™¨åŸºç±»"""
    
    def send_kv_cache(self, req: Req, kv_data: torch.Tensor):
        """å‘é€KVç¼“å­˜æ•°æ®åˆ°è§£ç æœåŠ¡å™¨"""
        raise NotImplementedError

class BaseKVReceiver:
    """KVç¼“å­˜æ¥æ”¶å™¨åŸºç±»"""
    
    def receive_kv_cache(self, req: Req) -> torch.Tensor:
        """ä»é¢„å¡«å……æœåŠ¡å™¨æ¥æ”¶KVç¼“å­˜æ•°æ®"""
        raise NotImplementedError
    
    def is_ready(self) -> bool:
        """æ£€æŸ¥KVç¼“å­˜æ˜¯å¦å‡†å¤‡å°±ç»ª"""
        raise NotImplementedError
```

### ä¼ è¾“åç«¯

SGLangæ”¯æŒå¤šç§ä¼ è¾“åç«¯ï¼š

```python
class TransferBackend(Enum):
    FAKE = "fake"           # å‡ä¼ è¾“ï¼ˆæµ‹è¯•ç”¨ï¼‰
    NCCL = "nccl"          # NCCLä¼ è¾“
    GLOO = "gloo"          # Glooä¼ è¾“
    TCP = "tcp"            # TCPä¼ è¾“
```

## ç›‘æ§å’Œè°ƒè¯•

åˆ†ç¦»å¼æ¶æ„éœ€è¦é¢å¤–çš„ç›‘æ§æŒ‡æ ‡ï¼š

```python
def log_decode_stats(self, can_run_cuda_graph: bool, running_batch: ScheduleBatch = None):
    """è®°å½•è§£ç é˜¶æ®µç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…æ‹¬åˆ†ç¦»å¼æ¶æ„æŒ‡æ ‡"""
    
    # å¤„ç†åˆ†ç¦»å¼æ¶æ„çš„é¢å¤–æŒ‡æ ‡
    if self.disaggregation_mode == DisaggregationMode.DECODE:
        self.stats.num_decode_prealloc_queue_reqs = len(
            self.disagg_decode_prealloc_queue.queue
        )
        self.stats.num_decode_transfer_queue_reqs = len(
            self.disagg_decode_transfer_queue.queue
        )
    
    # å‘é€æ ‡å‡†æŒ‡æ ‡
    self.metrics_collector.log_stats(self.stats)
```

## éƒ¨ç½²é…ç½®

### é¢„å¡«å……æœåŠ¡å™¨é…ç½®

```python
server_args = ServerArgs(
    model_path="model_name",
    disaggregation_mode="prefill",
    disaggregation_bootstrap_port=8001,
    kv_transfer_backend="nccl",
    # å…¶ä»–é¢„å¡«å……æœåŠ¡å™¨é…ç½®...
)
```

### è§£ç æœåŠ¡å™¨é…ç½®

```python
server_args = ServerArgs(
    model_path="model_name", 
    disaggregation_mode="decode",
    disaggregation_bootstrap_port=8001,
    kv_transfer_backend="nccl",
    # å…¶ä»–è§£ç æœåŠ¡å™¨é…ç½®...
)
```

## æ€»ç»“

SGLangçš„åˆ†ç¦»å¼æ¶æ„æ”¯æŒå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

**æ ¸å¿ƒåŠŸèƒ½**:
- é¢„å¡«å……å’Œè§£ç è¿‡ç¨‹å®Œå…¨åˆ†ç¦»
- ä¸“é—¨çš„é˜Ÿåˆ—ç®¡ç†ç³»ç»Ÿ
- é«˜æ•ˆçš„KVç¼“å­˜ä¼ è¾“æœºåˆ¶
- å¤šç§ä¼ è¾“åç«¯æ”¯æŒ

**æ¶æ„ä¼˜åŠ¿**:
- æ”¯æŒè¶…å¤§æ¨¡å‹çš„åˆ†å¸ƒå¼æ¨ç†
- æé«˜èµ„æºåˆ©ç”¨ç‡å’Œç³»ç»Ÿå¯æ‰©å±•æ€§
- ç‹¬ç«‹çš„äº‹ä»¶å¾ªç¯ä¼˜åŒ–ä¸åŒé˜¶æ®µçš„æ€§èƒ½
- å®Œæ•´çš„ç›‘æ§å’Œè°ƒè¯•æ”¯æŒ

**é€‚ç”¨åœºæ™¯**:
- è¶…å¤§è¯­è¨€æ¨¡å‹æ¨ç†ï¼ˆå¦‚175B+å‚æ•°æ¨¡å‹ï¼‰
- èµ„æºå—é™çš„éƒ¨ç½²ç¯å¢ƒ
- éœ€è¦ç‹¬ç«‹æ‰©å±•é¢„å¡«å……å’Œè§£ç èƒ½åŠ›çš„åœºæ™¯
- å¤šç§Ÿæˆ·éƒ¨ç½²ç¯å¢ƒ

åˆ†ç¦»å¼æ¶æ„æ˜¯SGLangæ”¯æŒè¶…å¤§æ¨¡å‹æ¨ç†çš„é‡è¦ç‰¹æ€§ï¼Œé€šè¿‡å°†è®¡ç®—å¯†é›†çš„é¢„å¡«å……å’Œå†…å­˜å¯†é›†çš„è§£ç åˆ†ç¦»ï¼Œå®ç°äº†æ›´å¥½çš„èµ„æºåˆ©ç”¨å’Œç³»ç»Ÿæ‰©å±•æ€§ã€‚
