# è°ƒåº¦å™¨æ€»è§ˆä¸æ¶æ„

---

SGLangçš„è°ƒåº¦å™¨æ˜¯æ•´ä¸ªæ¨ç†ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£ç®¡ç†å¼ é‡å¹¶è¡ŒGPU workerçš„å·¥ä½œåè°ƒã€‚ä½œä¸ºç³»ç»Ÿçš„"å¤§è„‘"ï¼Œè°ƒåº¦å™¨æ‰¿æ‹…ç€è¯·æ±‚è°ƒåº¦ã€æ‰¹å¤„ç†ä¼˜åŒ–ã€èµ„æºç®¡ç†å’Œå¹¶è¡Œåè°ƒç­‰å…³é”®èŒè´£ã€‚

---

## ğŸ¯ æ ¸å¿ƒèŒè´£

è°ƒåº¦å™¨çš„ä¸»è¦èŒè´£åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š

**è¯·æ±‚ç”Ÿå‘½å‘¨æœŸç®¡ç†**  
ä»æ¥æ”¶ç”¨æˆ·è¯·æ±‚å¼€å§‹ï¼Œè°ƒåº¦å™¨è´Ÿè´£æ•´ä¸ªè¯·æ±‚çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ŒåŒ…æ‹¬è¯·æ±‚éªŒè¯ã€æ’é˜Ÿè°ƒåº¦ã€æ‰¹æ¬¡ç»„ç»‡ã€æ¨ç†æ‰§è¡Œå’Œç»“æœè¿”å›çš„å…¨æµç¨‹æ§åˆ¶ã€‚

**æ‰¹å¤„ç†ä¼˜åŒ–**  
è°ƒåº¦å™¨å®ç°äº†åŠ¨æ€æ‰¹å¤„ç†æœºåˆ¶ï¼Œèƒ½å¤Ÿæ™ºèƒ½åœ°å°†å¤šä¸ªè¯·æ±‚ç»„ç»‡æˆæ‰¹æ¬¡è¿›è¡Œå¤„ç†ï¼Œæœ€å¤§åŒ–GPUçš„åˆ©ç”¨ç‡å’Œç³»ç»Ÿååé‡ã€‚

**èµ„æºç»Ÿä¸€è°ƒåº¦**  
è°ƒåº¦å™¨ç»Ÿä¸€ç®¡ç†GPUå†…å­˜ã€KVç¼“å­˜ã€è®¡ç®—èµ„æºç­‰ç³»ç»Ÿèµ„æºï¼Œç¡®ä¿èµ„æºçš„é«˜æ•ˆåˆ†é…å’Œä½¿ç”¨ã€‚

**å¤šç»´å¹¶è¡Œåè°ƒ**  
åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹ï¼Œè°ƒåº¦å™¨åè°ƒå¼ é‡å¹¶è¡Œã€æµæ°´çº¿å¹¶è¡Œã€æ•°æ®å¹¶è¡Œå’Œä¸“å®¶å¹¶è¡Œç­‰å¤šç§å¹¶è¡Œç­–ç•¥ã€‚

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### ğŸ§© Mixinæ¨¡å¼è®¾è®¡

SGLangè°ƒåº¦å™¨é‡‡ç”¨å¤šé‡ç»§æ‰¿çš„Mixinæ¨¡å¼ï¼Œå°†ä¸åŒåŠŸèƒ½æ¨¡å—è§£è€¦ï¼Œå®ç°é«˜åº¦çš„æ¨¡å—åŒ–è®¾è®¡ï¼š

```python
class Scheduler(
    SchedulerOutputProcessorMixin,
    SchedulerUpdateWeightsMixin,
    SchedulerProfilerMixin,
    SchedulerMetricsMixin,
    SchedulerDisaggregationDecodeMixin,
    SchedulerDisaggregationPrefillMixin,
):
    """A scheduler that manages a tensor parallel GPU worker."""
    
    def __init__(self, server_args: ServerArgs, port_args: PortArgs, 
                 gpu_id: int, tp_rank: int, moe_ep_rank: int, 
                 pp_rank: int, dp_rank: Optional[int]):
        # è§£æå¹¶è¡Œé…ç½®å‚æ•°
        self.server_args = server_args
        self.tp_rank = tp_rank              # å¼ é‡å¹¶è¡Œrank
        self.moe_ep_rank = moe_ep_rank      # ä¸“å®¶å¹¶è¡Œrank  
        self.pp_rank = pp_rank              # æµæ°´çº¿å¹¶è¡Œrank
        self.dp_rank = dp_rank              # æ•°æ®å¹¶è¡Œrank
        self.tp_size = server_args.tp_size  # å¼ é‡å¹¶è¡Œå¤§å°
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.running_batch = ScheduleBatch(reqs=[], batch_is_full=False)
        self.waiting_queue = []
        self.forward_ct = 0
        
        # åˆå§‹åŒ–è¯·æ±‚åˆ†å‘å™¨
        self._request_dispatcher = TypeBasedDispatcher([
            (TokenizedGenerateReqInput, self.handle_generate_request),
            (TokenizedEmbeddingReqInput, self.handle_embedding_request),
            (FlushCacheReqInput, self.flush_cache_wrapped),
            # ... æ›´å¤šè¯·æ±‚ç±»å‹æ˜ å°„
        ])
```

æ¯ä¸ªMixinç±»è´Ÿè´£ç‰¹å®šçš„åŠŸèƒ½é¢†åŸŸï¼Œè¿™ç§è®¾è®¡å¸¦æ¥äº†è‰¯å¥½çš„ä»£ç ç»„ç»‡å’Œå¯ç»´æŠ¤æ€§ï¼š

**SchedulerOutputProcessorMixin**: å¤„ç†æ¨¡å‹è¾“å‡ºå’Œæµå¼å“åº”ï¼ŒåŒ…æ‹¬é¢„å¡«å……å’Œè§£ç ç»“æœçš„å¤„ç†ã€æµå¼ä¼ è¾“æ§åˆ¶ç­‰ã€‚

**SchedulerUpdateWeightsMixin**: æ”¯æŒåŠ¨æ€æƒé‡æ›´æ–°ï¼ŒåŒ…æ‹¬ä»ç£ç›˜åŠ è½½æƒé‡ã€åˆ†å¸ƒå¼æƒé‡æ›´æ–°ã€å¼ é‡æƒé‡æ›´æ–°ç­‰åŠŸèƒ½ï¼š

```python
def update_weights_from_disk(self, recv_req: UpdateWeightFromDiskReqInput):
    """ä»ç£ç›˜å°±åœ°æ›´æ–°æƒé‡"""
    success, message = self.tp_worker.update_weights_from_disk(recv_req)
    if success:
        flush_cache_success = self.flush_cache()
        assert flush_cache_success, "æƒé‡æ›´æ–°åç¼“å­˜åˆ·æ–°å¤±è´¥"
    return UpdateWeightFromDiskReqOutput(success, message, 0)
```

**SchedulerProfilerMixin**: æä¾›æ€§èƒ½åˆ†æåŠŸèƒ½ï¼Œæ”¯æŒPyTorch profileré›†æˆå’Œè‡ªå®šä¹‰æ€§èƒ½æŒ‡æ ‡æ”¶é›†ã€‚

**SchedulerMetricsMixin**: è´Ÿè´£æŒ‡æ ‡æ”¶é›†ï¼ŒåŒ…æ‹¬ååé‡ã€å»¶è¿Ÿã€ç¼“å­˜å‘½ä¸­ç‡ç­‰å…³é”®æ€§èƒ½æŒ‡æ ‡çš„ç»Ÿè®¡ã€‚

**SchedulerDisaggregationDecodeMixin/PrefillMixin**: æ”¯æŒé¢„å¡«å……å’Œè§£ç åˆ†ç¦»çš„åˆ†ç¦»å¼æ¶æ„ï¼Œç”¨äºè¶…å¤§æ¨¡å‹çš„åˆ†å¸ƒå¼æ¨ç†ã€‚

### âš™ï¸ å¹¶è¡Œé…ç½®ç®¡ç†

è°ƒåº¦å™¨çš„åˆå§‹åŒ–è¿‡ç¨‹ä¸­ï¼Œéœ€è¦å¤„ç†å¤æ‚çš„å¹¶è¡Œé…ç½®ã€‚åœ¨__init__æ–¹æ³•ä¸­ï¼Œè°ƒåº¦å™¨è§£æå„ç§å¹¶è¡Œå‚æ•°ï¼š

```python
def __init__(self, server_args: ServerArgs, port_args: PortArgs, gpu_id: int,
             tp_rank: int, moe_ep_rank: int, pp_rank: int, dp_rank: Optional[int]):
    # è§£æå¹¶è¡Œé…ç½®
    self.tp_rank = tp_rank              # å¼ é‡å¹¶è¡Œrank
    self.moe_ep_rank = moe_ep_rank      # ä¸“å®¶å¹¶è¡Œrank
    self.pp_rank = pp_rank              # æµæ°´çº¿å¹¶è¡Œrank
    self.dp_rank = dp_rank              # æ•°æ®å¹¶è¡Œrank
    self.tp_size = server_args.tp_size  # å¼ é‡å¹¶è¡Œå¤§å°
    self.moe_ep_size = server_args.ep_size
    self.pp_size = server_args.pp_size
    self.dp_size = server_args.dp_size
```

è¿™äº›å‚æ•°å®šä¹‰äº†å½“å‰è°ƒåº¦å™¨å®ä¾‹åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„ä½ç½®å’Œä½œç”¨èŒƒå›´ï¼Œæ˜¯åç»­æ‰€æœ‰å¹¶è¡Œåè°ƒå·¥ä½œçš„åŸºç¡€ã€‚

---

## ğŸ”„ äº‹ä»¶å¾ªç¯æ¶æ„

è°ƒåº¦å™¨çš„æ ¸å¿ƒå·¥ä½œæœºåˆ¶æ˜¯äº‹ä»¶å¾ªç¯ï¼Œæ ¹æ®ä¸åŒçš„é…ç½®å’Œä¼˜åŒ–éœ€æ±‚ï¼ŒSGLangæä¾›äº†å¤šç§äº‹ä»¶å¾ªç¯å®ç°ï¼š

**æ ‡å‡†äº‹ä»¶å¾ªç¯**  
event_loop_normalæ˜¯æœ€åŸºç¡€çš„äº‹ä»¶å¾ªç¯å®ç°ï¼Œé‡‡ç”¨ä¸²è¡Œå¤„ç†æ¨¡å¼ï¼Œé€»è¾‘æ¸…æ™°ï¼Œé€‚åˆè°ƒè¯•å’Œå¼€å‘ç¯å¢ƒã€‚

**é‡å äº‹ä»¶å¾ªç¯**  
event_loop_overlapå®ç°äº†CPUå¤„ç†å’ŒGPUè®¡ç®—çš„é‡å ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡ç³»ç»Ÿååé‡ï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒã€‚

**æµæ°´çº¿å¹¶è¡Œäº‹ä»¶å¾ªç¯**  
event_loop_ppä¸“é—¨ä¸ºæµæ°´çº¿å¹¶è¡Œè®¾è®¡ï¼Œæ”¯æŒå¤§æ¨¡å‹çš„åˆ†å±‚å¤„ç†ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨å¤šä¸ªGPUèŠ‚ç‚¹ã€‚

**åˆ†ç¦»å¼äº‹ä»¶å¾ªç¯**  
é’ˆå¯¹é¢„å¡«å……å’Œè§£ç åˆ†ç¦»çš„åœºæ™¯ï¼Œæä¾›äº†ä¸“é—¨çš„äº‹ä»¶å¾ªç¯å®ç°ï¼ŒåŒ…æ‹¬event_loop_overlap_disagg_prefillå’Œevent_loop_normal_disagg_decodeç­‰ã€‚

### ğŸ›ï¸ äº‹ä»¶å¾ªç¯é€‰æ‹©é€»è¾‘

è°ƒåº¦å™¨æ ¹æ®é…ç½®å‚æ•°è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„äº‹ä»¶å¾ªç¯ï¼š

```python
# äº‹ä»¶å¾ªç¯é€‰æ‹©é€»è¾‘ (run_scheduler_process)
disaggregation_mode = scheduler.disaggregation_mode
if disaggregation_mode == DisaggregationMode.NULL:
    if server_args.pp_size > 1:
        scheduler.event_loop_pp()          # æµæ°´çº¿å¹¶è¡Œäº‹ä»¶å¾ªç¯
    elif scheduler.enable_overlap:
        scheduler.event_loop_overlap()     # CPU-GPUé‡å äº‹ä»¶å¾ªç¯
    else:
        scheduler.event_loop_normal()      # æ ‡å‡†äº‹ä»¶å¾ªç¯
elif disaggregation_mode == DisaggregationMode.PREFILL:
    if scheduler.enable_overlap:
        scheduler.event_loop_overlap_disagg_prefill()
    else:
        scheduler.event_loop_normal_disagg_prefill()
elif disaggregation_mode == DisaggregationMode.DECODE:
    if scheduler.enable_overlap:
        scheduler.event_loop_overlap_disagg_decode()
    else:
        scheduler.event_loop_normal_disagg_decode()
```

**æ ‡å‡†äº‹ä»¶å¾ªç¯ç¤ºä¾‹**ï¼š
```python
@DynamicGradMode()
def event_loop_normal(self):
    """æ ‡å‡†è°ƒåº¦å™¨å¾ªç¯"""
    while True:
        # 1. æ¥æ”¶æ–°è¯·æ±‚
        recv_reqs = self.recv_requests()
        self.process_input_requests(recv_reqs)

        # 2. è·å–ä¸‹ä¸€ä¸ªæ‰¹æ¬¡
        batch = self.get_next_batch_to_run()
        self.cur_batch = batch

        # 3. æ‰§è¡Œæ¨ç†æˆ–ç©ºé—²æ£€æŸ¥
        if batch:
            result = self.run_batch(batch)
            self.process_batch_result(batch, result)
        else:
            self.self_check_during_idle()

        self.last_batch = batch
```

è¿™ç§è‡ªåŠ¨é€‰æ‹©æœºåˆ¶ç¡®ä¿äº†è°ƒåº¦å™¨èƒ½å¤Ÿæ ¹æ®éƒ¨ç½²é…ç½®é€‰æ‹©æœ€ä¼˜çš„æ‰§è¡Œæ¨¡å¼ã€‚

---

## ğŸ“¦ è¯·æ±‚åˆ†å‘æœºåˆ¶

### ğŸ”€ TypeBasedDispatcheræ¶æ„

è°ƒåº¦å™¨ä½¿ç”¨TypeBasedDispatcheræ¥åˆ†å‘ä¸åŒç±»å‹çš„è¯·æ±‚ï¼š

```python
# åˆå§‹åŒ–è¯·æ±‚åˆ†å‘å™¨
self._request_dispatcher = TypeBasedDispatcher([
    (TokenizedGenerateReqInput, self.handle_generate_request),
    (TokenizedEmbeddingReqInput, self.handle_embedding_request),
    (FlushCacheReqInput, self.flush_cache_wrapped),
    (AbortReq, self.abort_request),
    (OpenSessionReqInput, self.open_session),
    (CloseSessionReqInput, self.close_session),
    (UpdateWeightFromDiskReqInput, self.update_weights_from_disk),
    (InitWeightsUpdateGroupReqInput, self.init_weights_update_group),
    (UpdateWeightsFromDistributedReqInput, self.update_weights_from_distributed),
    (UpdateWeightsFromTensorReqInput, self.update_weights_from_tensor),
    (GetWeightsByNameReqInput, self.get_weights_by_name),
    (ReleaseMemoryOccupationReqInput, self.release_memory_occupation),
    (ResumeMemoryOccupationReqInput, self.resume_memory_occupation),
    (SlowDownReqInput, self.slow_down),
    (ProfileReq, self.profile),
    (LoadLoRAAdapterReqInput, self.load_lora_adapter),
    (UnloadLoRAAdapterReqInput, self.unload_lora_adapter),
])
```

è¿™ç§è®¾è®¡æ”¯æŒäº†åå¤šç§ä¸åŒç±»å‹çš„è¯·æ±‚ï¼Œä»åŸºæœ¬çš„ç”Ÿæˆå’ŒåµŒå…¥è¯·æ±‚ï¼Œåˆ°é«˜çº§çš„æƒé‡æ›´æ–°ã€æ€§èƒ½åˆ†æã€LoRAé€‚é…å™¨ç®¡ç†ç­‰åŠŸèƒ½ã€‚

---

## âš¡ æ ¸å¿ƒæ–¹æ³•è§£æ

### ğŸ“¥ è¯·æ±‚å¤„ç†æµç¨‹

è°ƒåº¦å™¨é€šè¿‡ä¸€ç³»åˆ—æ ¸å¿ƒæ–¹æ³•å¤„ç†è¯·æ±‚ï¼š

**recv_requestsæ–¹æ³•**è´Ÿè´£ä»ç½‘ç»œæ¥æ”¶æ–°çš„è¯·æ±‚ï¼Œæ”¯æŒåˆ†å¸ƒå¼ç¯å¢ƒä¸‹çš„è¯·æ±‚å¹¿æ’­å’ŒåŒæ­¥ã€‚

```python
def recv_requests(self) -> List[Req]:
    """æ¥æ”¶æ¥è‡ªtokenizerçš„è¯·æ±‚"""
    recv_reqs = []
    while True:
        try:
            recv_req = self.recv_from_tokenizer.recv_pyobj(zmq.NOBLOCK)
            recv_reqs.append(recv_req)
        except zmq.ZMQError:
            break
    return recv_reqs
```

**process_input_requestsæ–¹æ³•**å¤„ç†æ¥æ”¶åˆ°çš„è¯·æ±‚ï¼ŒåŒ…æ‹¬è¯·æ±‚éªŒè¯ã€ç±»å‹åˆ†å‘å’Œå¼‚å¸¸å¤„ç†ã€‚

**handle_generate_requestæ–¹æ³•**ä¸“é—¨å¤„ç†ç”Ÿæˆç±»è¯·æ±‚ï¼Œåˆ›å»ºReqå¯¹è±¡å¹¶åŠ å…¥è°ƒåº¦é˜Ÿåˆ—ï¼š

```python
def handle_generate_request(self, recv_req: TokenizedGenerateReqInput):
    """å¤„ç†æ–‡æœ¬ç”Ÿæˆè¯·æ±‚"""
    # åˆ›å»ºæ–°çš„Reqå¯¹è±¡
    req = Req(
        recv_req.rid,
        recv_req.input_text,
        recv_req.input_ids,
        recv_req.sampling_params,
        return_logprob=recv_req.return_logprob,
        stream=recv_req.stream,
        lora_id=recv_req.lora_id,
    )
    req.tokenizer = self.tokenizer
    
    # æ·»åŠ åˆ°ç­‰å¾…é˜Ÿåˆ—
    self.waiting_queue.append(req)
```

**handle_embedding_requestæ–¹æ³•**å¤„ç†åµŒå…¥ç±»è¯·æ±‚ï¼Œæ”¯æŒå‘é‡æ£€ç´¢ç­‰åº”ç”¨åœºæ™¯ã€‚

### ğŸ”„ æ‰¹å¤„ç†ç®¡ç†

**get_next_batch_to_runæ–¹æ³•**æ˜¯æ‰¹å¤„ç†è°ƒåº¦çš„æ ¸å¿ƒï¼Œè´Ÿè´£ä»ç­‰å¾…é˜Ÿåˆ—ä¸­é€‰æ‹©è¯·æ±‚ç»„æˆæ‰¹æ¬¡ï¼š

```python
def get_next_batch_to_run(self) -> Optional[ScheduleBatch]:
    # åˆå¹¶é¢„å¡«å……æ‰¹æ¬¡åˆ°è¿è¡Œæ‰¹æ¬¡
    if self.last_batch and self.last_batch.forward_mode.is_extend():
        if not self.last_batch.is_empty():
            if self.running_batch.is_empty():
                self.running_batch = self.last_batch
            else:
                self.running_batch.merge_batch(self.last_batch)

    # è·å–æ–°çš„é¢„å¡«å……æ‰¹æ¬¡
    new_batch = self.get_new_batch_prefill()
    
    return new_batch or self.running_batch
```

**get_new_batch_prefillæ–¹æ³•**åˆ›å»ºæ–°çš„é¢„å¡«å……æ‰¹æ¬¡ï¼Œå®ç°åŠ¨æ€æ‰¹å¤„ç†å’Œè¿ç»­æ‰¹å¤„ç†ã€‚

**run_batchæ–¹æ³•**æ‰§è¡Œæ‰¹æ¬¡æ¨ç†ï¼Œè°ƒç”¨åº•å±‚çš„æ¨¡å‹è¿è¡Œå™¨è¿›è¡ŒGPUè®¡ç®—ï¼š

```python
def run_batch(self, batch: ScheduleBatch):
    """æ‰§è¡Œæ‰¹æ¬¡æ¨ç†"""
    if batch.forward_mode.is_decode() or batch.forward_mode.is_mixed():
        # è§£ç é˜¶æ®µ
        model_worker_batch = batch.get_model_worker_batch()
        result = self.tp_worker.forward_batch_generation(model_worker_batch)
    else:
        # é¢„å¡«å……é˜¶æ®µ  
        result = self.tp_worker.forward_batch_generation(model_worker_batch)
    
    return result
```

**process_batch_resultæ–¹æ³•**å¤„ç†æ¨ç†ç»“æœï¼ŒåŒ…æ‹¬è¾“å‡ºè§£ç ã€æµå¼ä¼ è¾“å’Œè¯·æ±‚å®Œæˆæ£€æŸ¥ã€‚

### ğŸ”§ ç³»ç»Ÿç»´æŠ¤

**self_check_during_idleæ–¹æ³•**åœ¨ç³»ç»Ÿç©ºé—²æ—¶æ‰§è¡Œè‡ªæ£€å’ŒçŠ¶æ€é‡ç½®ï¼Œç¡®ä¿ç³»ç»Ÿçš„é•¿æœŸç¨³å®šè¿è¡Œï¼š

```python
def self_check_during_idle(self):
    """ç³»ç»Ÿç©ºé—²æ—¶çš„è‡ªæ£€å’Œç»´æŠ¤"""
    # æ‰§è¡Œå†…å­˜æ³„æ¼æ£€æŸ¥
    self.check_memory()
    
    # å¦‚æœæœ‰ç©ºé—²ç¡çœ å™¨ï¼Œå¯èƒ½è¿›å…¥ç¡çœ èŠ‚èƒ½æ¨¡å¼
    self.maybe_sleep_on_idle()
    
    # é‡ç½®ä¸€äº›ç»Ÿè®¡è®¡æ•°å™¨
    if self.forward_ct % 1000 == 0:
        logger.info(f"Forward count: {self.forward_ct}")
```

**flush_cacheæ–¹æ³•**æä¾›ç¼“å­˜æ¸…ç†åŠŸèƒ½ï¼Œæ”¯æŒç³»ç»ŸçŠ¶æ€é‡ç½®ï¼š

```python
def flush_cache(self) -> bool:
    """åˆ·æ–°æ‰€æœ‰ç¼“å­˜ï¼Œé‡ç½®è°ƒåº¦å™¨çŠ¶æ€"""
    try:
        # æ¸…ç©ºæ‰€æœ‰é˜Ÿåˆ—
        self.waiting_queue.clear()
        self.running_batch = ScheduleBatch(reqs=[], batch_is_full=False)
        
        # åˆ·æ–°KVç¼“å­˜
        if hasattr(self, 'tree_cache'):
            self.tree_cache.reset()
        
        # é‡ç½®å†…å­˜æ± 
        if hasattr(self, 'req_to_token_pool'):
            self.req_to_token_pool.clear()
            
        # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
        self.forward_ct = 0
        
        logger.info("Cache flushed successfully")
        return True
    except Exception as e:
        logger.error(f"Cache flush failed: {e}")
        return False
```

**watchdog_threadæ–¹æ³•**å®ç°çœ‹é—¨ç‹—æœºåˆ¶ï¼Œç›‘æ§ç³»ç»Ÿå¥åº·çŠ¶æ€ï¼š

```python
def watchdog_thread(self):
    """çœ‹é—¨ç‹—çº¿ç¨‹ï¼Œç›‘æ§è°ƒåº¦å™¨å¥åº·çŠ¶æ€"""
    while True:
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰åƒµæ­»çš„è¯·æ±‚
            current_time = time.time()
            for req in self.running_batch.reqs:
                if current_time - req.start_time > self.request_timeout:
                    logger.warning(f"Request {req.rid} timeout, aborting")
                    self.abort_request(AbortReq(req.rid))
            
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
            if self.get_memory_usage() > 0.9:
                logger.warning("High memory usage detected")
                
            time.sleep(10)  # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
        except Exception as e:
            logger.error(f"Watchdog error: {e}")
```

---

## ğŸ’¾ å†…å­˜å’Œèµ„æºç®¡ç†

è°ƒåº¦å™¨ç»Ÿä¸€ç®¡ç†ç³»ç»Ÿçš„å†…å­˜å’Œè®¡ç®—èµ„æºã€‚åœ¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­ï¼Œè°ƒåº¦å™¨ä¼šåˆ›å»ºå’Œé…ç½®å„ç§èµ„æºç®¡ç†ç»„ä»¶ï¼š

```python
def init_memory_pool_and_cache(self):
    """åˆå§‹åŒ–å†…å­˜æ± å’Œç¼“å­˜ç³»ç»Ÿ"""
    # åˆå§‹åŒ–è¯·æ±‚åˆ°tokenæ˜ å°„æ± 
    self.req_to_token_pool = ReqToTokenPool(
        size=self.max_running_requests,
        pre_alloc_size=self.pre_alloc_size if self.pre_alloc_size else 0,
    )
    
    # åˆå§‹åŒ–KVç¼“å­˜åˆ†é…å™¨
    if self.is_hybrid:
        # æ··åˆç¼“å­˜æ¶æ„ï¼ˆSWAï¼‰
        self.token_to_kv_pool = HybridKVPoolAllocator(
            size=self.max_total_num_tokens,
            max_context_len=self.context_len,
            dtype=self.dtype,
        )
    else:
        # æ ‡å‡†ç¼“å­˜æ¶æ„
        self.token_to_kv_pool = BaseTokenToKVPoolAllocator(
            size=self.max_total_num_tokens,
            dtype=self.dtype,
        )
    
    # åˆå§‹åŒ–å‰ç¼€ç¼“å­˜
    if self.server_args.disable_radix_cache:
        self.tree_cache = None
    else:
        if self.enable_chunk_cache:
            self.tree_cache = ChunkCache(
                req_to_token_pool=self.req_to_token_pool,
                token_to_kv_pool=self.token_to_kv_pool,
            )
        else:
            self.tree_cache = RadixCache(
                req_to_token_pool=self.req_to_token_pool,
                token_to_kv_pool=self.token_to_kv_pool,
                disable=False,
            )
```

**èµ„æºåˆ†é…ç¤ºä¾‹**ï¼š
```python
def allocate_resources_for_request(self, req: Req):
    """ä¸ºè¯·æ±‚åˆ†é…èµ„æº"""
    # åˆ†é…tokenæ§½ä½
    if not self.req_to_token_pool.available():
        logger.warning("No available request slots")
        return False
        
    req_pool_idx = self.req_to_token_pool.alloc()
    req.req_pool_idx = req_pool_idx
    
    # åˆ†é…KVç¼“å­˜
    num_tokens = len(req.origin_input_ids)
    if self.token_to_kv_pool.available_size() < num_tokens:
        logger.warning("Insufficient KV cache space")
        self.req_to_token_pool.free(req_pool_idx)
        return False
    
    # ä¸ºè¯·æ±‚åˆ†é…tokenæ± 
    req.token_pool_indices = self.token_to_kv_pool.alloc(num_tokens)
    
    return True

def release_resources_for_request(self, req: Req):
    """é‡Šæ”¾è¯·æ±‚å ç”¨çš„èµ„æº"""
    if hasattr(req, 'req_pool_idx'):
        self.req_to_token_pool.free(req.req_pool_idx)
    
    if hasattr(req, 'token_pool_indices'):
        self.token_to_kv_pool.free(req.token_pool_indices)
```

è¿™äº›ç»„ä»¶ååŒå·¥ä½œï¼Œç¡®ä¿å†…å­˜çš„é«˜æ•ˆä½¿ç”¨å’Œç¼“å­˜çš„æ™ºèƒ½ç®¡ç†ï¼ŒåŒæ—¶æ”¯æŒè¯·æ±‚çš„åŠ¨æ€èµ„æºåˆ†é…å’Œå›æ”¶ã€‚

---

## ğŸŒ å¹¶è¡Œç­–ç•¥åè°ƒ

åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹ï¼Œè°ƒåº¦å™¨éœ€è¦åè°ƒå¤šç§å¹¶è¡Œç­–ç•¥ï¼š

### ğŸ”€ å¼ é‡å¹¶è¡Œåè°ƒ
```python
def coordinate_tensor_parallel(self):
    """å¼ é‡å¹¶è¡Œåè°ƒæœºåˆ¶"""
    # ç¡®ä¿æ‰€æœ‰TP workeråŒæ­¥
    if self.tp_size > 1:
        # å¹¿æ’­æ‰¹æ¬¡ä¿¡æ¯åˆ°æ‰€æœ‰TP ranks
        if self.tp_rank == 0:
            # ä¸»rankè´Ÿè´£è°ƒåº¦å†³ç­–
            batch_info = {
                'batch_size': len(self.running_batch.reqs),
                'forward_mode': self.running_batch.forward_mode,
                'request_ids': [req.rid for req in self.running_batch.reqs]
            }
            # å¹¿æ’­åˆ°å…¶ä»–ranks
            self.broadcast_to_tp_group(batch_info)
        else:
            # ä»å±ranksæ¥æ”¶è°ƒåº¦ä¿¡æ¯
            batch_info = self.receive_from_tp_master()
```

### ğŸš€ æµæ°´çº¿å¹¶è¡Œç®¡ç†
```python
def manage_pipeline_parallel(self):
    """æµæ°´çº¿å¹¶è¡Œç®¡ç†"""
    if self.pp_size > 1:
        # å¤„ç†pipeline stageé—´çš„æ•°æ®ä¼ é€’
        if self.pp_rank == 0:
            # ç¬¬ä¸€é˜¶æ®µï¼šå¤„ç†è¾“å…¥
            hidden_states = self.process_input_stage(batch)
            self.send_to_next_stage(hidden_states)
        elif self.pp_rank == self.pp_size - 1:
            # æœ€åé˜¶æ®µï¼šç”Ÿæˆè¾“å‡º
            hidden_states = self.receive_from_prev_stage()
            output = self.process_output_stage(hidden_states)
        else:
            # ä¸­é—´é˜¶æ®µï¼šä¼ é€’éšè—çŠ¶æ€
            hidden_states = self.receive_from_prev_stage()
            hidden_states = self.process_middle_stage(hidden_states)
            self.send_to_next_stage(hidden_states)
```

### âš–ï¸ æ•°æ®å¹¶è¡Œè´Ÿè½½å‡è¡¡
```python
def balance_data_parallel_load(self):
    """æ•°æ®å¹¶è¡Œè´Ÿè½½å‡è¡¡"""
    if self.dp_size > 1:
        # æ”¶é›†å„å‰¯æœ¬çš„è´Ÿè½½ä¿¡æ¯
        local_load = {
            'running_requests': len(self.running_batch.reqs),
            'waiting_requests': len(self.waiting_queue),
            'memory_usage': self.get_memory_usage()
        }
        
        # ä¸å…¶ä»–å‰¯æœ¬äº¤æ¢è´Ÿè½½ä¿¡æ¯
        all_loads = self.gather_dp_loads(local_load)
        
        # æ ¹æ®è´Ÿè½½å·®å¼‚è¿›è¡Œè¯·æ±‚é‡åˆ†é…
        if self.dp_rank == 0:  # ä¸»å‰¯æœ¬è´Ÿè´£è°ƒåº¦
            self.redistribute_requests(all_loads)
```

### ğŸ§  ä¸“å®¶å¹¶è¡Œè°ƒåº¦
```python
def schedule_expert_parallel(self, expert_distribution: Dict):
    """MoEä¸“å®¶å¹¶è¡Œè°ƒåº¦"""
    if self.moe_ep_size > 1:
        # æ ¹æ®tokenè·¯ç”±åˆ°ä¸åŒä¸“å®¶
        for req in self.running_batch.reqs:
            expert_ids = req.get_expert_routing()
            for expert_id in expert_ids:
                target_ep_rank = expert_id % self.moe_ep_size
                if target_ep_rank == self.moe_ep_rank:
                    # å½“å‰rankå¤„ç†è¿™ä¸ªä¸“å®¶
                    self.process_expert_tokens(req, expert_id)
                else:
                    # å‘é€åˆ°å¯¹åº”çš„ä¸“å®¶rank
                    self.send_to_expert_rank(req, expert_id, target_ep_rank)
```

è¿™äº›å¹¶è¡Œç­–ç•¥ç¡®ä¿äº†SGLangèƒ½å¤Ÿåœ¨å¤§è§„æ¨¡åˆ†å¸ƒå¼ç¯å¢ƒä¸‹é«˜æ•ˆè¿è¡Œï¼Œæœ€å¤§åŒ–ç¡¬ä»¶èµ„æºåˆ©ç”¨ç‡ã€‚

---

## ğŸ“Š æ€§èƒ½ç›‘æ§å’Œè°ƒè¯•

è°ƒåº¦å™¨å†…ç½®äº†å®Œå–„çš„æ€§èƒ½ç›‘æ§å’Œè°ƒè¯•åŠŸèƒ½ï¼š

### ğŸ“ˆ å…³é”®æŒ‡æ ‡æ”¶é›†
```python
def collect_performance_metrics(self):
    """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
    # é€šè¿‡SchedulerMetricsMixinæ”¶é›†æŒ‡æ ‡
    current_time = time.time()
    
    # ååé‡è®¡ç®—
    if hasattr(self, 'last_metric_time'):
        time_diff = current_time - self.last_metric_time
        tokens_processed = self.forward_ct - self.last_forward_ct
        self.last_gen_throughput = tokens_processed / time_diff if time_diff > 0 else 0
    
    # èµ„æºåˆ©ç”¨ç‡
    token_usage = self.get_token_usage_ratio()
    memory_usage = self.get_memory_usage_ratio()
    
    # é˜Ÿåˆ—çŠ¶æ€
    queue_stats = {
        'running_requests': len(self.running_batch.reqs),
        'waiting_requests': len(self.waiting_queue),
        'total_requests_processed': self.total_requests_processed,
    }
    
    # ç¼“å­˜å‘½ä¸­ç‡
    if self.tree_cache:
        cache_hit_rate = self.tree_cache.get_hit_rate()
    else:
        cache_hit_rate = 0.0
    
    # æ›´æ–°æŒ‡æ ‡
    self.stats.update(
        gen_throughput=self.last_gen_throughput,
        token_usage=token_usage,
        memory_usage=memory_usage,
        cache_hit_rate=cache_hit_rate,
        **queue_stats
    )
```

### ğŸ” æ€§èƒ½åˆ†æå·¥å…·
```python
def start_profiling(self, profile_config: ProfileConfig):
    """å¯åŠ¨æ€§èƒ½åˆ†æ"""
    # é€šè¿‡SchedulerProfilerMixinå¯åŠ¨profiler
    import torch.profiler as profiler
    
    self.torch_profiler = profiler.profile(
        activities=[
            profiler.ProfilerActivity.CPU,
            profiler.ProfilerActivity.CUDA,
        ],
        schedule=profiler.schedule(
            wait=profile_config.wait_steps,
            warmup=profile_config.warmup_steps, 
            active=profile_config.active_steps,
            repeat=profile_config.repeat
        ),
        on_trace_ready=self.save_profiler_trace,
        record_shapes=profile_config.record_shapes,
        with_stack=profile_config.with_stack,
    )
    
    self.torch_profiler.start()
    logger.info("Performance profiling started")

def save_profiler_trace(self, prof):
    """ä¿å­˜æ€§èƒ½åˆ†æç»“æœ"""
    output_path = f"{self.profiler_output_dir}/trace_{self.forward_ct}.json"
    prof.export_chrome_trace(output_path)
    logger.info(f"Profiler trace saved to {output_path}")
```

### ğŸ› ï¸ è°ƒè¯•å·¥å…·
```python
def dump_scheduler_state(self):
    """è½¬å‚¨è°ƒåº¦å™¨çŠ¶æ€ç”¨äºè°ƒè¯•"""
    state_info = {
        'timestamp': time.time(),
        'forward_count': self.forward_ct,
        'running_batch': {
            'size': len(self.running_batch.reqs),
            'forward_mode': str(self.running_batch.forward_mode),
            'requests': [req.rid for req in self.running_batch.reqs[:5]]  # åªæ˜¾ç¤ºå‰5ä¸ª
        },
        'waiting_queue': {
            'size': len(self.waiting_queue),
            'requests': [req.rid for req in self.waiting_queue[:5]]
        },
        'memory_stats': {
            'token_usage': self.get_token_usage_ratio(),
            'available_tokens': self.token_to_kv_pool.available_size(),
            'req_pool_usage': len(self.req_to_token_pool.free_slots)
        },
        'parallel_config': {
            'tp_rank': self.tp_rank,
            'tp_size': self.tp_size,
            'pp_rank': self.pp_rank,
            'dp_rank': self.dp_rank
        }
    }
    
    logger.info(f"Scheduler state dump: {json.dumps(state_info, indent=2)}")
    return state_info

def monitor_request_lifecycle(self, req: Req, event: str):
    """ç›‘æ§è¯·æ±‚ç”Ÿå‘½å‘¨æœŸ"""
    if self.enable_request_tracing:
        trace_info = {
            'request_id': req.rid,
            'event': event,
            'timestamp': time.time(),
            'queue_position': len(self.waiting_queue) if event == 'queued' else None,
            'batch_size': len(self.running_batch.reqs) if event == 'processing' else None
        }
        self.request_traces.append(trace_info)
```

è¿™äº›åŠŸèƒ½ä¸ºç³»ç»Ÿä¼˜åŒ–å’Œé—®é¢˜æ’æŸ¥æä¾›äº†å¼ºæœ‰åŠ›çš„å·¥å…·ï¼Œå¸®åŠ©å¼€å‘è€…è¯†åˆ«æ€§èƒ½ç“¶é¢ˆå’Œä¼˜åŒ–æœºä¼šã€‚

---

## ğŸ“ æ€»ç»“

SGLangè°ƒåº¦å™¨é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ¶æ„å’Œä¸°å¯Œçš„åŠŸèƒ½ï¼Œä¸ºé«˜æ€§èƒ½è¯­è¨€æ¨¡å‹æ¨ç†æä¾›äº†å¼ºå¤§çš„è°ƒåº¦èƒ½åŠ›ã€‚å…¶Mixinæ¨¡å¼çš„è®¾è®¡ç¡®ä¿äº†åŠŸèƒ½çš„æ¨¡å—åŒ–å’Œå¯æ‰©å±•æ€§ï¼Œå¤šç§äº‹ä»¶å¾ªç¯æ¨¡å¼é€‚åº”äº†ä¸åŒçš„éƒ¨ç½²åœºæ™¯ï¼Œå®Œå–„çš„èµ„æºç®¡ç†å’Œå¹¶è¡Œåè°ƒæœºåˆ¶æ”¯æ’‘äº†å¤§è§„æ¨¡åˆ†å¸ƒå¼æ¨ç†ï¼Œè€Œå†…ç½®çš„ç›‘æ§å’Œè°ƒè¯•åŠŸèƒ½åˆ™ä¸ºç³»ç»Ÿçš„ç¨³å®šè¿è¡Œæä¾›äº†ä¿éšœã€‚ç†è§£è°ƒåº¦å™¨çš„è®¾è®¡ç†å¿µå’Œå·¥ä½œåŸç†ï¼Œæ˜¯æŒæ¡SGLangç³»ç»Ÿçš„å…³é”®æ‰€åœ¨ã€‚
