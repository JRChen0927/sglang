# 调度器总览与架构

---

SGLang的调度器是整个推理系统的核心组件，负责管理张量并行GPU worker的工作协调。作为系统的"大脑"，调度器承担着请求调度、批处理优化、资源管理和并行协调等关键职责。

---

## 🎯 核心职责

调度器的主要职责包括以下几个方面：

**请求生命周期管理**  
从接收用户请求开始，调度器负责整个请求的生命周期管理，包括请求验证、排队调度、批次组织、推理执行和结果返回的全流程控制。

**批处理优化**  
调度器实现了动态批处理机制，能够智能地将多个请求组织成批次进行处理，最大化GPU的利用率和系统吞吐量。

**资源统一调度**  
调度器统一管理GPU内存、KV缓存、计算资源等系统资源，确保资源的高效分配和使用。

**多维并行协调**  
在分布式环境下，调度器协调张量并行、流水线并行、数据并行和专家并行等多种并行策略。

---

## 🏗️ 架构设计

### 🧩 Mixin模式设计

SGLang调度器采用多重继承的Mixin模式，将不同功能模块解耦，实现高度的模块化设计：

```python
class Scheduler(
    SchedulerOutputProcessorMixin,
    SchedulerUpdateWeightsMixin,
    SchedulerProfilerMixin,
    SchedulerMetricsMixin,
    SchedulerDisaggregationDecodeMixin,
    SchedulerDisaggregationPrefillMixin,
):
    """A scheduler that manages a tensor parallel GPU worker."""
    
    def __init__(self, server_args: ServerArgs, port_args: PortArgs, 
                 gpu_id: int, tp_rank: int, moe_ep_rank: int, 
                 pp_rank: int, dp_rank: Optional[int]):
        # 解析并行配置参数
        self.server_args = server_args
        self.tp_rank = tp_rank              # 张量并行rank
        self.moe_ep_rank = moe_ep_rank      # 专家并行rank  
        self.pp_rank = pp_rank              # 流水线并行rank
        self.dp_rank = dp_rank              # 数据并行rank
        self.tp_size = server_args.tp_size  # 张量并行大小
        
        # 初始化核心组件
        self.running_batch = ScheduleBatch(reqs=[], batch_is_full=False)
        self.waiting_queue = []
        self.forward_ct = 0
        
        # 初始化请求分发器
        self._request_dispatcher = TypeBasedDispatcher([
            (TokenizedGenerateReqInput, self.handle_generate_request),
            (TokenizedEmbeddingReqInput, self.handle_embedding_request),
            (FlushCacheReqInput, self.flush_cache_wrapped),
            # ... 更多请求类型映射
        ])
```

每个Mixin类负责特定的功能领域，这种设计带来了良好的代码组织和可维护性：

**SchedulerOutputProcessorMixin**: 处理模型输出和流式响应，包括预填充和解码结果的处理、流式传输控制等。

**SchedulerUpdateWeightsMixin**: 支持动态权重更新，包括从磁盘加载权重、分布式权重更新、张量权重更新等功能：

```python
def update_weights_from_disk(self, recv_req: UpdateWeightFromDiskReqInput):
    """从磁盘就地更新权重"""
    success, message = self.tp_worker.update_weights_from_disk(recv_req)
    if success:
        flush_cache_success = self.flush_cache()
        assert flush_cache_success, "权重更新后缓存刷新失败"
    return UpdateWeightFromDiskReqOutput(success, message, 0)
```

**SchedulerProfilerMixin**: 提供性能分析功能，支持PyTorch profiler集成和自定义性能指标收集。

**SchedulerMetricsMixin**: 负责指标收集，包括吞吐量、延迟、缓存命中率等关键性能指标的统计。

**SchedulerDisaggregationDecodeMixin/PrefillMixin**: 支持预填充和解码分离的分离式架构，用于超大模型的分布式推理。

### ⚙️ 并行配置管理

调度器的初始化过程中，需要处理复杂的并行配置。在__init__方法中，调度器解析各种并行参数：

```python
def __init__(self, server_args: ServerArgs, port_args: PortArgs, gpu_id: int,
             tp_rank: int, moe_ep_rank: int, pp_rank: int, dp_rank: Optional[int]):
    # 解析并行配置
    self.tp_rank = tp_rank              # 张量并行rank
    self.moe_ep_rank = moe_ep_rank      # 专家并行rank
    self.pp_rank = pp_rank              # 流水线并行rank
    self.dp_rank = dp_rank              # 数据并行rank
    self.tp_size = server_args.tp_size  # 张量并行大小
    self.moe_ep_size = server_args.ep_size
    self.pp_size = server_args.pp_size
    self.dp_size = server_args.dp_size
```

这些参数定义了当前调度器实例在分布式系统中的位置和作用范围，是后续所有并行协调工作的基础。

---

## 🔄 事件循环架构

调度器的核心工作机制是事件循环，根据不同的配置和优化需求，SGLang提供了多种事件循环实现：

**标准事件循环**  
event_loop_normal是最基础的事件循环实现，采用串行处理模式，逻辑清晰，适合调试和开发环境。

**重叠事件循环**  
event_loop_overlap实现了CPU处理和GPU计算的重叠，能够显著提升系统吞吐量，适合生产环境。

**流水线并行事件循环**  
event_loop_pp专门为流水线并行设计，支持大模型的分层处理，能够有效利用多个GPU节点。

**分离式事件循环**  
针对预填充和解码分离的场景，提供了专门的事件循环实现，包括event_loop_overlap_disagg_prefill和event_loop_normal_disagg_decode等。

### 🎛️ 事件循环选择逻辑

调度器根据配置参数自动选择合适的事件循环：

```python
# 事件循环选择逻辑 (run_scheduler_process)
disaggregation_mode = scheduler.disaggregation_mode
if disaggregation_mode == DisaggregationMode.NULL:
    if server_args.pp_size > 1:
        scheduler.event_loop_pp()          # 流水线并行事件循环
    elif scheduler.enable_overlap:
        scheduler.event_loop_overlap()     # CPU-GPU重叠事件循环
    else:
        scheduler.event_loop_normal()      # 标准事件循环
elif disaggregation_mode == DisaggregationMode.PREFILL:
    if scheduler.enable_overlap:
        scheduler.event_loop_overlap_disagg_prefill()
    else:
        scheduler.event_loop_normal_disagg_prefill()
elif disaggregation_mode == DisaggregationMode.DECODE:
    if scheduler.enable_overlap:
        scheduler.event_loop_overlap_disagg_decode()
    else:
        scheduler.event_loop_normal_disagg_decode()
```

**标准事件循环示例**：
```python
@DynamicGradMode()
def event_loop_normal(self):
    """标准调度器循环"""
    while True:
        # 1. 接收新请求
        recv_reqs = self.recv_requests()
        self.process_input_requests(recv_reqs)

        # 2. 获取下一个批次
        batch = self.get_next_batch_to_run()
        self.cur_batch = batch

        # 3. 执行推理或空闲检查
        if batch:
            result = self.run_batch(batch)
            self.process_batch_result(batch, result)
        else:
            self.self_check_during_idle()

        self.last_batch = batch
```

这种自动选择机制确保了调度器能够根据部署配置选择最优的执行模式。

---

## 📦 请求分发机制

### 🔀 TypeBasedDispatcher架构

调度器使用TypeBasedDispatcher来分发不同类型的请求：

```python
# 初始化请求分发器
self._request_dispatcher = TypeBasedDispatcher([
    (TokenizedGenerateReqInput, self.handle_generate_request),
    (TokenizedEmbeddingReqInput, self.handle_embedding_request),
    (FlushCacheReqInput, self.flush_cache_wrapped),
    (AbortReq, self.abort_request),
    (OpenSessionReqInput, self.open_session),
    (CloseSessionReqInput, self.close_session),
    (UpdateWeightFromDiskReqInput, self.update_weights_from_disk),
    (InitWeightsUpdateGroupReqInput, self.init_weights_update_group),
    (UpdateWeightsFromDistributedReqInput, self.update_weights_from_distributed),
    (UpdateWeightsFromTensorReqInput, self.update_weights_from_tensor),
    (GetWeightsByNameReqInput, self.get_weights_by_name),
    (ReleaseMemoryOccupationReqInput, self.release_memory_occupation),
    (ResumeMemoryOccupationReqInput, self.resume_memory_occupation),
    (SlowDownReqInput, self.slow_down),
    (ProfileReq, self.profile),
    (LoadLoRAAdapterReqInput, self.load_lora_adapter),
    (UnloadLoRAAdapterReqInput, self.unload_lora_adapter),
])
```

这种设计支持了十多种不同类型的请求，从基本的生成和嵌入请求，到高级的权重更新、性能分析、LoRA适配器管理等功能。

---

## ⚡ 核心方法解析

### 📥 请求处理流程

调度器通过一系列核心方法处理请求：

**recv_requests方法**负责从网络接收新的请求，支持分布式环境下的请求广播和同步。

```python
def recv_requests(self) -> List[Req]:
    """接收来自tokenizer的请求"""
    recv_reqs = []
    while True:
        try:
            recv_req = self.recv_from_tokenizer.recv_pyobj(zmq.NOBLOCK)
            recv_reqs.append(recv_req)
        except zmq.ZMQError:
            break
    return recv_reqs
```

**process_input_requests方法**处理接收到的请求，包括请求验证、类型分发和异常处理。

**handle_generate_request方法**专门处理生成类请求，创建Req对象并加入调度队列：

```python
def handle_generate_request(self, recv_req: TokenizedGenerateReqInput):
    """处理文本生成请求"""
    # 创建新的Req对象
    req = Req(
        recv_req.rid,
        recv_req.input_text,
        recv_req.input_ids,
        recv_req.sampling_params,
        return_logprob=recv_req.return_logprob,
        stream=recv_req.stream,
        lora_id=recv_req.lora_id,
    )
    req.tokenizer = self.tokenizer
    
    # 添加到等待队列
    self.waiting_queue.append(req)
```

**handle_embedding_request方法**处理嵌入类请求，支持向量检索等应用场景。

### 🔄 批处理管理

**get_next_batch_to_run方法**是批处理调度的核心，负责从等待队列中选择请求组成批次：

```python
def get_next_batch_to_run(self) -> Optional[ScheduleBatch]:
    # 合并预填充批次到运行批次
    if self.last_batch and self.last_batch.forward_mode.is_extend():
        if not self.last_batch.is_empty():
            if self.running_batch.is_empty():
                self.running_batch = self.last_batch
            else:
                self.running_batch.merge_batch(self.last_batch)

    # 获取新的预填充批次
    new_batch = self.get_new_batch_prefill()
    
    return new_batch or self.running_batch
```

**get_new_batch_prefill方法**创建新的预填充批次，实现动态批处理和连续批处理。

**run_batch方法**执行批次推理，调用底层的模型运行器进行GPU计算：

```python
def run_batch(self, batch: ScheduleBatch):
    """执行批次推理"""
    if batch.forward_mode.is_decode() or batch.forward_mode.is_mixed():
        # 解码阶段
        model_worker_batch = batch.get_model_worker_batch()
        result = self.tp_worker.forward_batch_generation(model_worker_batch)
    else:
        # 预填充阶段  
        result = self.tp_worker.forward_batch_generation(model_worker_batch)
    
    return result
```

**process_batch_result方法**处理推理结果，包括输出解码、流式传输和请求完成检查。

### 🔧 系统维护

**self_check_during_idle方法**在系统空闲时执行自检和状态重置，确保系统的长期稳定运行：

```python
def self_check_during_idle(self):
    """系统空闲时的自检和维护"""
    # 执行内存泄漏检查
    self.check_memory()
    
    # 如果有空闲睡眠器，可能进入睡眠节能模式
    self.maybe_sleep_on_idle()
    
    # 重置一些统计计数器
    if self.forward_ct % 1000 == 0:
        logger.info(f"Forward count: {self.forward_ct}")
```

**flush_cache方法**提供缓存清理功能，支持系统状态重置：

```python
def flush_cache(self) -> bool:
    """刷新所有缓存，重置调度器状态"""
    try:
        # 清空所有队列
        self.waiting_queue.clear()
        self.running_batch = ScheduleBatch(reqs=[], batch_is_full=False)
        
        # 刷新KV缓存
        if hasattr(self, 'tree_cache'):
            self.tree_cache.reset()
        
        # 重置内存池
        if hasattr(self, 'req_to_token_pool'):
            self.req_to_token_pool.clear()
            
        # 重置统计信息
        self.forward_ct = 0
        
        logger.info("Cache flushed successfully")
        return True
    except Exception as e:
        logger.error(f"Cache flush failed: {e}")
        return False
```

**watchdog_thread方法**实现看门狗机制，监控系统健康状态：

```python
def watchdog_thread(self):
    """看门狗线程，监控调度器健康状态"""
    while True:
        try:
            # 检查是否有僵死的请求
            current_time = time.time()
            for req in self.running_batch.reqs:
                if current_time - req.start_time > self.request_timeout:
                    logger.warning(f"Request {req.rid} timeout, aborting")
                    self.abort_request(AbortReq(req.rid))
            
            # 检查内存使用情况
            if self.get_memory_usage() > 0.9:
                logger.warning("High memory usage detected")
                
            time.sleep(10)  # 每10秒检查一次
        except Exception as e:
            logger.error(f"Watchdog error: {e}")
```

---

## 💾 内存和资源管理

调度器统一管理系统的内存和计算资源。在初始化过程中，调度器会创建和配置各种资源管理组件：

```python
def init_memory_pool_and_cache(self):
    """初始化内存池和缓存系统"""
    # 初始化请求到token映射池
    self.req_to_token_pool = ReqToTokenPool(
        size=self.max_running_requests,
        pre_alloc_size=self.pre_alloc_size if self.pre_alloc_size else 0,
    )
    
    # 初始化KV缓存分配器
    if self.is_hybrid:
        # 混合缓存架构（SWA）
        self.token_to_kv_pool = HybridKVPoolAllocator(
            size=self.max_total_num_tokens,
            max_context_len=self.context_len,
            dtype=self.dtype,
        )
    else:
        # 标准缓存架构
        self.token_to_kv_pool = BaseTokenToKVPoolAllocator(
            size=self.max_total_num_tokens,
            dtype=self.dtype,
        )
    
    # 初始化前缀缓存
    if self.server_args.disable_radix_cache:
        self.tree_cache = None
    else:
        if self.enable_chunk_cache:
            self.tree_cache = ChunkCache(
                req_to_token_pool=self.req_to_token_pool,
                token_to_kv_pool=self.token_to_kv_pool,
            )
        else:
            self.tree_cache = RadixCache(
                req_to_token_pool=self.req_to_token_pool,
                token_to_kv_pool=self.token_to_kv_pool,
                disable=False,
            )
```

**资源分配示例**：
```python
def allocate_resources_for_request(self, req: Req):
    """为请求分配资源"""
    # 分配token槽位
    if not self.req_to_token_pool.available():
        logger.warning("No available request slots")
        return False
        
    req_pool_idx = self.req_to_token_pool.alloc()
    req.req_pool_idx = req_pool_idx
    
    # 分配KV缓存
    num_tokens = len(req.origin_input_ids)
    if self.token_to_kv_pool.available_size() < num_tokens:
        logger.warning("Insufficient KV cache space")
        self.req_to_token_pool.free(req_pool_idx)
        return False
    
    # 为请求分配token池
    req.token_pool_indices = self.token_to_kv_pool.alloc(num_tokens)
    
    return True

def release_resources_for_request(self, req: Req):
    """释放请求占用的资源"""
    if hasattr(req, 'req_pool_idx'):
        self.req_to_token_pool.free(req.req_pool_idx)
    
    if hasattr(req, 'token_pool_indices'):
        self.token_to_kv_pool.free(req.token_pool_indices)
```

这些组件协同工作，确保内存的高效使用和缓存的智能管理，同时支持请求的动态资源分配和回收。

---

## 🌐 并行策略协调

在分布式环境下，调度器需要协调多种并行策略：

### 🔀 张量并行协调
```python
def coordinate_tensor_parallel(self):
    """张量并行协调机制"""
    # 确保所有TP worker同步
    if self.tp_size > 1:
        # 广播批次信息到所有TP ranks
        if self.tp_rank == 0:
            # 主rank负责调度决策
            batch_info = {
                'batch_size': len(self.running_batch.reqs),
                'forward_mode': self.running_batch.forward_mode,
                'request_ids': [req.rid for req in self.running_batch.reqs]
            }
            # 广播到其他ranks
            self.broadcast_to_tp_group(batch_info)
        else:
            # 从属ranks接收调度信息
            batch_info = self.receive_from_tp_master()
```

### 🚀 流水线并行管理
```python
def manage_pipeline_parallel(self):
    """流水线并行管理"""
    if self.pp_size > 1:
        # 处理pipeline stage间的数据传递
        if self.pp_rank == 0:
            # 第一阶段：处理输入
            hidden_states = self.process_input_stage(batch)
            self.send_to_next_stage(hidden_states)
        elif self.pp_rank == self.pp_size - 1:
            # 最后阶段：生成输出
            hidden_states = self.receive_from_prev_stage()
            output = self.process_output_stage(hidden_states)
        else:
            # 中间阶段：传递隐藏状态
            hidden_states = self.receive_from_prev_stage()
            hidden_states = self.process_middle_stage(hidden_states)
            self.send_to_next_stage(hidden_states)
```

### ⚖️ 数据并行负载均衡
```python
def balance_data_parallel_load(self):
    """数据并行负载均衡"""
    if self.dp_size > 1:
        # 收集各副本的负载信息
        local_load = {
            'running_requests': len(self.running_batch.reqs),
            'waiting_requests': len(self.waiting_queue),
            'memory_usage': self.get_memory_usage()
        }
        
        # 与其他副本交换负载信息
        all_loads = self.gather_dp_loads(local_load)
        
        # 根据负载差异进行请求重分配
        if self.dp_rank == 0:  # 主副本负责调度
            self.redistribute_requests(all_loads)
```

### 🧠 专家并行调度
```python
def schedule_expert_parallel(self, expert_distribution: Dict):
    """MoE专家并行调度"""
    if self.moe_ep_size > 1:
        # 根据token路由到不同专家
        for req in self.running_batch.reqs:
            expert_ids = req.get_expert_routing()
            for expert_id in expert_ids:
                target_ep_rank = expert_id % self.moe_ep_size
                if target_ep_rank == self.moe_ep_rank:
                    # 当前rank处理这个专家
                    self.process_expert_tokens(req, expert_id)
                else:
                    # 发送到对应的专家rank
                    self.send_to_expert_rank(req, expert_id, target_ep_rank)
```

这些并行策略确保了SGLang能够在大规模分布式环境下高效运行，最大化硬件资源利用率。

---

## 📊 性能监控和调试

调度器内置了完善的性能监控和调试功能：

### 📈 关键指标收集
```python
def collect_performance_metrics(self):
    """收集性能指标"""
    # 通过SchedulerMetricsMixin收集指标
    current_time = time.time()
    
    # 吞吐量计算
    if hasattr(self, 'last_metric_time'):
        time_diff = current_time - self.last_metric_time
        tokens_processed = self.forward_ct - self.last_forward_ct
        self.last_gen_throughput = tokens_processed / time_diff if time_diff > 0 else 0
    
    # 资源利用率
    token_usage = self.get_token_usage_ratio()
    memory_usage = self.get_memory_usage_ratio()
    
    # 队列状态
    queue_stats = {
        'running_requests': len(self.running_batch.reqs),
        'waiting_requests': len(self.waiting_queue),
        'total_requests_processed': self.total_requests_processed,
    }
    
    # 缓存命中率
    if self.tree_cache:
        cache_hit_rate = self.tree_cache.get_hit_rate()
    else:
        cache_hit_rate = 0.0
    
    # 更新指标
    self.stats.update(
        gen_throughput=self.last_gen_throughput,
        token_usage=token_usage,
        memory_usage=memory_usage,
        cache_hit_rate=cache_hit_rate,
        **queue_stats
    )
```

### 🔍 性能分析工具
```python
def start_profiling(self, profile_config: ProfileConfig):
    """启动性能分析"""
    # 通过SchedulerProfilerMixin启动profiler
    import torch.profiler as profiler
    
    self.torch_profiler = profiler.profile(
        activities=[
            profiler.ProfilerActivity.CPU,
            profiler.ProfilerActivity.CUDA,
        ],
        schedule=profiler.schedule(
            wait=profile_config.wait_steps,
            warmup=profile_config.warmup_steps, 
            active=profile_config.active_steps,
            repeat=profile_config.repeat
        ),
        on_trace_ready=self.save_profiler_trace,
        record_shapes=profile_config.record_shapes,
        with_stack=profile_config.with_stack,
    )
    
    self.torch_profiler.start()
    logger.info("Performance profiling started")

def save_profiler_trace(self, prof):
    """保存性能分析结果"""
    output_path = f"{self.profiler_output_dir}/trace_{self.forward_ct}.json"
    prof.export_chrome_trace(output_path)
    logger.info(f"Profiler trace saved to {output_path}")
```

### 🛠️ 调试工具
```python
def dump_scheduler_state(self):
    """转储调度器状态用于调试"""
    state_info = {
        'timestamp': time.time(),
        'forward_count': self.forward_ct,
        'running_batch': {
            'size': len(self.running_batch.reqs),
            'forward_mode': str(self.running_batch.forward_mode),
            'requests': [req.rid for req in self.running_batch.reqs[:5]]  # 只显示前5个
        },
        'waiting_queue': {
            'size': len(self.waiting_queue),
            'requests': [req.rid for req in self.waiting_queue[:5]]
        },
        'memory_stats': {
            'token_usage': self.get_token_usage_ratio(),
            'available_tokens': self.token_to_kv_pool.available_size(),
            'req_pool_usage': len(self.req_to_token_pool.free_slots)
        },
        'parallel_config': {
            'tp_rank': self.tp_rank,
            'tp_size': self.tp_size,
            'pp_rank': self.pp_rank,
            'dp_rank': self.dp_rank
        }
    }
    
    logger.info(f"Scheduler state dump: {json.dumps(state_info, indent=2)}")
    return state_info

def monitor_request_lifecycle(self, req: Req, event: str):
    """监控请求生命周期"""
    if self.enable_request_tracing:
        trace_info = {
            'request_id': req.rid,
            'event': event,
            'timestamp': time.time(),
            'queue_position': len(self.waiting_queue) if event == 'queued' else None,
            'batch_size': len(self.running_batch.reqs) if event == 'processing' else None
        }
        self.request_traces.append(trace_info)
```

这些功能为系统优化和问题排查提供了强有力的工具，帮助开发者识别性能瓶颈和优化机会。

---

## 📝 总结

SGLang调度器通过精心设计的架构和丰富的功能，为高性能语言模型推理提供了强大的调度能力。其Mixin模式的设计确保了功能的模块化和可扩展性，多种事件循环模式适应了不同的部署场景，完善的资源管理和并行协调机制支撑了大规模分布式推理，而内置的监控和调试功能则为系统的稳定运行提供了保障。理解调度器的设计理念和工作原理，是掌握SGLang系统的关键所在。
