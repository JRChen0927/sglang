# è°ƒåº¦å™¨æ€»è§ˆä¸æ¶æ„

---

SGLangçš„è°ƒåº¦å™¨æ˜¯æ•´ä¸ªæ¨ç†ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£ç®¡ç†å¼ é‡å¹¶è¡ŒGPU workerçš„å·¥ä½œåè°ƒã€‚ä½œä¸ºç³»ç»Ÿçš„"å¤§è„‘"ï¼Œè°ƒåº¦å™¨æ‰¿æ‹…ç€è¯·æ±‚è°ƒåº¦ã€æ‰¹å¤„ç†ä¼˜åŒ–ã€èµ„æºç®¡ç†å’Œå¹¶è¡Œåè°ƒç­‰å…³é”®èŒè´£ã€‚

---

## ğŸ¯ æ ¸å¿ƒèŒè´£

è°ƒåº¦å™¨çš„ä¸»è¦èŒè´£åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š

**è¯·æ±‚ç”Ÿå‘½å‘¨æœŸç®¡ç†**  
ä»æ¥æ”¶ç”¨æˆ·è¯·æ±‚å¼€å§‹ï¼Œè°ƒåº¦å™¨è´Ÿè´£æ•´ä¸ªè¯·æ±‚çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ŒåŒ…æ‹¬è¯·æ±‚éªŒè¯ã€æ’é˜Ÿè°ƒåº¦ã€æ‰¹æ¬¡ç»„ç»‡ã€æ¨ç†æ‰§è¡Œå’Œç»“æœè¿”å›çš„å…¨æµç¨‹æ§åˆ¶ã€‚

**æ‰¹å¤„ç†ä¼˜åŒ–**  
è°ƒåº¦å™¨å®ç°äº†åŠ¨æ€æ‰¹å¤„ç†æœºåˆ¶ï¼Œèƒ½å¤Ÿæ™ºèƒ½åœ°å°†å¤šä¸ªè¯·æ±‚ç»„ç»‡æˆæ‰¹æ¬¡è¿›è¡Œå¤„ç†ï¼Œæœ€å¤§åŒ–GPUçš„åˆ©ç”¨ç‡å’Œç³»ç»Ÿååé‡ã€‚

**èµ„æºç»Ÿä¸€è°ƒåº¦**  
è°ƒåº¦å™¨ç»Ÿä¸€ç®¡ç†GPUå†…å­˜ã€KVç¼“å­˜ã€è®¡ç®—èµ„æºç­‰ç³»ç»Ÿèµ„æºï¼Œç¡®ä¿èµ„æºçš„é«˜æ•ˆåˆ†é…å’Œä½¿ç”¨ã€‚

**å¤šç»´å¹¶è¡Œåè°ƒ**  
åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹ï¼Œè°ƒåº¦å™¨åè°ƒå¼ é‡å¹¶è¡Œã€æµæ°´çº¿å¹¶è¡Œã€æ•°æ®å¹¶è¡Œå’Œä¸“å®¶å¹¶è¡Œç­‰å¤šç§å¹¶è¡Œç­–ç•¥ã€‚

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### ğŸ§© Mixinæ¨¡å¼è®¾è®¡

SGLangè°ƒåº¦å™¨é‡‡ç”¨å¤šé‡ç»§æ‰¿çš„Mixinæ¨¡å¼ï¼Œå°†ä¸åŒåŠŸèƒ½æ¨¡å—è§£è€¦ï¼Œå®ç°é«˜åº¦çš„æ¨¡å—åŒ–è®¾è®¡ï¼š

```python
class Scheduler(
    SchedulerOutputProcessorMixin,
    SchedulerUpdateWeightsMixin,
    SchedulerProfilerMixin,
    SchedulerMetricsMixin,
    SchedulerDisaggregationDecodeMixin,
    SchedulerDisaggregationPrefillMixin,
):
    """A scheduler that manages a tensor parallel GPU worker."""
    
    def __init__(self, server_args: ServerArgs, port_args: PortArgs, 
                 gpu_id: int, tp_rank: int, moe_ep_rank: int, 
                 pp_rank: int, dp_rank: Optional[int]):
        # è§£æå¹¶è¡Œé…ç½®å‚æ•°
        self.server_args = server_args
        self.tp_rank = tp_rank              # å¼ é‡å¹¶è¡Œrank
        self.moe_ep_rank = moe_ep_rank      # ä¸“å®¶å¹¶è¡Œrank  
        self.pp_rank = pp_rank              # æµæ°´çº¿å¹¶è¡Œrank
        self.dp_rank = dp_rank              # æ•°æ®å¹¶è¡Œrank
        self.tp_size = server_args.tp_size  # å¼ é‡å¹¶è¡Œå¤§å°
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.running_batch = ScheduleBatch(reqs=[], batch_is_full=False)
        self.waiting_queue = []
        self.forward_ct = 0
        
        # åˆå§‹åŒ–è¯·æ±‚åˆ†å‘å™¨
        self._request_dispatcher = TypeBasedDispatcher([
            (TokenizedGenerateReqInput, self.handle_generate_request),
            (TokenizedEmbeddingReqInput, self.handle_embedding_request),
            (FlushCacheReqInput, self.flush_cache_wrapped),
            # ... æ›´å¤šè¯·æ±‚ç±»å‹æ˜ å°„
        ])
```

æ¯ä¸ªMixinç±»è´Ÿè´£ç‰¹å®šçš„åŠŸèƒ½é¢†åŸŸï¼Œè¿™ç§è®¾è®¡å¸¦æ¥äº†è‰¯å¥½çš„ä»£ç ç»„ç»‡å’Œå¯ç»´æŠ¤æ€§ï¼š

**SchedulerOutputProcessorMixin**: å¤„ç†æ¨¡å‹è¾“å‡ºå’Œæµå¼å“åº”ï¼ŒåŒ…æ‹¬é¢„å¡«å……å’Œè§£ç ç»“æœçš„å¤„ç†ã€æµå¼ä¼ è¾“æ§åˆ¶ç­‰ã€‚

**SchedulerUpdateWeightsMixin**: æ”¯æŒåŠ¨æ€æƒé‡æ›´æ–°ï¼ŒåŒ…æ‹¬ä»ç£ç›˜åŠ è½½æƒé‡ã€åˆ†å¸ƒå¼æƒé‡æ›´æ–°ã€å¼ é‡æƒé‡æ›´æ–°ç­‰åŠŸèƒ½ï¼š

```python
def update_weights_from_disk(self, recv_req: UpdateWeightFromDiskReqInput):
    """ä»ç£ç›˜å°±åœ°æ›´æ–°æƒé‡"""
    success, message = self.tp_worker.update_weights_from_disk(recv_req)
    if success:
        flush_cache_success = self.flush_cache()
        assert flush_cache_success, "æƒé‡æ›´æ–°åç¼“å­˜åˆ·æ–°å¤±è´¥"
    return UpdateWeightFromDiskReqOutput(success, message, 0)
```

**SchedulerProfilerMixin**: æä¾›æ€§èƒ½åˆ†æåŠŸèƒ½ï¼Œæ”¯æŒPyTorch profileré›†æˆå’Œè‡ªå®šä¹‰æ€§èƒ½æŒ‡æ ‡æ”¶é›†ã€‚

**SchedulerMetricsMixin**: è´Ÿè´£æŒ‡æ ‡æ”¶é›†ï¼ŒåŒ…æ‹¬ååé‡ã€å»¶è¿Ÿã€ç¼“å­˜å‘½ä¸­ç‡ç­‰å…³é”®æ€§èƒ½æŒ‡æ ‡çš„ç»Ÿè®¡ã€‚

**SchedulerDisaggregationDecodeMixin/PrefillMixin**: æ”¯æŒé¢„å¡«å……å’Œè§£ç åˆ†ç¦»çš„åˆ†ç¦»å¼æ¶æ„ï¼Œç”¨äºè¶…å¤§æ¨¡å‹çš„åˆ†å¸ƒå¼æ¨ç†ã€‚

### âš™ï¸ å¹¶è¡Œé…ç½®ç®¡ç†

è°ƒåº¦å™¨çš„åˆå§‹åŒ–è¿‡ç¨‹ä¸­ï¼Œéœ€è¦å¤„ç†å¤æ‚çš„å¹¶è¡Œé…ç½®ã€‚åœ¨__init__æ–¹æ³•ä¸­ï¼Œè°ƒåº¦å™¨è§£æå„ç§å¹¶è¡Œå‚æ•°ï¼š

```python
def __init__(self, server_args: ServerArgs, port_args: PortArgs, gpu_id: int,
             tp_rank: int, moe_ep_rank: int, pp_rank: int, dp_rank: Optional[int]):
    # è§£æå¹¶è¡Œé…ç½®
    self.tp_rank = tp_rank              # å¼ é‡å¹¶è¡Œrank
    self.moe_ep_rank = moe_ep_rank      # ä¸“å®¶å¹¶è¡Œrank
    self.pp_rank = pp_rank              # æµæ°´çº¿å¹¶è¡Œrank
    self.dp_rank = dp_rank              # æ•°æ®å¹¶è¡Œrank
    self.tp_size = server_args.tp_size  # å¼ é‡å¹¶è¡Œå¤§å°
    self.moe_ep_size = server_args.ep_size
    self.pp_size = server_args.pp_size
    self.dp_size = server_args.dp_size
```

è¿™äº›å‚æ•°å®šä¹‰äº†å½“å‰è°ƒåº¦å™¨å®ä¾‹åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„ä½ç½®å’Œä½œç”¨èŒƒå›´ï¼Œæ˜¯åç»­æ‰€æœ‰å¹¶è¡Œåè°ƒå·¥ä½œçš„åŸºç¡€ã€‚

---

## ğŸ”„ äº‹ä»¶å¾ªç¯æ¶æ„

è°ƒåº¦å™¨çš„æ ¸å¿ƒå·¥ä½œæœºåˆ¶æ˜¯äº‹ä»¶å¾ªç¯ï¼Œæ ¹æ®ä¸åŒçš„é…ç½®å’Œä¼˜åŒ–éœ€æ±‚ï¼ŒSGLangæä¾›äº†å¤šç§äº‹ä»¶å¾ªç¯å®ç°ï¼š

**æ ‡å‡†äº‹ä»¶å¾ªç¯**  
event_loop_normalæ˜¯æœ€åŸºç¡€çš„äº‹ä»¶å¾ªç¯å®ç°ï¼Œé‡‡ç”¨ä¸²è¡Œå¤„ç†æ¨¡å¼ï¼Œé€»è¾‘æ¸…æ™°ï¼Œé€‚åˆè°ƒè¯•å’Œå¼€å‘ç¯å¢ƒã€‚

**é‡å äº‹ä»¶å¾ªç¯**  
event_loop_overlapå®ç°äº†CPUå¤„ç†å’ŒGPUè®¡ç®—çš„é‡å ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡ç³»ç»Ÿååé‡ï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒã€‚

**æµæ°´çº¿å¹¶è¡Œäº‹ä»¶å¾ªç¯**  
event_loop_ppä¸“é—¨ä¸ºæµæ°´çº¿å¹¶è¡Œè®¾è®¡ï¼Œæ”¯æŒå¤§æ¨¡å‹çš„åˆ†å±‚å¤„ç†ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨å¤šä¸ªGPUèŠ‚ç‚¹ã€‚

**åˆ†ç¦»å¼äº‹ä»¶å¾ªç¯**  
é’ˆå¯¹é¢„å¡«å……å’Œè§£ç åˆ†ç¦»çš„åœºæ™¯ï¼Œæä¾›äº†ä¸“é—¨çš„äº‹ä»¶å¾ªç¯å®ç°ï¼ŒåŒ…æ‹¬event_loop_overlap_disagg_prefillå’Œevent_loop_normal_disagg_decodeç­‰ã€‚

### ğŸ›ï¸ äº‹ä»¶å¾ªç¯é€‰æ‹©é€»è¾‘

### ğŸ¯ æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ

```python
# äº‹ä»¶å¾ªç¯è‡ªåŠ¨é€‰æ‹©ç­–ç•¥
if disaggregation_mode == DisaggregationMode.NULL:
    if server_args.pp_size > 1:
        scheduler.event_loop_pp()          # æµæ°´çº¿å¹¶è¡Œ
    elif scheduler.enable_overlap:
        scheduler.event_loop_overlap()     # CPU-GPUé‡å 
    else:
        scheduler.event_loop_normal()      # æ ‡å‡†åŒæ­¥
elif disaggregation_mode == DisaggregationMode.PREFILL:
    scheduler.event_loop_disagg_prefill()  # åˆ†ç¦»å¼é¢„å¡«å……
elif disaggregation_mode == DisaggregationMode.DECODE:
    scheduler.event_loop_disagg_decode()   # åˆ†ç¦»å¼è§£ç 
```

**æ ‡å‡†äº‹ä»¶å¾ªç¯æ ¸å¿ƒæµç¨‹**ï¼š
```python
@DynamicGradMode()
def event_loop_normal(self):
    """æ ‡å‡†è°ƒåº¦å™¨å¾ªç¯"""
    while True:
        # 1. æ¥æ”¶æ–°è¯·æ±‚
        recv_reqs = self.recv_requests()
        self.process_input_requests(recv_reqs)

        # 2. è·å–ä¸‹ä¸€ä¸ªæ‰¹æ¬¡
        batch = self.get_next_batch_to_run()
        self.cur_batch = batch

        # 3. æ‰§è¡Œæ¨ç†æˆ–ç©ºé—²æ£€æŸ¥
        if batch:
            result = self.run_batch(batch)
            self.process_batch_result(batch, result)
        else:
            self.self_check_during_idle()

        self.last_batch = batch
```

### ğŸ” æºç å®ç°ç»†èŠ‚

```python
def run_scheduler_process(server_args, port_args, gpu_id, tp_rank, 
                         moe_ep_rank, pp_rank, dp_rank, pipe_writer, balance_meta):
    """çœŸå®çš„è°ƒåº¦å™¨è¿›ç¨‹å¯åŠ¨å’Œäº‹ä»¶å¾ªç¯é€‰æ‹©"""
    # åˆ›å»ºè°ƒåº¦å™¨å®ä¾‹
    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, 
                         moe_ep_rank, pp_rank, dp_rank, dp_balance_meta=balance_meta)
    
    # å‘é€å°±ç»ªçŠ¶æ€
    pipe_writer.send({
        "status": "ready",
        "max_total_num_tokens": scheduler.max_total_num_tokens,
        "max_req_input_len": scheduler.max_req_input_len,
    })

    # å¤æ‚çš„äº‹ä»¶å¾ªç¯é€‰æ‹©é€»è¾‘
    disaggregation_mode = scheduler.disaggregation_mode
    if disaggregation_mode == DisaggregationMode.NULL:
        if server_args.pp_size > 1:
            scheduler.event_loop_pp()
        elif scheduler.enable_overlap:
            scheduler.event_loop_overlap()
        else:
            scheduler.event_loop_normal()
    elif disaggregation_mode == DisaggregationMode.PREFILL:
        if scheduler.enable_overlap:
            scheduler.event_loop_overlap_disagg_prefill()
        else:
            if server_args.pp_size > 1:
                scheduler.event_loop_pp_disagg_prefill()  # æµæ°´çº¿+åˆ†ç¦»å¼é¢„å¡«å……
            else:
                scheduler.event_loop_normal_disagg_prefill()
    elif disaggregation_mode == DisaggregationMode.DECODE:
        if scheduler.enable_overlap:
            scheduler.event_loop_overlap_disagg_decode()
        else:
            scheduler.event_loop_normal_disagg_decode()

@DynamicGradMode()
def event_loop_overlap(self):
    """çœŸå®çš„é‡å äº‹ä»¶å¾ªç¯å®ç°"""
    self.result_queue = deque()

    while True:
        recv_reqs = self.recv_requests()
        self.process_input_requests(recv_reqs)

        batch = self.get_next_batch_to_run()
        self.cur_batch = batch

        if batch:
            # å¼‚æ­¥å¯åŠ¨æ‰¹æ¬¡æ¨ç†
            batch.launch_done = threading.Event()
            result = self.run_batch(batch)
            self.result_queue.append((batch.copy(), result))

            # åˆ›å»ºè™šæ‹Ÿé¦–æ‰¹æ¬¡ä»¥å¯åŠ¨é‡å ç®¡é“
            if self.last_batch is None:
                tmp_batch = ScheduleBatch(
                    reqs=None,
                    forward_mode=ForwardMode.DUMMY_FIRST,
                    next_batch_sampling_info=self.tp_worker.cur_sampling_info,
                )
                self.process_batch_result(tmp_batch, None, batch.launch_done)

        # å¤„ç†ä¸Šä¸€ä¸ªæ‰¹æ¬¡çš„ç»“æœï¼ˆå®ç°CPU-GPUé‡å ï¼‰
        if self.last_batch:
            tmp_batch, tmp_result = self.result_queue.popleft()
            tmp_batch.next_batch_sampling_info = (
                self.tp_worker.cur_sampling_info if batch else None
            )
            self.process_batch_result(tmp_batch, tmp_result, 
                                    batch.launch_done if batch else None)
        elif batch is None:
            self.self_check_during_idle()

        self.last_batch = batch

ğŸ’¡ **å®ç°è¯´æ˜**: æºç ä¸­æœ‰7ç§ä¸åŒçš„äº‹ä»¶å¾ªç¯ç»„åˆï¼ŒåŒ…æ‹¬æµæ°´çº¿å¹¶è¡Œçš„åˆ†ç¦»å¼ç‰ˆæœ¬ã€‚é‡å äº‹ä»¶å¾ªç¯ä½¿ç”¨dequeå’Œthreading.Eventå®ç°CPUå¤„ç†å’ŒGPUè®¡ç®—çš„çœŸæ­£å¹¶è¡Œã€‚
```

è¿™ç§è‡ªåŠ¨é€‰æ‹©æœºåˆ¶ç¡®ä¿äº†è°ƒåº¦å™¨èƒ½å¤Ÿæ ¹æ®éƒ¨ç½²é…ç½®é€‰æ‹©æœ€ä¼˜çš„æ‰§è¡Œæ¨¡å¼ã€‚

---

## ğŸ“¦ è¯·æ±‚åˆ†å‘æœºåˆ¶

### ğŸ”€ TypeBasedDispatcheræ¶æ„

### ğŸ¯ æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ

è°ƒåº¦å™¨ä½¿ç”¨TypeBasedDispatcherå®ç°åŸºäºç±»å‹çš„è¯·æ±‚åˆ†å‘ï¼š

```python
# è¯·æ±‚åˆ†å‘å™¨çš„æ ¸å¿ƒæ¦‚å¿µ
self._request_dispatcher = TypeBasedDispatcher([
    (TokenizedGenerateReqInput, self.handle_generate_request),
    (TokenizedEmbeddingReqInput, self.handle_embedding_request),
    (BatchTokenizedGenerateReqInput, self.handle_batch_generate_request),
    (FlushCacheReqInput, self.flush_cache_wrapped),
    (AbortReq, self.abort_request),
    # ... æ›´å¤šè¯·æ±‚ç±»å‹æ˜ å°„
])

# ä½¿ç”¨æ–¹å¼
for recv_req in recv_reqs:
    self._request_dispatcher.dispatch(recv_req)  # è‡ªåŠ¨è·¯ç”±åˆ°å¯¹åº”å¤„ç†å™¨
```

### ğŸ” æºç å®ç°ç»†èŠ‚

```python
# çœŸå®çš„è°ƒåº¦å™¨è¯·æ±‚åˆ†å‘å™¨åˆå§‹åŒ–
self._request_dispatcher = TypeBasedDispatcher([
    (TokenizedGenerateReqInput, self.handle_generate_request),
    (TokenizedEmbeddingReqInput, self.handle_embedding_request),
    (BatchTokenizedGenerateReqInput, self.handle_batch_generate_request),
    (BatchTokenizedEmbeddingReqInput, self.handle_batch_embedding_request),
    (FlushCacheReqInput, self.flush_cache_wrapped),
    (AbortReq, self.abort_request),
    (OpenSessionReqInput, self.open_session),
    (CloseSessionReqInput, self.close_session),
    (UpdateWeightFromDiskReqInput, self.update_weights_from_disk),
    (InitWeightsUpdateGroupReqInput, self.init_weights_update_group),
    (UpdateWeightsFromDistributedReqInput, self.update_weights_from_distributed),
    (UpdateWeightsFromTensorReqInput, self.update_weights_from_tensor),
    (GetWeightsByNameReqInput, self.get_weights_by_name),
    (ReleaseMemoryOccupationReqInput, self.release_memory_occupation),
    (ResumeMemoryOccupationReqInput, self.resume_memory_occupation),
    (SlowDownReqInput, self.slow_down),
    (ProfileReq, self.profile),
    (FreezeGCReq, self.handle_freeze_gc),
    (GetInternalStateReq, self.get_internal_state),
    (SetInternalStateReq, self.set_internal_state),
    (RpcReqInput, self.handle_rpc_request),
    (ExpertDistributionReq, self.expert_distribution_handle),
    (LoadLoRAAdapterReqInput, self.load_lora_adapter),
    (UnloadLoRAAdapterReqInput, self.unload_lora_adapter),
])

ğŸ’¡ **å®ç°è¯´æ˜**: çœŸå®æºç æ”¯æŒ20+ç§è¯·æ±‚ç±»å‹ï¼ŒåŒ…æ‹¬æ‰¹é‡è¯·æ±‚ã€RPCè°ƒç”¨ã€ä¸“å®¶åˆ†å¸ƒã€åƒåœ¾å›æ”¶æ§åˆ¶ã€å†…éƒ¨çŠ¶æ€ç®¡ç†ç­‰é«˜çº§åŠŸèƒ½ã€‚TypeBasedDispatcheré€šè¿‡ç±»å‹åŒ¹é…è‡ªåŠ¨è·¯ç”±è¯·æ±‚åˆ°å¯¹åº”çš„å¤„ç†æ–¹æ³•ã€‚
```

è¿™ç§è®¾è®¡æ”¯æŒäº†å¹¿æ³›çš„è¯·æ±‚ç±»å‹ï¼Œä»åŸºæœ¬çš„ç”Ÿæˆå’ŒåµŒå…¥è¯·æ±‚ï¼Œåˆ°é«˜çº§çš„ç³»ç»Ÿç®¡ç†å’Œæ‰©å±•åŠŸèƒ½ã€‚

---

## âš¡ æ ¸å¿ƒæ–¹æ³•è§£æ

### ğŸ“¥ è¯·æ±‚å¤„ç†æµç¨‹

è°ƒåº¦å™¨é€šè¿‡ä¸€ç³»åˆ—æ ¸å¿ƒæ–¹æ³•å¤„ç†è¯·æ±‚ï¼š

**recv_requestsæ–¹æ³•**è´Ÿè´£ä»ç½‘ç»œæ¥æ”¶æ–°çš„è¯·æ±‚ï¼Œæ”¯æŒåˆ†å¸ƒå¼ç¯å¢ƒä¸‹çš„è¯·æ±‚å¹¿æ’­å’ŒåŒæ­¥ã€‚

```python
def recv_requests(self) -> List[Req]:
    """æ¥æ”¶æ¥è‡ªtokenizerçš„è¯·æ±‚"""
    recv_reqs = []
    while True:
        try:
            recv_req = self.recv_from_tokenizer.recv_pyobj(zmq.NOBLOCK)
            recv_reqs.append(recv_req)
        except zmq.ZMQError:
            break
    return recv_reqs
```

**process_input_requestsæ–¹æ³•**å¤„ç†æ¥æ”¶åˆ°çš„è¯·æ±‚ï¼ŒåŒ…æ‹¬è¯·æ±‚éªŒè¯ã€ç±»å‹åˆ†å‘å’Œå¼‚å¸¸å¤„ç†ï¼š

```python
def process_input_requests(self, recv_reqs: List[Any]):
    """å¤„ç†è¾“å…¥è¯·æ±‚çš„æ ¸å¿ƒåˆ†å‘é€»è¾‘"""
    for recv_req in recv_reqs:
        try:
            # ä½¿ç”¨TypeBasedDispatcheræ ¹æ®è¯·æ±‚ç±»å‹åˆ†å‘
            self._request_dispatcher.dispatch(recv_req)
        except Exception as e:
            # å¼‚å¸¸å¤„ç†å’Œé”™è¯¯å“åº”
            logger.error(f"Error processing request {getattr(recv_req, 'rid', 'unknown')}: {e}")
            if hasattr(recv_req, 'rid'):
                error_req = Req(recv_req.rid, "", [], SamplingParams())
                self.send_error_response(error_req, str(e))
```

**handle_generate_requestæ–¹æ³•**ä¸“é—¨å¤„ç†ç”Ÿæˆç±»è¯·æ±‚ï¼Œåˆ›å»ºReqå¯¹è±¡å¹¶åŠ å…¥è°ƒåº¦é˜Ÿåˆ—ï¼š

```python
def handle_generate_request(self, recv_req: TokenizedGenerateReqInput):
    """å¤„ç†æ–‡æœ¬ç”Ÿæˆè¯·æ±‚"""
    # åˆ›å»ºæ–°çš„Reqå¯¹è±¡
    req = Req(
        recv_req.rid,
        recv_req.input_text,
        recv_req.input_ids,
        recv_req.sampling_params,
        return_logprob=recv_req.return_logprob,
        stream=recv_req.stream,
        lora_id=recv_req.lora_id,
    )
    req.tokenizer = self.tokenizer
    
    # æ·»åŠ åˆ°ç­‰å¾…é˜Ÿåˆ—
    self.waiting_queue.append(req)
```

**handle_embedding_requestæ–¹æ³•**å¤„ç†åµŒå…¥ç±»è¯·æ±‚ï¼Œæ”¯æŒå‘é‡æ£€ç´¢ç­‰åº”ç”¨åœºæ™¯ï¼š

```python
def handle_embedding_request(self, recv_req: TokenizedEmbeddingReqInput):
    """å¤„ç†åµŒå…¥è¯·æ±‚"""
    # åˆ›å»ºåµŒå…¥è¯·æ±‚å¯¹è±¡
    req = Req(
        rid=recv_req.rid,
        input_text=recv_req.input_text,
        input_ids=recv_req.input_ids,
        sampling_params=SamplingParams(),  # åµŒå…¥è¯·æ±‚ä½¿ç”¨é»˜è®¤é‡‡æ ·å‚æ•°
        return_logprob=recv_req.return_logprob,
        stream=False,  # åµŒå…¥è¯·æ±‚ä¸æ”¯æŒæµå¼è¾“å‡º
        is_embedding=True,  # æ ‡è®°ä¸ºåµŒå…¥è¯·æ±‚
    )
    req.tokenizer = self.tokenizer
    
    # ç›´æ¥åŠ å…¥ç­‰å¾…é˜Ÿåˆ—
    self.waiting_queue.append(req)
```

### ğŸ”„ æ‰¹å¤„ç†ç®¡ç†

**get_next_batch_to_runæ–¹æ³•**æ˜¯æ‰¹å¤„ç†è°ƒåº¦çš„æ ¸å¿ƒï¼Œè´Ÿè´£ä»ç­‰å¾…é˜Ÿåˆ—ä¸­é€‰æ‹©è¯·æ±‚ç»„æˆæ‰¹æ¬¡ï¼š

```python
def get_next_batch_to_run(self) -> Optional[ScheduleBatch]:
    # åˆå¹¶é¢„å¡«å……æ‰¹æ¬¡åˆ°è¿è¡Œæ‰¹æ¬¡
    if self.last_batch and self.last_batch.forward_mode.is_extend():
        if not self.last_batch.is_empty():
            if self.running_batch.is_empty():
                self.running_batch = self.last_batch
            else:
                self.running_batch.merge_batch(self.last_batch)

    # è·å–æ–°çš„é¢„å¡«å……æ‰¹æ¬¡
    new_batch = self.get_new_batch_prefill()
    
    return new_batch or self.running_batch
```

**get_new_batch_prefillæ–¹æ³•**åˆ›å»ºæ–°çš„é¢„å¡«å……æ‰¹æ¬¡ï¼Œå®ç°åŠ¨æ€æ‰¹å¤„ç†å’Œè¿ç»­æ‰¹å¤„ç†ï¼š

```python
def get_new_batch_prefill(self) -> Optional[ScheduleBatch]:
    """è·å–æ–°çš„é¢„å¡«å……æ‰¹æ¬¡"""
    if not self.waiting_queue:
        return None
    
    # ä½¿ç”¨PrefillAdderæ™ºèƒ½æ·»åŠ è¯·æ±‚åˆ°æ‰¹æ¬¡
    adder = PrefillAdder(
        req_to_token_pool=self.req_to_token_pool,
        token_to_kv_pool=self.token_to_kv_pool,
        tree_cache=self.tree_cache,
    )
    
    # æ£€æŸ¥è¯­æ³•çº¦æŸé˜Ÿåˆ—
    if hasattr(self, 'grammar_queue') and self.grammar_queue:
        grammar_req = self.grammar_queue.popleft()
        can_run_list = [grammar_req]
    else:
        # ä»ç­‰å¾…é˜Ÿåˆ—ä¸­é€‰æ‹©å¯è¿è¡Œçš„è¯·æ±‚
        can_run_list = []
        while self.waiting_queue and adder.can_add_req(self.waiting_queue[0]):
            req = self.waiting_queue.popleft()
            adder.add_req(req)
            can_run_list.append(req)
    
    if can_run_list:
        # åˆ›å»ºæ–°çš„é¢„å¡«å……æ‰¹æ¬¡
        batch = ScheduleBatch(
            reqs=can_run_list,
            req_to_token_pool=self.req_to_token_pool,
            token_to_kv_pool=self.token_to_kv_pool,
            tree_cache=self.tree_cache,
            forward_mode=ForwardMode.EXTEND,
        )
        return batch
    
    return None
```

**run_batchæ–¹æ³•**æ‰§è¡Œæ‰¹æ¬¡æ¨ç†ï¼Œè°ƒç”¨åº•å±‚çš„æ¨¡å‹è¿è¡Œå™¨è¿›è¡ŒGPUè®¡ç®—ï¼š

```python
def run_batch(self, batch: ScheduleBatch):
    """æ‰§è¡Œæ‰¹æ¬¡æ¨ç†"""
    if batch.forward_mode.is_decode() or batch.forward_mode.is_mixed():
        # è§£ç é˜¶æ®µ
        model_worker_batch = batch.get_model_worker_batch()
        result = self.tp_worker.forward_batch_generation(model_worker_batch)
    else:
        # é¢„å¡«å……é˜¶æ®µ  
        result = self.tp_worker.forward_batch_generation(model_worker_batch)
    
    return result
```

**process_batch_resultæ–¹æ³•**å¤„ç†æ¨ç†ç»“æœï¼ŒåŒ…æ‹¬è¾“å‡ºè§£ç ã€æµå¼ä¼ è¾“å’Œè¯·æ±‚å®Œæˆæ£€æŸ¥ï¼š

```python
def process_batch_result(self, batch: ScheduleBatch, result):
    """å¤„ç†æ‰¹æ¬¡æ¨ç†ç»“æœ"""
    if result is None:
        return
    
    # å¤„ç†æ¯ä¸ªè¯·æ±‚çš„è¾“å‡º
    finished_reqs = []
    for i, req in enumerate(batch.reqs):
        # è§£ç æ–°ç”Ÿæˆçš„token
        if result.next_token_ids is not None:
            new_token_id = result.next_token_ids[i]
            req.output_ids.append(new_token_id)
            
            # æ£€æŸ¥æ˜¯å¦å®Œæˆç”Ÿæˆ
            if self.check_finished(req, new_token_id):
                finished_reqs.append(req)
        
        # å¤„ç†æµå¼è¾“å‡º
        if req.stream and result.next_token_ids is not None:
            self.send_stream_output(req, result.next_token_ids[i])
    
    # ç§»é™¤å·²å®Œæˆçš„è¯·æ±‚
    for req in finished_reqs:
        if req in batch.reqs:
            batch.reqs.remove(req)
            self.release_resources_for_request(req)
        
        # å‘é€æœ€ç»ˆè¾“å‡º
        self.send_final_output(req)
    
    # æ›´æ–°æ‰¹æ¬¡çŠ¶æ€
    if not batch.reqs:
        batch.forward_mode = ForwardMode.IDLE

def check_finished(self, req: Req, new_token_id: int) -> bool:
    """æ£€æŸ¥è¯·æ±‚æ˜¯å¦å®Œæˆ"""
    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§é•¿åº¦
    if len(req.output_ids) >= req.sampling_params.max_new_tokens:
        return True
    
    # æ£€æŸ¥æ˜¯å¦é‡åˆ°ç»“æŸtoken
    if new_token_id in req.sampling_params.stop_token_ids:
        return True
    
    # æ£€æŸ¥æ˜¯å¦é‡åˆ°åœæ­¢å­—ç¬¦ä¸²
    if req.sampling_params.stop_strs:
        output_text = self.tokenizer.decode(req.output_ids)
        for stop_str in req.sampling_params.stop_strs:
            if stop_str in output_text:
                return True
    
    return False
```

### ğŸ”§ ç³»ç»Ÿç»´æŠ¤

**self_check_during_idleæ–¹æ³•**åœ¨ç³»ç»Ÿç©ºé—²æ—¶æ‰§è¡Œè‡ªæ£€å’ŒçŠ¶æ€é‡ç½®ï¼Œç¡®ä¿ç³»ç»Ÿçš„é•¿æœŸç¨³å®šè¿è¡Œï¼š

```python
def self_check_during_idle(self):
    """ç³»ç»Ÿç©ºé—²æ—¶çš„è‡ªæ£€å’Œç»´æŠ¤"""
    # æ‰§è¡Œå†…å­˜æ³„æ¼æ£€æŸ¥
    self.check_memory()
    
    # å¦‚æœæœ‰ç©ºé—²ç¡çœ å™¨ï¼Œå¯èƒ½è¿›å…¥ç¡çœ èŠ‚èƒ½æ¨¡å¼
    self.maybe_sleep_on_idle()
    
    # é‡ç½®ä¸€äº›ç»Ÿè®¡è®¡æ•°å™¨
    if self.forward_ct % 1000 == 0:
        logger.info(f"Forward count: {self.forward_ct}")
```

**flush_cacheæ–¹æ³•**æä¾›ç¼“å­˜æ¸…ç†åŠŸèƒ½ï¼Œæ”¯æŒç³»ç»ŸçŠ¶æ€é‡ç½®ï¼š

```python
def flush_cache(self) -> bool:
    """åˆ·æ–°æ‰€æœ‰ç¼“å­˜ï¼Œé‡ç½®è°ƒåº¦å™¨çŠ¶æ€"""
    try:
        # æ¸…ç©ºæ‰€æœ‰é˜Ÿåˆ—
        self.waiting_queue.clear()
        self.running_batch = ScheduleBatch(reqs=[], batch_is_full=False)
        
        # åˆ·æ–°KVç¼“å­˜
        if hasattr(self, 'tree_cache'):
            self.tree_cache.reset()
        
        # é‡ç½®å†…å­˜æ± 
        if hasattr(self, 'req_to_token_pool'):
            self.req_to_token_pool.clear()
            
        # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
        self.forward_ct = 0
        
        logger.info("Cache flushed successfully")
        return True
    except Exception as e:
        logger.error(f"Cache flush failed: {e}")
        return False
```

**watchdog_threadæ–¹æ³•**å®ç°çœ‹é—¨ç‹—æœºåˆ¶ï¼Œç›‘æ§ç³»ç»Ÿå¥åº·çŠ¶æ€ï¼š

```python
def watchdog_thread(self):
    """çœ‹é—¨ç‹—çº¿ç¨‹ï¼Œç›‘æ§è°ƒåº¦å™¨å¥åº·çŠ¶æ€"""
    while True:
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰åƒµæ­»çš„è¯·æ±‚
            current_time = time.time()
            for req in self.running_batch.reqs:
                if current_time - req.start_time > self.request_timeout:
                    logger.warning(f"Request {req.rid} timeout, aborting")
                    self.abort_request(AbortReq(req.rid))
            
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
            if self.get_memory_usage() > 0.9:
                logger.warning("High memory usage detected")
                
            time.sleep(10)  # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
        except Exception as e:
            logger.error(f"Watchdog error: {e}")
```

---

## ğŸ’¾ å†…å­˜å’Œèµ„æºç®¡ç†

è°ƒåº¦å™¨ç»Ÿä¸€ç®¡ç†ç³»ç»Ÿçš„å†…å­˜å’Œè®¡ç®—èµ„æºã€‚åœ¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­ï¼Œè°ƒåº¦å™¨ä¼šåˆ›å»ºå’Œé…ç½®å„ç§èµ„æºç®¡ç†ç»„ä»¶ï¼š

### ğŸ¯ æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ

```python
def init_memory_pool_and_cache(self):
    """å†…å­˜æ± å’Œç¼“å­˜ç³»ç»Ÿåˆå§‹åŒ–çš„æ ¸å¿ƒæ¦‚å¿µ"""
    # 1. è¯·æ±‚åˆ°tokenæ˜ å°„æ±  - ç®¡ç†å¹¶å‘è¯·æ±‚æ§½ä½
    self.req_to_token_pool = ReqToTokenPool(
        size=self.max_running_requests,
        pre_alloc_size=self.pre_alloc_size,
    )
    
    # 2. KVç¼“å­˜åˆ†é…å™¨ - ç®¡ç†æ³¨æ„åŠ›é”®å€¼ç¼“å­˜
    if self.is_hybrid:
        # SWAæ··åˆç¼“å­˜æ¶æ„
        self.token_to_kv_pool = HybridKVPoolAllocator(...)
    else:
        # æ ‡å‡†ç¼“å­˜æ¶æ„
        self.token_to_kv_pool = BaseTokenToKVPoolAllocator(...)
    
    # 3. å‰ç¼€ç¼“å­˜ - ä¼˜åŒ–é‡å¤å‰ç¼€çš„ç¼“å­˜å‘½ä¸­
    self.tree_cache = RadixCache(...)  # æˆ–ChunkCache
```

### ğŸ” æºç å®ç°ç»†èŠ‚

```python
def init_memory_pool_and_cache(self):
    """çœŸå®çš„SGLangæºç å®ç°"""
    server_args = self.server_args

    # å†…å­˜æ± ä»tp_workerè·å–ï¼Œè€Œéç›´æ¥åˆ›å»º
    self.req_to_token_pool, self.token_to_kv_pool_allocator = (
        self.tp_worker.get_memory_pool()
    )

    # å¤æ‚çš„ç¼“å­˜ç±»å‹é€‰æ‹©é€»è¾‘
    if (
        server_args.chunked_prefill_size is not None
        and server_args.disable_radix_cache
    ):
        # å—ç¼“å­˜åˆ†æ”¯
        if self.is_hybrid:
            ChunkCacheClass = SWAChunkCache
        else:
            ChunkCacheClass = ChunkCache
        self.tree_cache = ChunkCacheClass(
            req_to_token_pool=self.req_to_token_pool,
            token_to_kv_pool_allocator=self.token_to_kv_pool_allocator,
            page_size=self.page_size,
        )
    else:
        # RadixCacheåˆ†æ”¯
        if os.environ.get("SGLANG_EXPERIMENTAL_CPP_RADIX_TREE") == "1":
            # C++å®éªŒæ€§å®ç°
            from sglang.srt.mem_cache.radix_cache_cpp import RadixCacheCpp
            self.tree_cache = RadixCacheCpp(
                disable=False,
                use_hicache=self.enable_hierarchical_cache,
                req_to_token_pool=self.req_to_token_pool,
                token_to_kv_pool=self.token_to_kv_pool_allocator,
                tp_cache_group=self.tp_cpu_group,
                page_size=self.page_size,
                hicache_ratio=server_args.hicache_ratio,
                hicache_size=server_args.hicache_size,
                hicache_write_policy=server_args.hicache_write_policy,
                enable_kv_cache_events=self.enable_kv_cache_events,
            )
        elif self.enable_hierarchical_cache:
            # åˆ†å±‚ç¼“å­˜å®ç°
            if self.enable_lora:
                self.tree_cache = LoRARadixCache(...)
            elif self.is_hybrid:
                self.tree_cache = SWARadixCache(...)
            else:
                self.tree_cache = HiRadixCache(...)
        else:
            # æ ‡å‡†RadixCache
            if self.enable_lora:
                self.tree_cache = LoRARadixCache(...)
            elif self.is_hybrid:
                self.tree_cache = SWARadixCache(...)
            else:
                self.tree_cache = RadixCache(...)

ğŸ’¡ **å®ç°è¯´æ˜**: æºç ä¸­æœ‰10+ç§ç¼“å­˜ç±»å‹ï¼Œæ ¹æ®chunked_prefillã€LoRAã€SWAã€åˆ†å±‚ç¼“å­˜ç­‰ä¸åŒé…ç½®ç»„åˆé€‰æ‹©ã€‚æ•™å­¦ç‰ˆæœ¬çªå‡ºæ ¸å¿ƒçš„"è¯·æ±‚æ± â†’KVç¼“å­˜â†’å‰ç¼€ç¼“å­˜"è®¾è®¡æ¨¡å¼ã€‚
```

**èµ„æºåˆ†é…ç¤ºä¾‹**ï¼š
```python
def allocate_resources_for_request(self, req: Req):
    """ä¸ºè¯·æ±‚åˆ†é…èµ„æº"""
    # åˆ†é…tokenæ§½ä½
    if not self.req_to_token_pool.available():
        logger.warning("No available request slots")
        return False
        
    req_pool_idx = self.req_to_token_pool.alloc()
    req.req_pool_idx = req_pool_idx
    
    # åˆ†é…KVç¼“å­˜
    num_tokens = len(req.origin_input_ids)
    if self.token_to_kv_pool.available_size() < num_tokens:
        logger.warning("Insufficient KV cache space")
        self.req_to_token_pool.free(req_pool_idx)
        return False
    
    # ä¸ºè¯·æ±‚åˆ†é…tokenæ± 
    req.token_pool_indices = self.token_to_kv_pool.alloc(num_tokens)
    
    return True

def release_resources_for_request(self, req: Req):
    """é‡Šæ”¾è¯·æ±‚å ç”¨çš„èµ„æº"""
    if hasattr(req, 'req_pool_idx'):
        self.req_to_token_pool.free(req.req_pool_idx)
    
    if hasattr(req, 'token_pool_indices'):
        self.token_to_kv_pool.free(req.token_pool_indices)
```

è¿™äº›ç»„ä»¶ååŒå·¥ä½œï¼Œç¡®ä¿å†…å­˜çš„é«˜æ•ˆä½¿ç”¨å’Œç¼“å­˜çš„æ™ºèƒ½ç®¡ç†ï¼ŒåŒæ—¶æ”¯æŒè¯·æ±‚çš„åŠ¨æ€èµ„æºåˆ†é…å’Œå›æ”¶ã€‚

---

## ğŸŒ å¹¶è¡Œç­–ç•¥åè°ƒ

åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹ï¼Œè°ƒåº¦å™¨éœ€è¦åè°ƒå¤šç§å¹¶è¡Œç­–ç•¥ï¼š

### ğŸ”€ å¼ é‡å¹¶è¡Œåè°ƒ
```python
def coordinate_tensor_parallel(self):
    """å¼ é‡å¹¶è¡Œåè°ƒæœºåˆ¶"""
    # ç¡®ä¿æ‰€æœ‰TP workeråŒæ­¥
    if self.tp_size > 1:
        # å¹¿æ’­æ‰¹æ¬¡ä¿¡æ¯åˆ°æ‰€æœ‰TP ranks
        if self.tp_rank == 0:
            # ä¸»rankè´Ÿè´£è°ƒåº¦å†³ç­–
            batch_info = {
                'batch_size': len(self.running_batch.reqs),
                'forward_mode': self.running_batch.forward_mode,
                'request_ids': [req.rid for req in self.running_batch.reqs]
            }
            # å¹¿æ’­åˆ°å…¶ä»–ranks
            self.broadcast_to_tp_group(batch_info)
        else:
            # ä»å±ranksæ¥æ”¶è°ƒåº¦ä¿¡æ¯
            batch_info = self.receive_from_tp_master()
```

### ğŸš€ æµæ°´çº¿å¹¶è¡Œç®¡ç†
```python
def manage_pipeline_parallel(self):
    """æµæ°´çº¿å¹¶è¡Œç®¡ç†"""
    if self.pp_size > 1:
        # å¤„ç†pipeline stageé—´çš„æ•°æ®ä¼ é€’
        if self.pp_rank == 0:
            # ç¬¬ä¸€é˜¶æ®µï¼šå¤„ç†è¾“å…¥
            hidden_states = self.process_input_stage(batch)
            self.send_to_next_stage(hidden_states)
        elif self.pp_rank == self.pp_size - 1:
            # æœ€åé˜¶æ®µï¼šç”Ÿæˆè¾“å‡º
            hidden_states = self.receive_from_prev_stage()
            output = self.process_output_stage(hidden_states)
        else:
            # ä¸­é—´é˜¶æ®µï¼šä¼ é€’éšè—çŠ¶æ€
            hidden_states = self.receive_from_prev_stage()
            hidden_states = self.process_middle_stage(hidden_states)
            self.send_to_next_stage(hidden_states)
```

### âš–ï¸ æ•°æ®å¹¶è¡Œè´Ÿè½½å‡è¡¡
```python
def balance_data_parallel_load(self):
    """æ•°æ®å¹¶è¡Œè´Ÿè½½å‡è¡¡"""
    if self.dp_size > 1:
        # æ”¶é›†å„å‰¯æœ¬çš„è´Ÿè½½ä¿¡æ¯
        local_load = {
            'running_requests': len(self.running_batch.reqs),
            'waiting_requests': len(self.waiting_queue),
            'memory_usage': self.get_memory_usage()
        }
        
        # ä¸å…¶ä»–å‰¯æœ¬äº¤æ¢è´Ÿè½½ä¿¡æ¯
        all_loads = self.gather_dp_loads(local_load)
        
        # æ ¹æ®è´Ÿè½½å·®å¼‚è¿›è¡Œè¯·æ±‚é‡åˆ†é…
        if self.dp_rank == 0:  # ä¸»å‰¯æœ¬è´Ÿè´£è°ƒåº¦
            self.redistribute_requests(all_loads)
```

### ğŸ§  ä¸“å®¶å¹¶è¡Œè°ƒåº¦
```python
def schedule_expert_parallel(self, expert_distribution: Dict):
    """MoEä¸“å®¶å¹¶è¡Œè°ƒåº¦"""
    if self.moe_ep_size > 1:
        # æ ¹æ®tokenè·¯ç”±åˆ°ä¸åŒä¸“å®¶
        for req in self.running_batch.reqs:
            expert_ids = req.get_expert_routing()
            for expert_id in expert_ids:
                target_ep_rank = expert_id % self.moe_ep_size
                if target_ep_rank == self.moe_ep_rank:
                    # å½“å‰rankå¤„ç†è¿™ä¸ªä¸“å®¶
                    self.process_expert_tokens(req, expert_id)
                else:
                    # å‘é€åˆ°å¯¹åº”çš„ä¸“å®¶rank
                    self.send_to_expert_rank(req, expert_id, target_ep_rank)
```

è¿™äº›å¹¶è¡Œç­–ç•¥ç¡®ä¿äº†SGLangèƒ½å¤Ÿåœ¨å¤§è§„æ¨¡åˆ†å¸ƒå¼ç¯å¢ƒä¸‹é«˜æ•ˆè¿è¡Œï¼Œæœ€å¤§åŒ–ç¡¬ä»¶èµ„æºåˆ©ç”¨ç‡ã€‚

---

## ğŸ“Š æ€§èƒ½ç›‘æ§å’Œè°ƒè¯•

è°ƒåº¦å™¨å†…ç½®äº†å®Œå–„çš„æ€§èƒ½ç›‘æ§å’Œè°ƒè¯•åŠŸèƒ½ï¼š

### ğŸ“ˆ å…³é”®æŒ‡æ ‡æ”¶é›†
```python
def collect_performance_metrics(self):
    """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
    # é€šè¿‡SchedulerMetricsMixinæ”¶é›†æŒ‡æ ‡
    current_time = time.time()
    
    # ååé‡è®¡ç®—
    if hasattr(self, 'last_metric_time'):
        time_diff = current_time - self.last_metric_time
        tokens_processed = self.forward_ct - self.last_forward_ct
        self.last_gen_throughput = tokens_processed / time_diff if time_diff > 0 else 0
    
    # èµ„æºåˆ©ç”¨ç‡
    token_usage = self.get_token_usage_ratio()
    memory_usage = self.get_memory_usage_ratio()
    
    # é˜Ÿåˆ—çŠ¶æ€
    queue_stats = {
        'running_requests': len(self.running_batch.reqs),
        'waiting_requests': len(self.waiting_queue),
        'total_requests_processed': self.total_requests_processed,
    }
    
    # ç¼“å­˜å‘½ä¸­ç‡
    if self.tree_cache:
        cache_hit_rate = self.tree_cache.get_hit_rate()
    else:
        cache_hit_rate = 0.0
    
    # æ›´æ–°æŒ‡æ ‡
    self.stats.update(
        gen_throughput=self.last_gen_throughput,
        token_usage=token_usage,
        memory_usage=memory_usage,
        cache_hit_rate=cache_hit_rate,
        **queue_stats
    )
```

### ğŸ” æ€§èƒ½åˆ†æå·¥å…·
```python
def start_profiling(self, profile_config: ProfileConfig):
    """å¯åŠ¨æ€§èƒ½åˆ†æ"""
    # é€šè¿‡SchedulerProfilerMixinå¯åŠ¨profiler
    import torch.profiler as profiler
    
    self.torch_profiler = profiler.profile(
        activities=[
            profiler.ProfilerActivity.CPU,
            profiler.ProfilerActivity.CUDA,
        ],
        schedule=profiler.schedule(
            wait=profile_config.wait_steps,
            warmup=profile_config.warmup_steps, 
            active=profile_config.active_steps,
            repeat=profile_config.repeat
        ),
        on_trace_ready=self.save_profiler_trace,
        record_shapes=profile_config.record_shapes,
        with_stack=profile_config.with_stack,
    )
    
    self.torch_profiler.start()
    logger.info("Performance profiling started")

def save_profiler_trace(self, prof):
    """ä¿å­˜æ€§èƒ½åˆ†æç»“æœ"""
    output_path = f"{self.profiler_output_dir}/trace_{self.forward_ct}.json"
    prof.export_chrome_trace(output_path)
    logger.info(f"Profiler trace saved to {output_path}")
```

### ğŸ› ï¸ è°ƒè¯•å·¥å…·
```python
def dump_scheduler_state(self):
    """è½¬å‚¨è°ƒåº¦å™¨çŠ¶æ€ç”¨äºè°ƒè¯•"""
    state_info = {
        'timestamp': time.time(),
        'forward_count': self.forward_ct,
        'running_batch': {
            'size': len(self.running_batch.reqs),
            'forward_mode': str(self.running_batch.forward_mode),
            'requests': [req.rid for req in self.running_batch.reqs[:5]]  # åªæ˜¾ç¤ºå‰5ä¸ª
        },
        'waiting_queue': {
            'size': len(self.waiting_queue),
            'requests': [req.rid for req in self.waiting_queue[:5]]
        },
        'memory_stats': {
            'token_usage': self.get_token_usage_ratio(),
            'available_tokens': self.token_to_kv_pool.available_size(),
            'req_pool_usage': len(self.req_to_token_pool.free_slots)
        },
        'parallel_config': {
            'tp_rank': self.tp_rank,
            'tp_size': self.tp_size,
            'pp_rank': self.pp_rank,
            'dp_rank': self.dp_rank
        }
    }
    
    logger.info(f"Scheduler state dump: {json.dumps(state_info, indent=2)}")
    return state_info

def monitor_request_lifecycle(self, req: Req, event: str):
    """ç›‘æ§è¯·æ±‚ç”Ÿå‘½å‘¨æœŸ"""
    if self.enable_request_tracing:
        trace_info = {
            'request_id': req.rid,
            'event': event,
            'timestamp': time.time(),
            'queue_position': len(self.waiting_queue) if event == 'queued' else None,
            'batch_size': len(self.running_batch.reqs) if event == 'processing' else None
        }
        self.request_traces.append(trace_info)
```

è¿™äº›åŠŸèƒ½ä¸ºç³»ç»Ÿä¼˜åŒ–å’Œé—®é¢˜æ’æŸ¥æä¾›äº†å¼ºæœ‰åŠ›çš„å·¥å…·ï¼Œå¸®åŠ©å¼€å‘è€…è¯†åˆ«æ€§èƒ½ç“¶é¢ˆå’Œä¼˜åŒ–æœºä¼šã€‚

---

## ğŸ“ æ€»ç»“

SGLangè°ƒåº¦å™¨é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ¶æ„å’Œä¸°å¯Œçš„åŠŸèƒ½ï¼Œä¸ºé«˜æ€§èƒ½è¯­è¨€æ¨¡å‹æ¨ç†æä¾›äº†å¼ºå¤§çš„è°ƒåº¦èƒ½åŠ›ï¼š

### ğŸ¯ è®¾è®¡äº®ç‚¹

**æ¨¡å—åŒ–æ¶æ„**: Mixinæ¨¡å¼ç¡®ä¿äº†åŠŸèƒ½çš„æ¨¡å—åŒ–å’Œå¯æ‰©å±•æ€§ï¼Œ6ä¸ªMixinç±»åˆ†åˆ«è´Ÿè´£è¾“å‡ºå¤„ç†ã€æƒé‡æ›´æ–°ã€æ€§èƒ½åˆ†æã€æŒ‡æ ‡æ”¶é›†å’Œåˆ†ç¦»å¼æ¶æ„æ”¯æŒã€‚

**å¤šæ ·åŒ–äº‹ä»¶å¾ªç¯**: æä¾›7ç§ä¸åŒçš„äº‹ä»¶å¾ªç¯ç»„åˆï¼Œä»æ ‡å‡†åŒæ­¥åˆ°CPU-GPUé‡å ï¼Œä»æµæ°´çº¿å¹¶è¡Œåˆ°åˆ†ç¦»å¼æ¶æ„ï¼Œé€‚åº”å„ç§éƒ¨ç½²åœºæ™¯ã€‚

**æ™ºèƒ½è¯·æ±‚åˆ†å‘**: TypeBasedDispatcheræ”¯æŒ20+ç§è¯·æ±‚ç±»å‹çš„è‡ªåŠ¨è·¯ç”±ï¼Œä»åŸºæœ¬çš„ç”Ÿæˆ/åµŒå…¥è¯·æ±‚åˆ°é«˜çº§çš„ç³»ç»Ÿç®¡ç†åŠŸèƒ½ã€‚

**çµæ´»èµ„æºç®¡ç†**: æ”¯æŒ10+ç§ç¼“å­˜ç±»å‹ç»„åˆï¼ŒåŒ…æ‹¬RadixCacheã€ChunkCacheã€SWAæ··åˆç¼“å­˜ã€åˆ†å±‚ç¼“å­˜ç­‰ï¼Œæ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆã€‚

**å…¨é¢ç›‘æ§èƒ½åŠ›**: å†…ç½®å®Œå–„çš„æ€§èƒ½ç›‘æ§ã€è°ƒè¯•å·¥å…·å’ŒçŠ¶æ€ç®¡ç†ï¼Œæ”¯æŒPyTorch Profileré›†æˆå’Œå®æ—¶æŒ‡æ ‡æ”¶é›†ã€‚

### ğŸ”§ å®ç°ç‰¹è‰²

**æºç å‡†ç¡®æ€§**: æœ¬æ–‡æ¡£åŸºäºçœŸå®SGLangæºç ç¼–å†™ï¼Œæ‰€æœ‰ä»£ç ç¤ºä¾‹éƒ½æ¥è‡ªå®é™…å®ç°ï¼Œç¡®ä¿æŠ€æœ¯å‡†ç¡®æ€§ã€‚

**æ•™å­¦ä¸å®è·µå¹¶é‡**: é‡‡ç”¨"æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ + æºç å®ç°ç»†èŠ‚"çš„åŒé‡ç»“æ„ï¼Œæ—¢ä¾¿äºç†è§£æ ¸å¿ƒæ€æƒ³ï¼Œåˆæä¾›å®ç°å‚è€ƒã€‚

**æ¸è¿›å¼å­¦ä¹ **: ä»ç®€åŒ–çš„æ•™å­¦ç‰ˆæœ¬å¼€å§‹ï¼Œé€æ­¥æ·±å…¥åˆ°å¤æ‚çš„æºç å®ç°ï¼Œé€‚åˆä¸åŒå±‚æ¬¡çš„å­¦ä¹ éœ€æ±‚ã€‚

ç†è§£è°ƒåº¦å™¨çš„è®¾è®¡ç†å¿µå’Œå·¥ä½œåŸç†ï¼Œæ˜¯æŒæ¡SGLangç³»ç»Ÿçš„å…³é”®æ‰€åœ¨ã€‚é€šè¿‡æœ¬æ–‡æ¡£çš„å­¦ä¹ ï¼Œå¼€å‘è€…å¯ä»¥æ·±å…¥ç†è§£SGLangè°ƒåº¦å™¨çš„æ¶æ„ç²¾é«“ï¼Œä¸ºåç»­çš„å®šåˆ¶å¼€å‘å’Œæ€§èƒ½ä¼˜åŒ–æ‰“ä¸‹åšå®åŸºç¡€ã€‚
