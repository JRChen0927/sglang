# è°ƒåº¦å™¨æ€»è§ˆä¸æ¶æ„

---

SGLangçš„è°ƒåº¦å™¨æ˜¯æ•´ä¸ªæ¨ç†ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£ç®¡ç†å¼ é‡å¹¶è¡ŒGPU workerçš„å·¥ä½œåè°ƒã€‚ä½œä¸ºç³»ç»Ÿçš„"å¤§è„‘"ï¼Œè°ƒåº¦å™¨æ‰¿æ‹…ç€è¯·æ±‚è°ƒåº¦ã€æ‰¹å¤„ç†ä¼˜åŒ–ã€èµ„æºç®¡ç†å’Œå¹¶è¡Œåè°ƒç­‰å…³é”®èŒè´£ã€‚

---

## ğŸ¯ æ ¸å¿ƒèŒè´£

è°ƒåº¦å™¨çš„ä¸»è¦èŒè´£åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š

**è¯·æ±‚ç”Ÿå‘½å‘¨æœŸç®¡ç†**  
ä»æ¥æ”¶ç”¨æˆ·è¯·æ±‚å¼€å§‹ï¼Œè°ƒåº¦å™¨è´Ÿè´£æ•´ä¸ªè¯·æ±‚çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ŒåŒ…æ‹¬è¯·æ±‚éªŒè¯ã€æ’é˜Ÿè°ƒåº¦ã€æ‰¹æ¬¡ç»„ç»‡ã€æ¨ç†æ‰§è¡Œå’Œç»“æœè¿”å›çš„å…¨æµç¨‹æ§åˆ¶ã€‚

**æ‰¹å¤„ç†ä¼˜åŒ–**  
è°ƒåº¦å™¨å®ç°äº†åŠ¨æ€æ‰¹å¤„ç†æœºåˆ¶ï¼Œèƒ½å¤Ÿæ™ºèƒ½åœ°å°†å¤šä¸ªè¯·æ±‚ç»„ç»‡æˆæ‰¹æ¬¡è¿›è¡Œå¤„ç†ï¼Œæœ€å¤§åŒ–GPUçš„åˆ©ç”¨ç‡å’Œç³»ç»Ÿååé‡ã€‚

**èµ„æºç»Ÿä¸€è°ƒåº¦**  
è°ƒåº¦å™¨ç»Ÿä¸€ç®¡ç†GPUå†…å­˜ã€KVç¼“å­˜ã€è®¡ç®—èµ„æºç­‰ç³»ç»Ÿèµ„æºï¼Œç¡®ä¿èµ„æºçš„é«˜æ•ˆåˆ†é…å’Œä½¿ç”¨ã€‚

**å¤šç»´å¹¶è¡Œåè°ƒ**  
åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹ï¼Œè°ƒåº¦å™¨åè°ƒå¼ é‡å¹¶è¡Œã€æµæ°´çº¿å¹¶è¡Œã€æ•°æ®å¹¶è¡Œå’Œä¸“å®¶å¹¶è¡Œç­‰å¤šç§å¹¶è¡Œç­–ç•¥ã€‚

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### ğŸ§© Mixinæ¨¡å¼è®¾è®¡

SGLangè°ƒåº¦å™¨é‡‡ç”¨å¤šé‡ç»§æ‰¿çš„Mixinæ¨¡å¼ï¼Œå°†ä¸åŒåŠŸèƒ½æ¨¡å—è§£è€¦ï¼Œå®ç°é«˜åº¦çš„æ¨¡å—åŒ–è®¾è®¡ã€‚

**è°ƒåº¦å™¨ç±»å®šä¹‰çš„æ ¸å¿ƒæ€è·¯**ï¼šé€šè¿‡ç»§æ‰¿6ä¸ªMixinç±»ï¼Œæ¯ä¸ªç±»ä¸“æ³¨äºç‰¹å®šåŠŸèƒ½é¢†åŸŸï¼Œé¿å…äº†å•ä¸€ç±»è¿‡äºåºå¤§çš„é—®é¢˜ã€‚è¿™ç§è®¾è®¡è®©ä»£ç æ›´åŠ æ¸…æ™°ï¼Œä¾¿äºç»´æŠ¤å’Œæ‰©å±•ã€‚è°ƒåº¦å™¨çš„åˆå§‹åŒ–è¿‡ç¨‹åŒ…å«å¹¶è¡Œé…ç½®è§£æã€æ ¸å¿ƒç»„ä»¶åˆ›å»ºå’Œè¯·æ±‚åˆ†å‘å™¨è®¾ç½®ä¸‰ä¸ªå…³é”®æ­¥éª¤ã€‚

```python
class Scheduler(
    SchedulerOutputProcessorMixin,
    SchedulerUpdateWeightsMixin,
    SchedulerProfilerMixin,
    SchedulerMetricsMixin,
    SchedulerDisaggregationDecodeMixin,
    SchedulerDisaggregationPrefillMixin,
):
    """A scheduler that manages a tensor parallel GPU worker."""
    
    def __init__(self, server_args: ServerArgs, port_args: PortArgs, 
                 gpu_id: int, tp_rank: int, moe_ep_rank: int, 
                 pp_rank: int, dp_rank: Optional[int]):
        # è§£æå¹¶è¡Œé…ç½®å‚æ•°
        self.server_args = server_args
        self.tp_rank = tp_rank              # å¼ é‡å¹¶è¡Œrank
        self.moe_ep_rank = moe_ep_rank      # ä¸“å®¶å¹¶è¡Œrank  
        self.pp_rank = pp_rank              # æµæ°´çº¿å¹¶è¡Œrank
        self.dp_rank = dp_rank              # æ•°æ®å¹¶è¡Œrank
        self.tp_size = server_args.tp_size  # å¼ é‡å¹¶è¡Œå¤§å°
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.running_batch = ScheduleBatch(reqs=[], batch_is_full=False)
        self.waiting_queue = []
        self.forward_ct = 0
        
        # åˆå§‹åŒ–è¯·æ±‚åˆ†å‘å™¨
        self._request_dispatcher = TypeBasedDispatcher([
            (TokenizedGenerateReqInput, self.handle_generate_request),
            (TokenizedEmbeddingReqInput, self.handle_embedding_request),
            (FlushCacheReqInput, self.flush_cache_wrapped),
            # ... æ›´å¤šè¯·æ±‚ç±»å‹æ˜ å°„
        ])
```

æ¯ä¸ªMixinç±»è´Ÿè´£ç‰¹å®šçš„åŠŸèƒ½é¢†åŸŸï¼Œè¿™ç§è®¾è®¡å¸¦æ¥äº†è‰¯å¥½çš„ä»£ç ç»„ç»‡å’Œå¯ç»´æŠ¤æ€§ï¼š

**SchedulerOutputProcessorMixin**: å¤„ç†æ¨¡å‹è¾“å‡ºå’Œæµå¼å“åº”ï¼ŒåŒ…æ‹¬é¢„å¡«å……å’Œè§£ç ç»“æœçš„å¤„ç†ã€æµå¼ä¼ è¾“æ§åˆ¶ç­‰ã€‚

**SchedulerUpdateWeightsMixin**: æ”¯æŒåŠ¨æ€æƒé‡æ›´æ–°ï¼ŒåŒ…æ‹¬ä»ç£ç›˜åŠ è½½æƒé‡ã€åˆ†å¸ƒå¼æƒé‡æ›´æ–°ã€å¼ é‡æƒé‡æ›´æ–°ç­‰åŠŸèƒ½ã€‚

**åŠ¨æ€æƒé‡æ›´æ–°çš„å®ç°æœºåˆ¶**ï¼šè¿™æ˜¯SGLangçš„ä¸€ä¸ªé‡è¦ç‰¹æ€§ï¼Œå…è®¸åœ¨ä¸é‡å¯æœåŠ¡çš„æƒ…å†µä¸‹æ›´æ–°æ¨¡å‹æƒé‡ã€‚å®ç°è¿‡ç¨‹åŒ…æ‹¬æƒé‡åŠ è½½ã€ç¼“å­˜åˆ·æ–°å’ŒçŠ¶æ€éªŒè¯ä¸‰ä¸ªæ­¥éª¤ï¼Œç¡®ä¿æ›´æ–°è¿‡ç¨‹çš„å®‰å…¨æ€§å’Œä¸€è‡´æ€§ã€‚

**SchedulerProfilerMixin**: æä¾›æ€§èƒ½åˆ†æåŠŸèƒ½ï¼Œæ”¯æŒPyTorch profileré›†æˆå’Œè‡ªå®šä¹‰æ€§èƒ½æŒ‡æ ‡æ”¶é›†ã€‚

**SchedulerMetricsMixin**: è´Ÿè´£æŒ‡æ ‡æ”¶é›†ï¼ŒåŒ…æ‹¬ååé‡ã€å»¶è¿Ÿã€ç¼“å­˜å‘½ä¸­ç‡ç­‰å…³é”®æ€§èƒ½æŒ‡æ ‡çš„ç»Ÿè®¡ã€‚

**SchedulerDisaggregationDecodeMixin/PrefillMixin**: æ”¯æŒé¢„å¡«å……å’Œè§£ç åˆ†ç¦»çš„åˆ†ç¦»å¼æ¶æ„ï¼Œç”¨äºè¶…å¤§æ¨¡å‹çš„åˆ†å¸ƒå¼æ¨ç†ã€‚

### âš™ï¸ å¹¶è¡Œé…ç½®ç®¡ç†

è°ƒåº¦å™¨çš„åˆå§‹åŒ–è¿‡ç¨‹ä¸­ï¼Œéœ€è¦å¤„ç†å¤æ‚çš„å¹¶è¡Œé…ç½®ã€‚

**å¹¶è¡Œé…ç½®å‚æ•°çš„ä½œç”¨å’Œæ„ä¹‰**ï¼šè¿™äº›å‚æ•°å®šä¹‰äº†å½“å‰è°ƒåº¦å™¨åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„è§’è‰²å’Œä½ç½®ã€‚tp_rankç¡®å®šå¼ é‡å¹¶è¡Œä¸­çš„ä½ç½®ï¼Œmoe_ep_rankç”¨äºä¸“å®¶å¹¶è¡Œè·¯ç”±ï¼Œpp_rankå†³å®šæµæ°´çº¿é˜¶æ®µï¼Œdp_rankå¤„ç†æ•°æ®å¹¶è¡Œå‰¯æœ¬ã€‚ç†è§£è¿™äº›å‚æ•°æ˜¯æŒæ¡SGLangåˆ†å¸ƒå¼æ¶æ„çš„å…³é”®ã€‚

```python
def __init__(self, server_args: ServerArgs, port_args: PortArgs, gpu_id: int,
             tp_rank: int, moe_ep_rank: int, pp_rank: int, dp_rank: Optional[int]):
    # è§£æå¹¶è¡Œé…ç½®
    self.tp_rank = tp_rank              # å¼ é‡å¹¶è¡Œrank
    self.moe_ep_rank = moe_ep_rank      # ä¸“å®¶å¹¶è¡Œrank
    self.pp_rank = pp_rank              # æµæ°´çº¿å¹¶è¡Œrank
    self.dp_rank = dp_rank              # æ•°æ®å¹¶è¡Œrank
    self.tp_size = server_args.tp_size  # å¼ é‡å¹¶è¡Œå¤§å°
    self.moe_ep_size = server_args.ep_size
    self.pp_size = server_args.pp_size
    self.dp_size = server_args.dp_size
```

è¿™äº›å‚æ•°å®šä¹‰äº†å½“å‰è°ƒåº¦å™¨å®ä¾‹åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„ä½ç½®å’Œä½œç”¨èŒƒå›´ï¼Œæ˜¯åç»­æ‰€æœ‰å¹¶è¡Œåè°ƒå·¥ä½œçš„åŸºç¡€ã€‚

---

## ğŸ”„ äº‹ä»¶å¾ªç¯æ¶æ„

è°ƒåº¦å™¨çš„æ ¸å¿ƒå·¥ä½œæœºåˆ¶æ˜¯äº‹ä»¶å¾ªç¯ï¼Œæ ¹æ®ä¸åŒçš„é…ç½®å’Œä¼˜åŒ–éœ€æ±‚ï¼ŒSGLangæä¾›äº†å¤šç§äº‹ä»¶å¾ªç¯å®ç°ï¼š

**æ ‡å‡†äº‹ä»¶å¾ªç¯**  
event_loop_normalæ˜¯æœ€åŸºç¡€çš„äº‹ä»¶å¾ªç¯å®ç°ï¼Œé‡‡ç”¨ä¸²è¡Œå¤„ç†æ¨¡å¼ï¼Œé€»è¾‘æ¸…æ™°ï¼Œé€‚åˆè°ƒè¯•å’Œå¼€å‘ç¯å¢ƒã€‚

**é‡å äº‹ä»¶å¾ªç¯**  
event_loop_overlapå®ç°äº†CPUå¤„ç†å’ŒGPUè®¡ç®—çš„é‡å ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡ç³»ç»Ÿååé‡ï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒã€‚

**æµæ°´çº¿å¹¶è¡Œäº‹ä»¶å¾ªç¯**  
event_loop_ppä¸“é—¨ä¸ºæµæ°´çº¿å¹¶è¡Œè®¾è®¡ï¼Œæ”¯æŒå¤§æ¨¡å‹çš„åˆ†å±‚å¤„ç†ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨å¤šä¸ªGPUèŠ‚ç‚¹ã€‚

**åˆ†ç¦»å¼äº‹ä»¶å¾ªç¯**  
é’ˆå¯¹é¢„å¡«å……å’Œè§£ç åˆ†ç¦»çš„åœºæ™¯ï¼Œæä¾›äº†ä¸“é—¨çš„äº‹ä»¶å¾ªç¯å®ç°ï¼ŒåŒ…æ‹¬event_loop_overlap_disagg_prefillå’Œevent_loop_normal_disagg_decodeç­‰ã€‚

### ğŸ›ï¸ äº‹ä»¶å¾ªç¯é€‰æ‹©é€»è¾‘

### ğŸ¯ æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ

**äº‹ä»¶å¾ªç¯é€‰æ‹©çš„æ™ºèƒ½å†³ç­–æ ‘**ï¼šSGLangæ ¹æ®éƒ¨ç½²é…ç½®è‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆçš„äº‹ä»¶å¾ªç¯ç±»å‹ã€‚å†³ç­–é€»è¾‘é¦–å…ˆåˆ¤æ–­æ˜¯å¦å¯ç”¨åˆ†ç¦»å¼æ¶æ„ï¼Œç„¶åæ ¹æ®æµæ°´çº¿å¹¶è¡Œå’ŒCPU-GPUé‡å è®¾ç½®é€‰æ‹©å…·ä½“å®ç°ã€‚è¿™ç§è‡ªåŠ¨é€‰æ‹©æœºåˆ¶ç¡®ä¿äº†åœ¨ä¸åŒç¡¬ä»¶å’Œé…ç½®ä¸‹éƒ½èƒ½è·å¾—æœ€ä¼˜æ€§èƒ½ã€‚

```python
# äº‹ä»¶å¾ªç¯è‡ªåŠ¨é€‰æ‹©ç­–ç•¥
if disaggregation_mode == DisaggregationMode.NULL:
    if server_args.pp_size > 1:
        scheduler.event_loop_pp()          # æµæ°´çº¿å¹¶è¡Œ
    elif scheduler.enable_overlap:
        scheduler.event_loop_overlap()     # CPU-GPUé‡å 
    else:
        scheduler.event_loop_normal()      # æ ‡å‡†åŒæ­¥
elif disaggregation_mode == DisaggregationMode.PREFILL:
    scheduler.event_loop_disagg_prefill()  # åˆ†ç¦»å¼é¢„å¡«å……
elif disaggregation_mode == DisaggregationMode.DECODE:
    scheduler.event_loop_disagg_decode()   # åˆ†ç¦»å¼è§£ç 
```

**æ ‡å‡†äº‹ä»¶å¾ªç¯çš„å·¥ä½œåŸç†**ï¼šè¿™æ˜¯æœ€åŸºç¡€ä½†ä¹Ÿæ˜¯æœ€é‡è¦çš„äº‹ä»¶å¾ªç¯å®ç°ï¼Œé‡‡ç”¨ç®€å•çš„ä¸²è¡Œå¤„ç†æ¨¡å¼ã€‚æ¯æ¬¡å¾ªç¯åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒæ­¥éª¤ï¼šæ¥æ”¶å’Œå¤„ç†æ–°è¯·æ±‚ã€è·å–å¾…è¿è¡Œçš„æ‰¹æ¬¡ã€æ‰§è¡Œæ¨ç†æˆ–è¿›è¡Œç©ºé—²æ£€æŸ¥ã€‚è¿™ç§è®¾è®¡ç®€æ´æ˜äº†ï¼Œä¾¿äºç†è§£æ•´ä¸ªè°ƒåº¦æµç¨‹ã€‚

```python
@DynamicGradMode()
def event_loop_normal(self):
    """æ ‡å‡†è°ƒåº¦å™¨å¾ªç¯"""
    while True:
        # 1. æ¥æ”¶æ–°è¯·æ±‚
        recv_reqs = self.recv_requests()
        self.process_input_requests(recv_reqs)

        # 2. è·å–ä¸‹ä¸€ä¸ªæ‰¹æ¬¡
        batch = self.get_next_batch_to_run()
        self.cur_batch = batch

        # 3. æ‰§è¡Œæ¨ç†æˆ–ç©ºé—²æ£€æŸ¥
        if batch:
            result = self.run_batch(batch)
            self.process_batch_result(batch, result)
        else:
            self.self_check_during_idle()

        self.last_batch = batch
```

### ğŸ” æºç å®ç°ç»†èŠ‚

**è°ƒåº¦å™¨è¿›ç¨‹å¯åŠ¨çš„å®Œæ•´æµç¨‹**ï¼šè¿™ä¸ªå‡½æ•°å±•ç¤ºäº†SGLangè°ƒåº¦å™¨ä»åˆ›å»ºåˆ°è¿è¡Œçš„å®Œæ•´è¿‡ç¨‹ã€‚é¦–å…ˆåˆ›å»ºè°ƒåº¦å™¨å®ä¾‹ï¼Œç„¶åå‘ä¸»è¿›ç¨‹æŠ¥å‘Šå°±ç»ªçŠ¶æ€ï¼Œæœ€åæ ¹æ®å¤æ‚çš„æ¡ä»¶é€»è¾‘é€‰æ‹©åˆé€‚çš„äº‹ä»¶å¾ªç¯ã€‚è¿™ç§è®¾è®¡ç¡®ä¿äº†è°ƒåº¦å™¨èƒ½å¤Ÿé€‚åº”å„ç§éƒ¨ç½²åœºæ™¯çš„éœ€æ±‚ã€‚

```python
def run_scheduler_process(server_args, port_args, gpu_id, tp_rank, 
                         moe_ep_rank, pp_rank, dp_rank, pipe_writer, balance_meta):
    """çœŸå®çš„è°ƒåº¦å™¨è¿›ç¨‹å¯åŠ¨å’Œäº‹ä»¶å¾ªç¯é€‰æ‹©"""
    # åˆ›å»ºè°ƒåº¦å™¨å®ä¾‹
    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, 
                         moe_ep_rank, pp_rank, dp_rank, dp_balance_meta=balance_meta)
    
    # å‘é€å°±ç»ªçŠ¶æ€
    pipe_writer.send({
        "status": "ready",
        "max_total_num_tokens": scheduler.max_total_num_tokens,
        "max_req_input_len": scheduler.max_req_input_len,
    })

    # å¤æ‚çš„äº‹ä»¶å¾ªç¯é€‰æ‹©é€»è¾‘
    disaggregation_mode = scheduler.disaggregation_mode
    if disaggregation_mode == DisaggregationMode.NULL:
        if server_args.pp_size > 1:
            scheduler.event_loop_pp()
        elif scheduler.enable_overlap:
            scheduler.event_loop_overlap()
        else:
            scheduler.event_loop_normal()
    elif disaggregation_mode == DisaggregationMode.PREFILL:
        if scheduler.enable_overlap:
            scheduler.event_loop_overlap_disagg_prefill()
        else:
            if server_args.pp_size > 1:
                scheduler.event_loop_pp_disagg_prefill()  # æµæ°´çº¿+åˆ†ç¦»å¼é¢„å¡«å……
            else:
                scheduler.event_loop_normal_disagg_prefill()
    elif disaggregation_mode == DisaggregationMode.DECODE:
        if scheduler.enable_overlap:
            scheduler.event_loop_overlap_disagg_decode()
        else:
            scheduler.event_loop_normal_disagg_decode()

**é‡å äº‹ä»¶å¾ªç¯çš„é«˜çº§ä¼˜åŒ–**ï¼šè¿™æ˜¯SGLangæ€§èƒ½ä¼˜åŒ–çš„æ ¸å¿ƒå®ç°ï¼Œé€šè¿‡é˜Ÿåˆ—å’Œäº‹ä»¶æœºåˆ¶å®ç°CPUå¤„ç†å’ŒGPUè®¡ç®—çš„çœŸæ­£å¹¶è¡Œã€‚å…³é”®åœ¨äºä½¿ç”¨result_queueç¼“å­˜ç»“æœï¼Œé€šè¿‡threading.EventåŒæ­¥æ‰¹æ¬¡å¯åŠ¨ï¼Œåˆ›å»ºè™šæ‹Ÿæ‰¹æ¬¡å¯åŠ¨é‡å ç®¡é“ï¼Œå®ç°äº†æ˜¾è‘—çš„ååé‡æå‡ã€‚

@DynamicGradMode()
def event_loop_overlap(self):
    """çœŸå®çš„é‡å äº‹ä»¶å¾ªç¯å®ç°"""
    self.result_queue = deque()

    while True:
        recv_reqs = self.recv_requests()
        self.process_input_requests(recv_reqs)

        batch = self.get_next_batch_to_run()
        self.cur_batch = batch

        if batch:
            # å¼‚æ­¥å¯åŠ¨æ‰¹æ¬¡æ¨ç†
            batch.launch_done = threading.Event()
            result = self.run_batch(batch)
            self.result_queue.append((batch.copy(), result))

            # åˆ›å»ºè™šæ‹Ÿé¦–æ‰¹æ¬¡ä»¥å¯åŠ¨é‡å ç®¡é“
            if self.last_batch is None:
                tmp_batch = ScheduleBatch(
                    reqs=None,
                    forward_mode=ForwardMode.DUMMY_FIRST,
                    next_batch_sampling_info=self.tp_worker.cur_sampling_info,
                )
                self.process_batch_result(tmp_batch, None, batch.launch_done)

        # å¤„ç†ä¸Šä¸€ä¸ªæ‰¹æ¬¡çš„ç»“æœï¼ˆå®ç°CPU-GPUé‡å ï¼‰
        if self.last_batch:
            tmp_batch, tmp_result = self.result_queue.popleft()
            tmp_batch.next_batch_sampling_info = (
                self.tp_worker.cur_sampling_info if batch else None
            )
            self.process_batch_result(tmp_batch, tmp_result, 
                                    batch.launch_done if batch else None)
        elif batch is None:
            self.self_check_during_idle()

        self.last_batch = batch

ğŸ’¡ **å®ç°è¯´æ˜**: æºç ä¸­æœ‰7ç§ä¸åŒçš„äº‹ä»¶å¾ªç¯ç»„åˆï¼ŒåŒ…æ‹¬æµæ°´çº¿å¹¶è¡Œçš„åˆ†ç¦»å¼ç‰ˆæœ¬ã€‚é‡å äº‹ä»¶å¾ªç¯ä½¿ç”¨dequeå’Œthreading.Eventå®ç°CPUå¤„ç†å’ŒGPUè®¡ç®—çš„çœŸæ­£å¹¶è¡Œã€‚
```

è¿™ç§è‡ªåŠ¨é€‰æ‹©æœºåˆ¶ç¡®ä¿äº†è°ƒåº¦å™¨èƒ½å¤Ÿæ ¹æ®éƒ¨ç½²é…ç½®é€‰æ‹©æœ€ä¼˜çš„æ‰§è¡Œæ¨¡å¼ã€‚

---

## ğŸ“¦ è¯·æ±‚åˆ†å‘æœºåˆ¶

### ğŸ”€ TypeBasedDispatcheræ¶æ„

### ğŸ¯ æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ

**åŸºäºç±»å‹çš„æ™ºèƒ½è¯·æ±‚è·¯ç”±**ï¼šTypeBasedDispatcheræ˜¯SGLangè¯·æ±‚å¤„ç†çš„æ ¸å¿ƒæœºåˆ¶ï¼Œé€šè¿‡ç±»å‹åŒ¹é…è‡ªåŠ¨å°†ä¸åŒç±»å‹çš„è¯·æ±‚è·¯ç”±åˆ°å¯¹åº”çš„å¤„ç†æ–¹æ³•ã€‚è¿™ç§è®¾è®¡é¿å…äº†å¤æ‚çš„if-elseåˆ¤æ–­é“¾ï¼Œæé«˜äº†ä»£ç çš„å¯ç»´æŠ¤æ€§å’Œæ‰©å±•æ€§ã€‚æ¯ç§è¯·æ±‚ç±»å‹éƒ½æœ‰ä¸“é—¨çš„å¤„ç†å™¨ï¼Œç¡®ä¿å¤„ç†é€»è¾‘çš„ä¸“ä¸šåŒ–å’Œé«˜æ•ˆæ€§ã€‚

```python
# è¯·æ±‚åˆ†å‘å™¨çš„æ ¸å¿ƒæ¦‚å¿µ
self._request_dispatcher = TypeBasedDispatcher([
    (TokenizedGenerateReqInput, self.handle_generate_request),
    (TokenizedEmbeddingReqInput, self.handle_embedding_request),
    (BatchTokenizedGenerateReqInput, self.handle_batch_generate_request),
    (FlushCacheReqInput, self.flush_cache_wrapped),
    (AbortReq, self.abort_request),
    # ... æ›´å¤šè¯·æ±‚ç±»å‹æ˜ å°„
])

# ä½¿ç”¨æ–¹å¼
for recv_req in recv_reqs:
    self._request_dispatcher.dispatch(recv_req)  # è‡ªåŠ¨è·¯ç”±åˆ°å¯¹åº”å¤„ç†å™¨
```

### ğŸ” æºç å®ç°ç»†èŠ‚

**ç”Ÿäº§ç¯å¢ƒä¸­çš„å®Œæ•´è¯·æ±‚ç±»å‹æ”¯æŒ**ï¼šçœŸå®çš„SGLangæºç æ”¯æŒ20å¤šç§ä¸åŒçš„è¯·æ±‚ç±»å‹ï¼Œæ¶µç›–äº†ä»åŸºç¡€çš„æ–‡æœ¬ç”Ÿæˆåˆ°é«˜çº§çš„ç³»ç»Ÿç®¡ç†åŠŸèƒ½ã€‚è¿™ä¸ªå®Œæ•´çš„æ˜ å°„è¡¨å±•ç¤ºäº†SGLangä½œä¸ºç”Ÿäº§çº§ç³»ç»Ÿçš„å¤æ‚æ€§å’Œå®Œæ•´æ€§ï¼Œæ¯ç§è¯·æ±‚ç±»å‹éƒ½ç»è¿‡ç²¾å¿ƒè®¾è®¡ä»¥æ»¡è¶³ä¸åŒçš„ä½¿ç”¨åœºæ™¯ã€‚

```python
# çœŸå®çš„è°ƒåº¦å™¨è¯·æ±‚åˆ†å‘å™¨åˆå§‹åŒ–
self._request_dispatcher = TypeBasedDispatcher([
    (TokenizedGenerateReqInput, self.handle_generate_request),
    (TokenizedEmbeddingReqInput, self.handle_embedding_request),
    (BatchTokenizedGenerateReqInput, self.handle_batch_generate_request),
    (BatchTokenizedEmbeddingReqInput, self.handle_batch_embedding_request),
    (FlushCacheReqInput, self.flush_cache_wrapped),
    (AbortReq, self.abort_request),
    (OpenSessionReqInput, self.open_session),
    (CloseSessionReqInput, self.close_session),
    (UpdateWeightFromDiskReqInput, self.update_weights_from_disk),
    (InitWeightsUpdateGroupReqInput, self.init_weights_update_group),
    (UpdateWeightsFromDistributedReqInput, self.update_weights_from_distributed),
    (UpdateWeightsFromTensorReqInput, self.update_weights_from_tensor),
    (GetWeightsByNameReqInput, self.get_weights_by_name),
    (ReleaseMemoryOccupationReqInput, self.release_memory_occupation),
    (ResumeMemoryOccupationReqInput, self.resume_memory_occupation),
    (SlowDownReqInput, self.slow_down),
    (ProfileReq, self.profile),
    (FreezeGCReq, self.handle_freeze_gc),
    (GetInternalStateReq, self.get_internal_state),
    (SetInternalStateReq, self.set_internal_state),
    (RpcReqInput, self.handle_rpc_request),
    (ExpertDistributionReq, self.expert_distribution_handle),
    (LoadLoRAAdapterReqInput, self.load_lora_adapter),
    (UnloadLoRAAdapterReqInput, self.unload_lora_adapter),
])

ğŸ’¡ **å®ç°è¯´æ˜**: çœŸå®æºç æ”¯æŒ20+ç§è¯·æ±‚ç±»å‹ï¼ŒåŒ…æ‹¬æ‰¹é‡è¯·æ±‚ã€RPCè°ƒç”¨ã€ä¸“å®¶åˆ†å¸ƒã€åƒåœ¾å›æ”¶æ§åˆ¶ã€å†…éƒ¨çŠ¶æ€ç®¡ç†ç­‰é«˜çº§åŠŸèƒ½ã€‚TypeBasedDispatcheré€šè¿‡ç±»å‹åŒ¹é…è‡ªåŠ¨è·¯ç”±è¯·æ±‚åˆ°å¯¹åº”çš„å¤„ç†æ–¹æ³•ã€‚
```

è¿™ç§è®¾è®¡æ”¯æŒäº†å¹¿æ³›çš„è¯·æ±‚ç±»å‹ï¼Œä»åŸºæœ¬çš„ç”Ÿæˆå’ŒåµŒå…¥è¯·æ±‚ï¼Œåˆ°é«˜çº§çš„ç³»ç»Ÿç®¡ç†å’Œæ‰©å±•åŠŸèƒ½ã€‚

---

## âš¡ æ ¸å¿ƒæ–¹æ³•è§£æ

### ğŸ“¥ è¯·æ±‚å¤„ç†æµç¨‹

è°ƒåº¦å™¨é€šè¿‡ä¸€ç³»åˆ—æ ¸å¿ƒæ–¹æ³•å¤„ç†è¯·æ±‚ï¼š

**ç½‘ç»œè¯·æ±‚æ¥æ”¶çš„éé˜»å¡æœºåˆ¶**ï¼šrecv_requestsæ–¹æ³•ä½¿ç”¨ZMQçš„éé˜»å¡æ¨¡å¼ä»tokenizeræ¥æ”¶è¯·æ±‚ï¼Œè¿™ç§è®¾è®¡ç¡®ä¿è°ƒåº¦å™¨ä¸ä¼šå› ä¸ºç­‰å¾…æ–°è¯·æ±‚è€Œé˜»å¡ã€‚é€šè¿‡whileå¾ªç¯å’Œå¼‚å¸¸å¤„ç†ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°æ‰¹é‡æ¥æ”¶å¤šä¸ªè¯·æ±‚ï¼Œæé«˜ç³»ç»Ÿçš„å“åº”æ€§èƒ½ã€‚

```python
def recv_requests(self) -> List[Req]:
    """æ¥æ”¶æ¥è‡ªtokenizerçš„è¯·æ±‚"""
    recv_reqs = []
    while True:
        try:
            recv_req = self.recv_from_tokenizer.recv_pyobj(zmq.NOBLOCK)
            recv_reqs.append(recv_req)
        except zmq.ZMQError:
            break
    return recv_reqs
```

**è¯·æ±‚å¤„ç†çš„å®¹é”™æœºåˆ¶**ï¼šprocess_input_requestsæ–¹æ³•æ˜¯è¯·æ±‚å¤„ç†çš„å…¥å£ç‚¹ï¼Œå®ƒä½¿ç”¨TypeBasedDispatcherè¿›è¡Œç±»å‹åˆ†å‘ï¼Œå¹¶æä¾›å®Œå–„çš„å¼‚å¸¸å¤„ç†æœºåˆ¶ã€‚å½“è¯·æ±‚å¤„ç†å‡ºç°é”™è¯¯æ—¶ï¼Œç³»ç»Ÿä¼šè®°å½•é”™è¯¯æ—¥å¿—å¹¶å‘å®¢æˆ·ç«¯å‘é€é”™è¯¯å“åº”ï¼Œç¡®ä¿ç³»ç»Ÿçš„å¥å£®æ€§ã€‚

```python
def process_input_requests(self, recv_reqs: List[Any]):
    """å¤„ç†è¾“å…¥è¯·æ±‚çš„æ ¸å¿ƒåˆ†å‘é€»è¾‘"""
    for recv_req in recv_reqs:
        try:
            # ä½¿ç”¨TypeBasedDispatcheræ ¹æ®è¯·æ±‚ç±»å‹åˆ†å‘
            self._request_dispatcher.dispatch(recv_req)
        except Exception as e:
            # å¼‚å¸¸å¤„ç†å’Œé”™è¯¯å“åº”
            logger.error(f"Error processing request {getattr(recv_req, 'rid', 'unknown')}: {e}")
            if hasattr(recv_req, 'rid'):
                error_req = Req(recv_req.rid, "", [], SamplingParams())
                self.send_error_response(error_req, str(e))
```

**æ–‡æœ¬ç”Ÿæˆè¯·æ±‚çš„æ ‡å‡†åŒ–å¤„ç†**ï¼šhandle_generate_requestæ–¹æ³•å°†å¤–éƒ¨è¯·æ±‚è½¬æ¢ä¸ºå†…éƒ¨çš„Reqå¯¹è±¡ï¼Œè¿™ä¸ªè¿‡ç¨‹åŒ…æ‹¬å‚æ•°æå–ã€å¯¹è±¡åˆ›å»ºå’Œé˜Ÿåˆ—åŠ å…¥ä¸‰ä¸ªæ­¥éª¤ã€‚Reqå¯¹è±¡æ˜¯SGLangå†…éƒ¨è¯·æ±‚è¡¨ç¤ºçš„æ ‡å‡†æ ¼å¼ï¼ŒåŒ…å«äº†æ¨ç†æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯ã€‚

```python
def handle_generate_request(self, recv_req: TokenizedGenerateReqInput):
    """å¤„ç†æ–‡æœ¬ç”Ÿæˆè¯·æ±‚"""
    # åˆ›å»ºæ–°çš„Reqå¯¹è±¡
    req = Req(
        recv_req.rid,
        recv_req.input_text,
        recv_req.input_ids,
        recv_req.sampling_params,
        return_logprob=recv_req.return_logprob,
        stream=recv_req.stream,
        lora_id=recv_req.lora_id,
    )
    req.tokenizer = self.tokenizer
    
    # æ·»åŠ åˆ°ç­‰å¾…é˜Ÿåˆ—
    self.waiting_queue.append(req)
```

**åµŒå…¥è¯·æ±‚çš„ç‰¹æ®Šå¤„ç†é€»è¾‘**ï¼šhandle_embedding_requestæ–¹æ³•ä¸“é—¨å¤„ç†å‘é‡åµŒå…¥è¯·æ±‚ï¼Œä¸ç”Ÿæˆè¯·æ±‚çš„ä¸»è¦åŒºåˆ«åœ¨äºä½¿ç”¨é»˜è®¤é‡‡æ ·å‚æ•°ã€ç¦ç”¨æµå¼è¾“å‡ºï¼Œå¹¶è®¾ç½®ç‰¹æ®Šçš„is_embeddingæ ‡è®°ã€‚è¿™ç§è®¾è®¡ç¡®ä¿äº†åµŒå…¥è¯·æ±‚èƒ½å¤Ÿè·å¾—æ­£ç¡®çš„å¤„ç†æ–¹å¼ã€‚

```python
def handle_embedding_request(self, recv_req: TokenizedEmbeddingReqInput):
    """å¤„ç†åµŒå…¥è¯·æ±‚"""
    # åˆ›å»ºåµŒå…¥è¯·æ±‚å¯¹è±¡
    req = Req(
        rid=recv_req.rid,
        input_text=recv_req.input_text,
        input_ids=recv_req.input_ids,
        sampling_params=SamplingParams(),  # åµŒå…¥è¯·æ±‚ä½¿ç”¨é»˜è®¤é‡‡æ ·å‚æ•°
        return_logprob=recv_req.return_logprob,
        stream=False,  # åµŒå…¥è¯·æ±‚ä¸æ”¯æŒæµå¼è¾“å‡º
        is_embedding=True,  # æ ‡è®°ä¸ºåµŒå…¥è¯·æ±‚
    )
    req.tokenizer = self.tokenizer
    
    # ç›´æ¥åŠ å…¥ç­‰å¾…é˜Ÿåˆ—
    self.waiting_queue.append(req)
```

### ğŸ”„ æ‰¹å¤„ç†ç®¡ç†

**æ‰¹æ¬¡è°ƒåº¦çš„æ ¸å¿ƒå†³ç­–é€»è¾‘**ï¼šget_next_batch_to_runæ–¹æ³•å®ç°äº†SGLangçš„åŠ¨æ€æ‰¹å¤„ç†ç­–ç•¥ï¼ŒåŒ…æ‹¬æ‰¹æ¬¡åˆå¹¶å’Œæ–°æ‰¹æ¬¡åˆ›å»ºä¸¤ä¸ªå…³é”®åŠŸèƒ½ã€‚å®ƒé¦–å…ˆå°è¯•å°†ä¸Šä¸€è½®çš„é¢„å¡«å……æ‰¹æ¬¡åˆå¹¶åˆ°è¿è¡Œæ‰¹æ¬¡ä¸­ï¼Œç„¶åè·å–æ–°çš„é¢„å¡«å……æ‰¹æ¬¡ï¼Œå®ç°äº†è¿ç»­æ‰¹å¤„ç†çš„é«˜æ•ˆè°ƒåº¦ã€‚

```python
def get_next_batch_to_run(self) -> Optional[ScheduleBatch]:
    # åˆå¹¶é¢„å¡«å……æ‰¹æ¬¡åˆ°è¿è¡Œæ‰¹æ¬¡
    if self.last_batch and self.last_batch.forward_mode.is_extend():
        if not self.last_batch.is_empty():
            if self.running_batch.is_empty():
                self.running_batch = self.last_batch
            else:
                self.running_batch.merge_batch(self.last_batch)

    # è·å–æ–°çš„é¢„å¡«å……æ‰¹æ¬¡
    new_batch = self.get_new_batch_prefill()
    
    return new_batch or self.running_batch
```

**æ™ºèƒ½é¢„å¡«å……æ‰¹æ¬¡æ„å»º**ï¼šget_new_batch_prefillæ–¹æ³•ä½¿ç”¨PrefillAdderæ™ºèƒ½åœ°é€‰æ‹©å’Œç»„ç»‡ç­‰å¾…é˜Ÿåˆ—ä¸­çš„è¯·æ±‚ã€‚å®ƒè€ƒè™‘äº†å†…å­˜é™åˆ¶ã€è¯­æ³•çº¦æŸå’Œèµ„æºå¯ç”¨æ€§ï¼Œç¡®ä¿åˆ›å»ºçš„æ‰¹æ¬¡æ—¢èƒ½æœ€å¤§åŒ–GPUåˆ©ç”¨ç‡ï¼Œåˆä¸ä¼šè¶…å‡ºç³»ç»Ÿèµ„æºé™åˆ¶ã€‚

```python
def get_new_batch_prefill(self) -> Optional[ScheduleBatch]:
    """è·å–æ–°çš„é¢„å¡«å……æ‰¹æ¬¡"""
    if not self.waiting_queue:
        return None
    
    # ä½¿ç”¨PrefillAdderæ™ºèƒ½æ·»åŠ è¯·æ±‚åˆ°æ‰¹æ¬¡
    adder = PrefillAdder(
        req_to_token_pool=self.req_to_token_pool,
        token_to_kv_pool=self.token_to_kv_pool,
        tree_cache=self.tree_cache,
    )
    
    # æ£€æŸ¥è¯­æ³•çº¦æŸé˜Ÿåˆ—
    if hasattr(self, 'grammar_queue') and self.grammar_queue:
        grammar_req = self.grammar_queue.popleft()
        can_run_list = [grammar_req]
    else:
        # ä»ç­‰å¾…é˜Ÿåˆ—ä¸­é€‰æ‹©å¯è¿è¡Œçš„è¯·æ±‚
        can_run_list = []
        while self.waiting_queue and adder.can_add_req(self.waiting_queue[0]):
            req = self.waiting_queue.popleft()
            adder.add_req(req)
            can_run_list.append(req)
    
    if can_run_list:
        # åˆ›å»ºæ–°çš„é¢„å¡«å……æ‰¹æ¬¡
        batch = ScheduleBatch(
            reqs=can_run_list,
            req_to_token_pool=self.req_to_token_pool,
            token_to_kv_pool=self.token_to_kv_pool,
            tree_cache=self.tree_cache,
            forward_mode=ForwardMode.EXTEND,
        )
        return batch
    
    return None
```

**æ‰¹æ¬¡æ¨ç†çš„æ‰§è¡Œå…¥å£**ï¼šrun_batchæ–¹æ³•æ˜¯è¿æ¥è°ƒåº¦å±‚å’Œè®¡ç®—å±‚çš„å…³é”®æ¥å£ï¼Œå®ƒæ ¹æ®æ‰¹æ¬¡çš„å‰å‘æ¨¡å¼ï¼ˆè§£ç æˆ–é¢„å¡«å……ï¼‰è°ƒç”¨ç›¸åº”çš„åº•å±‚è®¡ç®—å‡½æ•°ã€‚è¿™ä¸ªæ–¹æ³•å°†é«˜å±‚çš„æ‰¹æ¬¡è°ƒåº¦å†³ç­–è½¬æ¢ä¸ºå…·ä½“çš„GPUè®¡ç®—ä»»åŠ¡ã€‚

```python
def run_batch(self, batch: ScheduleBatch):
    """æ‰§è¡Œæ‰¹æ¬¡æ¨ç†"""
    if batch.forward_mode.is_decode() or batch.forward_mode.is_mixed():
        # è§£ç é˜¶æ®µ
        model_worker_batch = batch.get_model_worker_batch()
        result = self.tp_worker.forward_batch_generation(model_worker_batch)
    else:
        # é¢„å¡«å……é˜¶æ®µ  
        result = self.tp_worker.forward_batch_generation(model_worker_batch)
    
    return result
```

**æ¨ç†ç»“æœçš„ç»¼åˆå¤„ç†**ï¼šprocess_batch_resultæ–¹æ³•è´Ÿè´£å¤„ç†GPUæ¨ç†çš„è¾“å‡ºç»“æœï¼ŒåŒ…æ‹¬tokenè§£ç ã€å®ŒæˆçŠ¶æ€æ£€æŸ¥ã€æµå¼è¾“å‡ºå‘é€å’Œèµ„æºæ¸…ç†ç­‰å¤šä¸ªç¯èŠ‚ã€‚è¿™ä¸ªæ–¹æ³•ç¡®ä¿äº†æ¯ä¸ªè¯·æ±‚éƒ½èƒ½å¾—åˆ°æ­£ç¡®çš„å¤„ç†å’Œå“åº”ã€‚

```python
def process_batch_result(self, batch: ScheduleBatch, result):
    """å¤„ç†æ‰¹æ¬¡æ¨ç†ç»“æœ"""
    if result is None:
        return
    
    # å¤„ç†æ¯ä¸ªè¯·æ±‚çš„è¾“å‡º
    finished_reqs = []
    for i, req in enumerate(batch.reqs):
        # è§£ç æ–°ç”Ÿæˆçš„token
        if result.next_token_ids is not None:
            new_token_id = result.next_token_ids[i]
            req.output_ids.append(new_token_id)
            
            # æ£€æŸ¥æ˜¯å¦å®Œæˆç”Ÿæˆ
            if self.check_finished(req, new_token_id):
                finished_reqs.append(req)
        
        # å¤„ç†æµå¼è¾“å‡º
        if req.stream and result.next_token_ids is not None:
            self.send_stream_output(req, result.next_token_ids[i])
    
    # ç§»é™¤å·²å®Œæˆçš„è¯·æ±‚
    for req in finished_reqs:
        if req in batch.reqs:
            batch.reqs.remove(req)
            self.release_resources_for_request(req)
        
        # å‘é€æœ€ç»ˆè¾“å‡º
        self.send_final_output(req)
    
    # æ›´æ–°æ‰¹æ¬¡çŠ¶æ€
    if not batch.reqs:
        batch.forward_mode = ForwardMode.IDLE

def check_finished(self, req: Req, new_token_id: int) -> bool:
    """æ£€æŸ¥è¯·æ±‚æ˜¯å¦å®Œæˆ"""
    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§é•¿åº¦
    if len(req.output_ids) >= req.sampling_params.max_new_tokens:
        return True
    
    # æ£€æŸ¥æ˜¯å¦é‡åˆ°ç»“æŸtoken
    if new_token_id in req.sampling_params.stop_token_ids:
        return True
    
    # æ£€æŸ¥æ˜¯å¦é‡åˆ°åœæ­¢å­—ç¬¦ä¸²
    if req.sampling_params.stop_strs:
        output_text = self.tokenizer.decode(req.output_ids)
        for stop_str in req.sampling_params.stop_strs:
            if stop_str in output_text:
                return True
    
    return False
```

### ğŸ”§ ç³»ç»Ÿç»´æŠ¤

**ç³»ç»Ÿç©ºé—²æ—¶çš„ç»´æŠ¤æœºåˆ¶**ï¼šself_check_during_idleæ–¹æ³•åœ¨æ²¡æœ‰æ‰¹æ¬¡éœ€è¦å¤„ç†æ—¶æ‰§è¡Œç³»ç»Ÿç»´æŠ¤ä»»åŠ¡ï¼ŒåŒ…æ‹¬å†…å­˜æ£€æŸ¥ã€èŠ‚èƒ½æ¨¡å¼å’Œç»Ÿè®¡é‡ç½®ç­‰ã€‚è¿™ç§è®¾è®¡ç¡®ä¿äº†SGLangåœ¨é•¿æœŸè¿è¡Œä¸­çš„ç¨³å®šæ€§å’Œèµ„æºä½¿ç”¨æ•ˆç‡ã€‚

```python
def self_check_during_idle(self):
    """ç³»ç»Ÿç©ºé—²æ—¶çš„è‡ªæ£€å’Œç»´æŠ¤"""
    # æ‰§è¡Œå†…å­˜æ³„æ¼æ£€æŸ¥
    self.check_memory()
    
    # å¦‚æœæœ‰ç©ºé—²ç¡çœ å™¨ï¼Œå¯èƒ½è¿›å…¥ç¡çœ èŠ‚èƒ½æ¨¡å¼
    self.maybe_sleep_on_idle()
    
    # é‡ç½®ä¸€äº›ç»Ÿè®¡è®¡æ•°å™¨
    if self.forward_ct % 1000 == 0:
        logger.info(f"Forward count: {self.forward_ct}")
```

**å…¨é¢çš„ç¼“å­˜åˆ·æ–°æœºåˆ¶**ï¼šflush_cacheæ–¹æ³•æä¾›äº†ç³»ç»ŸçŠ¶æ€çš„å®Œå…¨é‡ç½®åŠŸèƒ½ï¼Œæ¸…ç†æ‰€æœ‰é˜Ÿåˆ—ã€ç¼“å­˜å’Œå†…å­˜æ± ï¼Œé‡ç½®ç»Ÿè®¡ä¿¡æ¯ã€‚è¿™ä¸ªåŠŸèƒ½åœ¨ç³»ç»Ÿç»´æŠ¤ã€æƒé‡æ›´æ–°æˆ–æ•…éšœæ¢å¤æ—¶éå¸¸é‡è¦ï¼Œç¡®ä¿ç³»ç»Ÿèƒ½å¤Ÿå›åˆ°ä¸€ä¸ªå¹²å‡€çš„åˆå§‹çŠ¶æ€ã€‚

```python
def flush_cache(self) -> bool:
    """åˆ·æ–°æ‰€æœ‰ç¼“å­˜ï¼Œé‡ç½®è°ƒåº¦å™¨çŠ¶æ€"""
    try:
        # æ¸…ç©ºæ‰€æœ‰é˜Ÿåˆ—
        self.waiting_queue.clear()
        self.running_batch = ScheduleBatch(reqs=[], batch_is_full=False)
        
        # åˆ·æ–°KVç¼“å­˜
        if hasattr(self, 'tree_cache'):
            self.tree_cache.reset()
        
        # é‡ç½®å†…å­˜æ± 
        if hasattr(self, 'req_to_token_pool'):
            self.req_to_token_pool.clear()
            
        # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
        self.forward_ct = 0
        
        logger.info("Cache flushed successfully")
        return True
    except Exception as e:
        logger.error(f"Cache flush failed: {e}")
        return False
```

**ç³»ç»Ÿå¥åº·ç›‘æ§çš„çœ‹é—¨ç‹—æœºåˆ¶**ï¼šwatchdog_threadæ–¹æ³•å®ç°äº†åå°ç›‘æ§åŠŸèƒ½ï¼Œå®šæœŸæ£€æŸ¥è¯·æ±‚è¶…æ—¶å’Œå†…å­˜ä½¿ç”¨æƒ…å†µã€‚è¿™ç§ä¸»åŠ¨ç›‘æ§æœºåˆ¶èƒ½å¤ŸåŠæ—¶å‘ç°å’Œå¤„ç†ç³»ç»Ÿå¼‚å¸¸ï¼Œé˜²æ­¢èµ„æºæ³„éœ²å’Œæ€§èƒ½ä¸‹é™ï¼Œæ˜¯ç”Ÿäº§ç¯å¢ƒä¸­ä¸å¯æˆ–ç¼ºçš„åŠŸèƒ½ã€‚

```python
def watchdog_thread(self):
    """çœ‹é—¨ç‹—çº¿ç¨‹ï¼Œç›‘æ§è°ƒåº¦å™¨å¥åº·çŠ¶æ€"""
    while True:
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰åƒµæ­»çš„è¯·æ±‚
            current_time = time.time()
            for req in self.running_batch.reqs:
                if current_time - req.start_time > self.request_timeout:
                    logger.warning(f"Request {req.rid} timeout, aborting")
                    self.abort_request(AbortReq(req.rid))
            
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
            if self.get_memory_usage() > 0.9:
                logger.warning("High memory usage detected")
                
            time.sleep(10)  # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
        except Exception as e:
            logger.error(f"Watchdog error: {e}")
```

---

## ğŸ’¾ å†…å­˜å’Œèµ„æºç®¡ç†

è°ƒåº¦å™¨ç»Ÿä¸€ç®¡ç†ç³»ç»Ÿçš„å†…å­˜å’Œè®¡ç®—èµ„æºã€‚åœ¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­ï¼Œè°ƒåº¦å™¨ä¼šåˆ›å»ºå’Œé…ç½®å„ç§èµ„æºç®¡ç†ç»„ä»¶ï¼š

### ğŸ¯ æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ

**ä¸‰å±‚å†…å­˜ç®¡ç†æ¶æ„çš„è®¾è®¡ç†å¿µ**ï¼šSGLangé‡‡ç”¨"è¯·æ±‚æ± â†’KVç¼“å­˜â†’å‰ç¼€ç¼“å­˜"çš„ä¸‰å±‚æ¶æ„æ¥ç®¡ç†å†…å­˜èµ„æºã€‚è¯·æ±‚æ± ç®¡ç†å¹¶å‘æ§½ä½ï¼ŒKVç¼“å­˜åˆ†é…å™¨å¤„ç†æ³¨æ„åŠ›æœºåˆ¶çš„é”®å€¼å­˜å‚¨ï¼Œå‰ç¼€ç¼“å­˜ä¼˜åŒ–é‡å¤å†…å®¹çš„å‘½ä¸­ç‡ã€‚è¿™ç§åˆ†å±‚è®¾è®¡å®ç°äº†èµ„æºçš„ç²¾ç»†åŒ–ç®¡ç†å’Œé«˜æ•ˆåˆ©ç”¨ã€‚

```python
def init_memory_pool_and_cache(self):
    """å†…å­˜æ± å’Œç¼“å­˜ç³»ç»Ÿåˆå§‹åŒ–çš„æ ¸å¿ƒæ¦‚å¿µ"""
    # 1. è¯·æ±‚åˆ°tokenæ˜ å°„æ±  - ç®¡ç†å¹¶å‘è¯·æ±‚æ§½ä½
    self.req_to_token_pool = ReqToTokenPool(
        size=self.max_running_requests,
        pre_alloc_size=self.pre_alloc_size,
    )
    
    # 2. KVç¼“å­˜åˆ†é…å™¨ - ç®¡ç†æ³¨æ„åŠ›é”®å€¼ç¼“å­˜
    if self.is_hybrid:
        # SWAæ··åˆç¼“å­˜æ¶æ„
        self.token_to_kv_pool = HybridKVPoolAllocator(...)
    else:
        # æ ‡å‡†ç¼“å­˜æ¶æ„
        self.token_to_kv_pool = BaseTokenToKVPoolAllocator(...)
    
    # 3. å‰ç¼€ç¼“å­˜ - ä¼˜åŒ–é‡å¤å‰ç¼€çš„ç¼“å­˜å‘½ä¸­
    self.tree_cache = RadixCache(...)  # æˆ–ChunkCache
```

### ğŸ” æºç å®ç°ç»†èŠ‚

```python
def init_memory_pool_and_cache(self):
    """çœŸå®çš„SGLangæºç å®ç°"""
    server_args = self.server_args

    # å†…å­˜æ± ä»tp_workerè·å–ï¼Œè€Œéç›´æ¥åˆ›å»º
    self.req_to_token_pool, self.token_to_kv_pool_allocator = (
        self.tp_worker.get_memory_pool()
    )

    # å¤æ‚çš„ç¼“å­˜ç±»å‹é€‰æ‹©é€»è¾‘
    if (
        server_args.chunked_prefill_size is not None
        and server_args.disable_radix_cache
    ):
        # å—ç¼“å­˜åˆ†æ”¯
        if self.is_hybrid:
            ChunkCacheClass = SWAChunkCache
        else:
            ChunkCacheClass = ChunkCache
        self.tree_cache = ChunkCacheClass(
            req_to_token_pool=self.req_to_token_pool,
            token_to_kv_pool_allocator=self.token_to_kv_pool_allocator,
            page_size=self.page_size,
        )
    else:
        # RadixCacheåˆ†æ”¯
        if os.environ.get("SGLANG_EXPERIMENTAL_CPP_RADIX_TREE") == "1":
            # C++å®éªŒæ€§å®ç°
            from sglang.srt.mem_cache.radix_cache_cpp import RadixCacheCpp
            self.tree_cache = RadixCacheCpp(
                disable=False,
                use_hicache=self.enable_hierarchical_cache,
                req_to_token_pool=self.req_to_token_pool,
                token_to_kv_pool=self.token_to_kv_pool_allocator,
                tp_cache_group=self.tp_cpu_group,
                page_size=self.page_size,
                hicache_ratio=server_args.hicache_ratio,
                hicache_size=server_args.hicache_size,
                hicache_write_policy=server_args.hicache_write_policy,
                enable_kv_cache_events=self.enable_kv_cache_events,
            )
        elif self.enable_hierarchical_cache:
            # åˆ†å±‚ç¼“å­˜å®ç°
            if self.enable_lora:
                self.tree_cache = LoRARadixCache(...)
            elif self.is_hybrid:
                self.tree_cache = SWARadixCache(...)
            else:
                self.tree_cache = HiRadixCache(...)
        else:
            # æ ‡å‡†RadixCache
            if self.enable_lora:
                self.tree_cache = LoRARadixCache(...)
            elif self.is_hybrid:
                self.tree_cache = SWARadixCache(...)
            else:
                self.tree_cache = RadixCache(...)

ğŸ’¡ **å®ç°è¯´æ˜**: æºç ä¸­æœ‰10+ç§ç¼“å­˜ç±»å‹ï¼Œæ ¹æ®chunked_prefillã€LoRAã€SWAã€åˆ†å±‚ç¼“å­˜ç­‰ä¸åŒé…ç½®ç»„åˆé€‰æ‹©ã€‚æ•™å­¦ç‰ˆæœ¬çªå‡ºæ ¸å¿ƒçš„"è¯·æ±‚æ± â†’KVç¼“å­˜â†’å‰ç¼€ç¼“å­˜"è®¾è®¡æ¨¡å¼ã€‚
```

**è¯·æ±‚èµ„æºåˆ†é…çš„å®Œæ•´æµç¨‹**ï¼šèµ„æºåˆ†é…æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„è¿‡ç¨‹ï¼Œé¦–å…ˆåˆ†é…è¯·æ±‚æ§½ä½ï¼Œç„¶ååˆ†é…KVç¼“å­˜ç©ºé—´ã€‚è¿™ç§è®¾è®¡ç¡®ä¿äº†èµ„æºåˆ†é…çš„åŸå­æ€§ï¼Œå¦‚æœä»»ä½•ä¸€ä¸ªé˜¶æ®µå¤±è´¥ï¼Œå·²åˆ†é…çš„èµ„æºä¼šè¢«æ­£ç¡®é‡Šæ”¾ï¼Œé¿å…èµ„æºæ³„éœ²ã€‚

```python
def allocate_resources_for_request(self, req: Req):
    """ä¸ºè¯·æ±‚åˆ†é…èµ„æº"""
    # åˆ†é…tokenæ§½ä½
    if not self.req_to_token_pool.available():
        logger.warning("No available request slots")
        return False
        
    req_pool_idx = self.req_to_token_pool.alloc()
    req.req_pool_idx = req_pool_idx
    
    # åˆ†é…KVç¼“å­˜
    num_tokens = len(req.origin_input_ids)
    if self.token_to_kv_pool.available_size() < num_tokens:
        logger.warning("Insufficient KV cache space")
        self.req_to_token_pool.free(req_pool_idx)
        return False
    
    # ä¸ºè¯·æ±‚åˆ†é…tokenæ± 
    req.token_pool_indices = self.token_to_kv_pool.alloc(num_tokens)
    
    return True

def release_resources_for_request(self, req: Req):
    """é‡Šæ”¾è¯·æ±‚å ç”¨çš„èµ„æº"""
    if hasattr(req, 'req_pool_idx'):
        self.req_to_token_pool.free(req.req_pool_idx)
    
    if hasattr(req, 'token_pool_indices'):
        self.token_to_kv_pool.free(req.token_pool_indices)
```

è¿™äº›ç»„ä»¶ååŒå·¥ä½œï¼Œç¡®ä¿å†…å­˜çš„é«˜æ•ˆä½¿ç”¨å’Œç¼“å­˜çš„æ™ºèƒ½ç®¡ç†ï¼ŒåŒæ—¶æ”¯æŒè¯·æ±‚çš„åŠ¨æ€èµ„æºåˆ†é…å’Œå›æ”¶ã€‚

---

## ğŸŒ å¹¶è¡Œç­–ç•¥åè°ƒ

åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹ï¼Œè°ƒåº¦å™¨éœ€è¦åè°ƒå¤šç§å¹¶è¡Œç­–ç•¥ï¼š

### ğŸ”€ å¼ é‡å¹¶è¡Œåè°ƒ

**ä¸»ä»æ¶æ„çš„åŒæ­¥æœºåˆ¶**ï¼šå¼ é‡å¹¶è¡Œé‡‡ç”¨ä¸»ä»æ¶æ„ï¼Œrank 0ä½œä¸ºä¸»èŠ‚ç‚¹è´Ÿè´£è°ƒåº¦å†³ç­–å¹¶å¹¿æ’­ç»™å…¶ä»–èŠ‚ç‚¹ï¼Œä»èŠ‚ç‚¹æ¥æ”¶è°ƒåº¦ä¿¡æ¯å¹¶æ‰§è¡Œç›¸åº”æ“ä½œã€‚è¿™ç§è®¾è®¡ç¡®ä¿äº†æ‰€æœ‰å¹¶è¡ŒèŠ‚ç‚¹çš„åŒæ­¥æ‰§è¡Œï¼Œé¿å…äº†åˆ†å¸ƒå¼ç¯å¢ƒä¸‹çš„æ•°æ®ä¸ä¸€è‡´é—®é¢˜ã€‚

```python
def coordinate_tensor_parallel(self):
    """å¼ é‡å¹¶è¡Œåè°ƒæœºåˆ¶"""
    # ç¡®ä¿æ‰€æœ‰TP workeråŒæ­¥
    if self.tp_size > 1:
        # å¹¿æ’­æ‰¹æ¬¡ä¿¡æ¯åˆ°æ‰€æœ‰TP ranks
        if self.tp_rank == 0:
            # ä¸»rankè´Ÿè´£è°ƒåº¦å†³ç­–
            batch_info = {
                'batch_size': len(self.running_batch.reqs),
                'forward_mode': self.running_batch.forward_mode,
                'request_ids': [req.rid for req in self.running_batch.reqs]
            }
            # å¹¿æ’­åˆ°å…¶ä»–ranks
            self.broadcast_to_tp_group(batch_info)
        else:
            # ä»å±ranksæ¥æ”¶è°ƒåº¦ä¿¡æ¯
            batch_info = self.receive_from_tp_master()
```

### ğŸš€ æµæ°´çº¿å¹¶è¡Œç®¡ç†

**åˆ†é˜¶æ®µå¤„ç†çš„æµæ°´çº¿æ¶æ„**ï¼šæµæ°´çº¿å¹¶è¡Œå°†æ¨¡å‹åˆ†å‰²æˆå¤šä¸ªé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µåœ¨ä¸åŒçš„GPUä¸Šæ‰§è¡Œã€‚ç¬¬ä¸€é˜¶æ®µå¤„ç†è¾“å…¥ï¼Œä¸­é—´é˜¶æ®µä¼ é€’éšè—çŠ¶æ€ï¼Œæœ€åé˜¶æ®µç”Ÿæˆè¾“å‡ºã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨å¤šä¸ªGPUèŠ‚ç‚¹ï¼Œæ”¯æŒè¶…å¤§æ¨¡å‹çš„æ¨ç†ã€‚

```python
def manage_pipeline_parallel(self):
    """æµæ°´çº¿å¹¶è¡Œç®¡ç†"""
    if self.pp_size > 1:
        # å¤„ç†pipeline stageé—´çš„æ•°æ®ä¼ é€’
        if self.pp_rank == 0:
            # ç¬¬ä¸€é˜¶æ®µï¼šå¤„ç†è¾“å…¥
            hidden_states = self.process_input_stage(batch)
            self.send_to_next_stage(hidden_states)
        elif self.pp_rank == self.pp_size - 1:
            # æœ€åé˜¶æ®µï¼šç”Ÿæˆè¾“å‡º
            hidden_states = self.receive_from_prev_stage()
            output = self.process_output_stage(hidden_states)
        else:
            # ä¸­é—´é˜¶æ®µï¼šä¼ é€’éšè—çŠ¶æ€
            hidden_states = self.receive_from_prev_stage()
            hidden_states = self.process_middle_stage(hidden_states)
            self.send_to_next_stage(hidden_states)
```

### âš–ï¸ æ•°æ®å¹¶è¡Œè´Ÿè½½å‡è¡¡

**åŠ¨æ€è´Ÿè½½å‡è¡¡çš„å®ç°æœºåˆ¶**ï¼šæ•°æ®å¹¶è¡Œé€šè¿‡æ”¶é›†å„å‰¯æœ¬çš„è´Ÿè½½ä¿¡æ¯ï¼ˆè¿è¡Œè¯·æ±‚æ•°ã€ç­‰å¾…è¯·æ±‚æ•°ã€å†…å­˜ä½¿ç”¨ç‡ï¼‰æ¥å®ç°è´Ÿè½½å‡è¡¡ã€‚ä¸»å‰¯æœ¬æ ¹æ®è´Ÿè½½å·®å¼‚åŠ¨æ€é‡åˆ†é…è¯·æ±‚ï¼Œç¡®ä¿å„ä¸ªå‰¯æœ¬çš„å·¥ä½œè´Ÿè½½ç›¸å¯¹å‡è¡¡ï¼Œæœ€å¤§åŒ–æ•´ä½“ç³»ç»Ÿååé‡ã€‚

```python
def balance_data_parallel_load(self):
    """æ•°æ®å¹¶è¡Œè´Ÿè½½å‡è¡¡"""
    if self.dp_size > 1:
        # æ”¶é›†å„å‰¯æœ¬çš„è´Ÿè½½ä¿¡æ¯
        local_load = {
            'running_requests': len(self.running_batch.reqs),
            'waiting_requests': len(self.waiting_queue),
            'memory_usage': self.get_memory_usage()
        }
        
        # ä¸å…¶ä»–å‰¯æœ¬äº¤æ¢è´Ÿè½½ä¿¡æ¯
        all_loads = self.gather_dp_loads(local_load)
        
        # æ ¹æ®è´Ÿè½½å·®å¼‚è¿›è¡Œè¯·æ±‚é‡åˆ†é…
        if self.dp_rank == 0:  # ä¸»å‰¯æœ¬è´Ÿè´£è°ƒåº¦
            self.redistribute_requests(all_loads)
```

### ğŸ§  ä¸“å®¶å¹¶è¡Œè°ƒåº¦

**åŸºäºtokenè·¯ç”±çš„ä¸“å®¶åˆ†å‘**ï¼šMoEä¸“å®¶å¹¶è¡Œæ ¹æ®tokençš„ä¸“å®¶è·¯ç”±ä¿¡æ¯å°†è®¡ç®—ä»»åŠ¡åˆ†å‘åˆ°ä¸åŒçš„ä¸“å®¶èŠ‚ç‚¹ã€‚æ¯ä¸ªtokenå¯èƒ½éœ€è¦å¤šä¸ªä¸“å®¶å¤„ç†ï¼Œè°ƒåº¦å™¨æ ¹æ®ä¸“å®¶IDå’Œå¹¶è¡Œrankçš„æ˜ å°„å…³ç³»ï¼Œå°†tokenå‘é€åˆ°å¯¹åº”çš„ä¸“å®¶èŠ‚ç‚¹è¿›è¡Œå¤„ç†ã€‚

```python
def schedule_expert_parallel(self, expert_distribution: Dict):
    """MoEä¸“å®¶å¹¶è¡Œè°ƒåº¦"""
    if self.moe_ep_size > 1:
        # æ ¹æ®tokenè·¯ç”±åˆ°ä¸åŒä¸“å®¶
        for req in self.running_batch.reqs:
            expert_ids = req.get_expert_routing()
            for expert_id in expert_ids:
                target_ep_rank = expert_id % self.moe_ep_size
                if target_ep_rank == self.moe_ep_rank:
                    # å½“å‰rankå¤„ç†è¿™ä¸ªä¸“å®¶
                    self.process_expert_tokens(req, expert_id)
                else:
                    # å‘é€åˆ°å¯¹åº”çš„ä¸“å®¶rank
                    self.send_to_expert_rank(req, expert_id, target_ep_rank)
```

è¿™äº›å¹¶è¡Œç­–ç•¥ç¡®ä¿äº†SGLangèƒ½å¤Ÿåœ¨å¤§è§„æ¨¡åˆ†å¸ƒå¼ç¯å¢ƒä¸‹é«˜æ•ˆè¿è¡Œï¼Œæœ€å¤§åŒ–ç¡¬ä»¶èµ„æºåˆ©ç”¨ç‡ã€‚

---

## ğŸ“Š æ€§èƒ½ç›‘æ§å’Œè°ƒè¯•

è°ƒåº¦å™¨å†…ç½®äº†å®Œå–„çš„æ€§èƒ½ç›‘æ§å’Œè°ƒè¯•åŠŸèƒ½ï¼š

### ğŸ“ˆ å…³é”®æŒ‡æ ‡æ”¶é›†

**å…¨æ–¹ä½æ€§èƒ½æŒ‡æ ‡ç›‘æ§**ï¼šæ€§èƒ½æŒ‡æ ‡æ”¶é›†æ¶µç›–äº†ååé‡ã€èµ„æºåˆ©ç”¨ç‡ã€é˜Ÿåˆ—çŠ¶æ€å’Œç¼“å­˜å‘½ä¸­ç‡ç­‰å…³é”®ç»´åº¦ã€‚é€šè¿‡SchedulerMetricsMixinæä¾›çš„ç»Ÿä¸€æ¥å£ï¼Œç³»ç»Ÿèƒ½å¤Ÿå®æ—¶ç›‘æ§è¿è¡ŒçŠ¶æ€ï¼Œä¸ºæ€§èƒ½ä¼˜åŒ–å’Œé—®é¢˜è¯Šæ–­æä¾›æ•°æ®æ”¯æŒã€‚

```python
def collect_performance_metrics(self):
    """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
    # é€šè¿‡SchedulerMetricsMixinæ”¶é›†æŒ‡æ ‡
    current_time = time.time()
    
    # ååé‡è®¡ç®—
    if hasattr(self, 'last_metric_time'):
        time_diff = current_time - self.last_metric_time
        tokens_processed = self.forward_ct - self.last_forward_ct
        self.last_gen_throughput = tokens_processed / time_diff if time_diff > 0 else 0
    
    # èµ„æºåˆ©ç”¨ç‡
    token_usage = self.get_token_usage_ratio()
    memory_usage = self.get_memory_usage_ratio()
    
    # é˜Ÿåˆ—çŠ¶æ€
    queue_stats = {
        'running_requests': len(self.running_batch.reqs),
        'waiting_requests': len(self.waiting_queue),
        'total_requests_processed': self.total_requests_processed,
    }
    
    # ç¼“å­˜å‘½ä¸­ç‡
    if self.tree_cache:
        cache_hit_rate = self.tree_cache.get_hit_rate()
    else:
        cache_hit_rate = 0.0
    
    # æ›´æ–°æŒ‡æ ‡
    self.stats.update(
        gen_throughput=self.last_gen_throughput,
        token_usage=token_usage,
        memory_usage=memory_usage,
        cache_hit_rate=cache_hit_rate,
        **queue_stats
    )
```

### ğŸ” æ€§èƒ½åˆ†æå·¥å…·

**PyTorch Profileré›†æˆçš„æ·±åº¦åˆ†æ**ï¼šæ€§èƒ½åˆ†æå·¥å…·é€šè¿‡é›†æˆPyTorch Profilerï¼Œèƒ½å¤Ÿè¯¦ç»†åˆ†æCPUå’ŒCUDAçš„æ‰§è¡Œæƒ…å†µã€‚æ”¯æŒçµæ´»çš„è°ƒåº¦é…ç½®ï¼ˆç­‰å¾…ã€é¢„çƒ­ã€æ´»è·ƒã€é‡å¤ï¼‰ï¼Œè®°å½•å½¢çŠ¶ä¿¡æ¯å’Œè°ƒç”¨æ ˆï¼Œç”ŸæˆChrome traceæ ¼å¼çš„åˆ†ææŠ¥å‘Šã€‚

```python
def start_profiling(self, profile_config: ProfileConfig):
    """å¯åŠ¨æ€§èƒ½åˆ†æ"""
    # é€šè¿‡SchedulerProfilerMixinå¯åŠ¨profiler
    import torch.profiler as profiler
    
    self.torch_profiler = profiler.profile(
        activities=[
            profiler.ProfilerActivity.CPU,
            profiler.ProfilerActivity.CUDA,
        ],
        schedule=profiler.schedule(
            wait=profile_config.wait_steps,
            warmup=profile_config.warmup_steps, 
            active=profile_config.active_steps,
            repeat=profile_config.repeat
        ),
        on_trace_ready=self.save_profiler_trace,
        record_shapes=profile_config.record_shapes,
        with_stack=profile_config.with_stack,
    )
    
    self.torch_profiler.start()
    logger.info("Performance profiling started")

def save_profiler_trace(self, prof):
    """ä¿å­˜æ€§èƒ½åˆ†æç»“æœ"""
    output_path = f"{self.profiler_output_dir}/trace_{self.forward_ct}.json"
    prof.export_chrome_trace(output_path)
    logger.info(f"Profiler trace saved to {output_path}")
```

### ğŸ› ï¸ è°ƒè¯•å·¥å…·

**ç³»ç»ŸçŠ¶æ€å¿«ç…§çš„è°ƒè¯•æ”¯æŒ**ï¼šè°ƒè¯•å·¥å…·æä¾›äº†å®Œæ•´çš„ç³»ç»ŸçŠ¶æ€å¿«ç…§åŠŸèƒ½ï¼ŒåŒ…æ‹¬è¿è¡Œæ‰¹æ¬¡ã€ç­‰å¾…é˜Ÿåˆ—ã€å†…å­˜ç»Ÿè®¡å’Œå¹¶è¡Œé…ç½®ç­‰å…³é”®ä¿¡æ¯ã€‚è¿™äº›å·¥å…·åœ¨é—®é¢˜æ’æŸ¥ã€æ€§èƒ½è°ƒä¼˜å’Œç³»ç»Ÿç›‘æ§ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œå¸®åŠ©å¼€å‘è€…å¿«é€Ÿå®šä½é—®é¢˜ã€‚

```python
def dump_scheduler_state(self):
    """è½¬å‚¨è°ƒåº¦å™¨çŠ¶æ€ç”¨äºè°ƒè¯•"""
    state_info = {
        'timestamp': time.time(),
        'forward_count': self.forward_ct,
        'running_batch': {
            'size': len(self.running_batch.reqs),
            'forward_mode': str(self.running_batch.forward_mode),
            'requests': [req.rid for req in self.running_batch.reqs[:5]]  # åªæ˜¾ç¤ºå‰5ä¸ª
        },
        'waiting_queue': {
            'size': len(self.waiting_queue),
            'requests': [req.rid for req in self.waiting_queue[:5]]
        },
        'memory_stats': {
            'token_usage': self.get_token_usage_ratio(),
            'available_tokens': self.token_to_kv_pool.available_size(),
            'req_pool_usage': len(self.req_to_token_pool.free_slots)
        },
        'parallel_config': {
            'tp_rank': self.tp_rank,
            'tp_size': self.tp_size,
            'pp_rank': self.pp_rank,
            'dp_rank': self.dp_rank
        }
    }
    
    logger.info(f"Scheduler state dump: {json.dumps(state_info, indent=2)}")
    return state_info

def monitor_request_lifecycle(self, req: Req, event: str):
    """ç›‘æ§è¯·æ±‚ç”Ÿå‘½å‘¨æœŸ"""
    if self.enable_request_tracing:
        trace_info = {
            'request_id': req.rid,
            'event': event,
            'timestamp': time.time(),
            'queue_position': len(self.waiting_queue) if event == 'queued' else None,
            'batch_size': len(self.running_batch.reqs) if event == 'processing' else None
        }
        self.request_traces.append(trace_info)
```

è¿™äº›åŠŸèƒ½ä¸ºç³»ç»Ÿä¼˜åŒ–å’Œé—®é¢˜æ’æŸ¥æä¾›äº†å¼ºæœ‰åŠ›çš„å·¥å…·ï¼Œå¸®åŠ©å¼€å‘è€…è¯†åˆ«æ€§èƒ½ç“¶é¢ˆå’Œä¼˜åŒ–æœºä¼šã€‚

---

## ğŸ“ æ€»ç»“

SGLangè°ƒåº¦å™¨é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ¶æ„å’Œä¸°å¯Œçš„åŠŸèƒ½ï¼Œä¸ºé«˜æ€§èƒ½è¯­è¨€æ¨¡å‹æ¨ç†æä¾›äº†å¼ºå¤§çš„è°ƒåº¦èƒ½åŠ›ï¼š

### ğŸ¯ è®¾è®¡äº®ç‚¹

**æ¨¡å—åŒ–æ¶æ„**: Mixinæ¨¡å¼ç¡®ä¿äº†åŠŸèƒ½çš„æ¨¡å—åŒ–å’Œå¯æ‰©å±•æ€§ï¼Œ6ä¸ªMixinç±»åˆ†åˆ«è´Ÿè´£è¾“å‡ºå¤„ç†ã€æƒé‡æ›´æ–°ã€æ€§èƒ½åˆ†æã€æŒ‡æ ‡æ”¶é›†å’Œåˆ†ç¦»å¼æ¶æ„æ”¯æŒã€‚

**å¤šæ ·åŒ–äº‹ä»¶å¾ªç¯**: æä¾›7ç§ä¸åŒçš„äº‹ä»¶å¾ªç¯ç»„åˆï¼Œä»æ ‡å‡†åŒæ­¥åˆ°CPU-GPUé‡å ï¼Œä»æµæ°´çº¿å¹¶è¡Œåˆ°åˆ†ç¦»å¼æ¶æ„ï¼Œé€‚åº”å„ç§éƒ¨ç½²åœºæ™¯ã€‚

**æ™ºèƒ½è¯·æ±‚åˆ†å‘**: TypeBasedDispatcheræ”¯æŒ20+ç§è¯·æ±‚ç±»å‹çš„è‡ªåŠ¨è·¯ç”±ï¼Œä»åŸºæœ¬çš„ç”Ÿæˆ/åµŒå…¥è¯·æ±‚åˆ°é«˜çº§çš„ç³»ç»Ÿç®¡ç†åŠŸèƒ½ã€‚

**çµæ´»èµ„æºç®¡ç†**: æ”¯æŒ10+ç§ç¼“å­˜ç±»å‹ç»„åˆï¼ŒåŒ…æ‹¬RadixCacheã€ChunkCacheã€SWAæ··åˆç¼“å­˜ã€åˆ†å±‚ç¼“å­˜ç­‰ï¼Œæ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆã€‚

**å…¨é¢ç›‘æ§èƒ½åŠ›**: å†…ç½®å®Œå–„çš„æ€§èƒ½ç›‘æ§ã€è°ƒè¯•å·¥å…·å’ŒçŠ¶æ€ç®¡ç†ï¼Œæ”¯æŒPyTorch Profileré›†æˆå’Œå®æ—¶æŒ‡æ ‡æ”¶é›†ã€‚

### ğŸ”§ å®ç°ç‰¹è‰²

**æºç å‡†ç¡®æ€§**: æœ¬æ–‡æ¡£åŸºäºçœŸå®SGLangæºç ç¼–å†™ï¼Œæ‰€æœ‰ä»£ç ç¤ºä¾‹éƒ½æ¥è‡ªå®é™…å®ç°ï¼Œç¡®ä¿æŠ€æœ¯å‡†ç¡®æ€§ã€‚

**æ•™å­¦ä¸å®è·µå¹¶é‡**: é‡‡ç”¨"æ ¸å¿ƒè®¾è®¡æ¦‚å¿µ + æºç å®ç°ç»†èŠ‚"çš„åŒé‡ç»“æ„ï¼Œæ—¢ä¾¿äºç†è§£æ ¸å¿ƒæ€æƒ³ï¼Œåˆæä¾›å®ç°å‚è€ƒã€‚

**æ¸è¿›å¼å­¦ä¹ **: ä»ç®€åŒ–çš„æ•™å­¦ç‰ˆæœ¬å¼€å§‹ï¼Œé€æ­¥æ·±å…¥åˆ°å¤æ‚çš„æºç å®ç°ï¼Œé€‚åˆä¸åŒå±‚æ¬¡çš„å­¦ä¹ éœ€æ±‚ã€‚

ç†è§£è°ƒåº¦å™¨çš„è®¾è®¡ç†å¿µå’Œå·¥ä½œåŸç†ï¼Œæ˜¯æŒæ¡SGLangç³»ç»Ÿçš„å…³é”®æ‰€åœ¨ã€‚é€šè¿‡æœ¬æ–‡æ¡£çš„å­¦ä¹ ï¼Œå¼€å‘è€…å¯ä»¥æ·±å…¥ç†è§£SGLangè°ƒåº¦å™¨çš„æ¶æ„ç²¾é«“ï¼Œä¸ºåç»­çš„å®šåˆ¶å¼€å‘å’Œæ€§èƒ½ä¼˜åŒ–æ‰“ä¸‹åšå®åŸºç¡€ã€‚
